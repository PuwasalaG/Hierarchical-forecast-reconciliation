\documentclass[a4paper, 11pt]{article}
\usepackage{monashwp}
\usepackage{amssymb, qtree, bm, multirow, textcmds, siunitx}
\usepackage{mathrsfs, float, booktabs,todonotes,amsthm}
\usepackage[bb=boondox]{mathalfa}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,shapes,fit,calc}
\usepackage{amsfonts}
\hypersetup{citecolor=blue,linkcolor=blue,urlcolor=blue}
\DeclareNameAlias{sortname}{last-first}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\def\mathbi#1{\textit{ #1}}
\def\mathB#1{\textbf{ #1}}
\def\E{\text{E}}
\mathtoolsset{showonlyrefs=true}

\def\PQ{\begin{pmatrix}\bm{G}\\[-0.2cm]\bm{H}\end{pmatrix}}
\def\bt{\begin{pmatrix}\tilde{\bm{b}}\\[-0.2cm]\tilde{\bm{a}}\end{pmatrix}}

\theoremstyle{theo}
\newtheorem{theo}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\bibliography{References_paper1}

\title{Probabilistic Forecasts in Hierarchical~Time~Series}
\author{Puwasala Gamakumara\\ Anastasios Panagiotelis\\ George Athanasopoulos\\ Rob J Hyndman}

\addresses{\textbf{Puwasala Gamakumara}\\
  Department of Econometrics and Business Statistics,\\
  Monash University,\\ VIC 3800, Australia.\\
  Email: Puwasala.Gamakumara@monash.edu \\[1cm]
  \textbf{Anastasios Panagiotelis}\\
  Department of Econometrics and Business Statistics,\\
  Monash University,\\ VIC 3800, Australia.\\
  Email: Anastasios.Panagiotelis@monash.edu\\[1cm]
  \textbf{George Athanasopoulos}\\
  Department of Econometrics and Business Statistics,\\
  Monash University,\\ VIC 3800, Australia.\\
  Email: George.Athanasopoulos@monash.edu\\[1cm]
  \textbf{Rob J Hyndman}\\
  Department of Econometrics and Business Statistics,\\
  Monash University,\\ VIC 3800, Australia.\\
  Email: Rob.Hyndman@monash.edu}

\lfoot{\sf Gamakumara, Panagiotelis, Athanasopoulos \& Hyndman: \today}

\begin{document}
  
\maketitle

\begin{abstract}
  TBC
\end{abstract}


\section{Introduction}\label{sec:intro}

Many research applications involve a large collection of time series, some of which are aggregates of others. These are called hierarchical time series. For example, electricity demand of a country can be disaggregated along a geographical hierarchy: the electricity demand of the whole country can be divided into the demand of states, cities, and households. 

When forecasting such time series, it is important to have ``coherent'' forecasts across the hierarchy: aggregates of the forecasts at lower levels should be equal to the forecasts at the upper levels of aggregation. In other words, sums of forecasts should be equal to the forecasts of the sums.

The traditional approaches to produce coherent point forecasts are the bottom-up, top-down and middle-out methods. In the bottom-up approach, forecasts of the lowest level are first generated and they are simply aggregated to forecast upper levels of the hierarchy \citep{Dunn1976}. In contrast, the top-down approach involves forecasting the most aggregated series first and then disaggregating these forecasts down the hierarchy based on the corresponding proportions of observed data \citep{Gross1990}. Many studies have discussed the relative advantages and disadvantages of bottom-up and top-down methods, and situations in which each would provide reliable forecasts \citep{Schwarzkopf1988,Kahn1998, Lapide1998,Fliedner2001}. A compromise between these two approaches is the middle-out method which entails forecasting each series of a selected middle level in the hierarchy and then forecasting upper levels by the bottom-up method and lower levels by the top-down method. 

It is apparent that these three approaches use only part of the information available when producing coherent forecasts. This might result in inaccurate forecasts. For example, if the bottom-level series are highly volatile or noisy, and hence challenging to forecast, then the resulting forecasts from the bottom-up approach are likely to be inaccurate.

As an alternative to these traditional methods, \citet{Hyndman2011} proposed to utilize the information from all levels of the hierarchy to obtain coherent point forecasts in  a two stage process. In the first stage, the forecasts of all series are independently obtained by fitting univariate models for individual series in the hierarchy. It is very unlikely that these forecasts are coherent. Thus in the second stage, these forecasts are optimally combined through a regression model to obtain coherent forecasts. This second step is referred to as ``reconciliation'' since it takes a set of incoherent forecasts and revises them to be coherent. The approach was further improved by \citet{Wickramasuriya2017} who proposed the ``MinT'' algorithm to obtain optimally reconciled point forecasts by minimizing the mean squared coherent forecast errors. 

Traditional bottom-up, top-down and middle-out forecasting methods are not strictly reconciliation methods since they use only a part of the information from the hierarchy to produce coherent forecasts. 

Previous studies on coherent point forecasting have shown that reconciliation provides better coherent forecasts than the traditional bottom-up and top-down methods \citep{Hyndman2011,VanErven2015a,Wickramasuriya2017}. However, this idea has not been explored in the context of probabilistic forecasting. 

Point forecasts are limited because they provide no indication of forecast uncertainty. Providing prediction intervals helps, but a richer description of forecast uncertainty is obtained by estimating the entire forecast distribution. These are often called ``probabilistic forecasts'' \citep{Gneiting2014}. For example, \citet{McSharry2005} produced probabilistic forecasts for electricity demand, \citet{BenTaieb2017} for smart meter data, \citet{Pinson2009} for wind power generation, and \citet{Gel2004}, \citet{Gneiting2005a} and \citet{Gneiting2005} for various weather variables.

Although there is a rich and growing literature on producing coherent point forecasts of hierarchical time series, little  attention has been given to coherent probabilistic forecasts. The only relevant paper we are aware of is \citet{BenTaieb2017}, who recently proposed an algorithm to produce coherent probabilistic forecasts and applied it to UK electricity smart meter data. In their approach, a sample from the bottom-level forecast distribution is first generated, and then aggregated to obtain coherent probabilistic forecasts of the upper levels of the hierarchy. Hence this method is a bottom-up approach. They propose to first use the MinT algorithm to reconcile the means of the bottom-level forecast distributions, and then a copula-based approach is employed to model the dependency structure of the hierarchy. The resulting multi-dimensional distribution is used to generating empirical forecast distributions for all bottom-level series. Thus, while \citet{BenTaieb2017} provide coherent probabilistic forecasts, they do no forecast reconciliation of the distributions. In that sense, their approach is analogous to bottom-up point forecasting rather than forecast reconciliation.

After introducing our notation in Section~\ref{sec:notation}, we define what is meant by probabilistic forecast reconciliation for hierarchical time series in Section~\ref{sec:definitions}. First, we provide a new definition for coherency of point forecasts, and the reconciliation of a set of incoherent point forecasts, using  concepts related to vector spaces and measure theory. Based on these, we provide a rigorous definition for probabilistic forecast reconciliation, and how we can reconcile the incoherent forecast densities in practice. 

Further, due to the aggregation structure of the hierarchy, the probability distribution is degenerate and hence the forecast distribution should also be degenerate. In Section~\ref{sec:reconciliation}, we discuss in detail how this degeneracy will be taken care of in probabilistic forecast reconciliation, and in Section~\ref{sec:evaluation} we consider the evaluation of probabilistic hierarchical forecasts. 

Some theoretical results on probabilistic forecast reconciliation in the Gaussian framework are given in Section~\ref{sec:gaussian}, including a simulation study to show the importance of reconciliation in the probabilistic framework. 

We conclude with some thoughts on extensions and limitations in Section~\ref{sec:conclusions}.


\section{Hierarchical Time Series}\label{sec:notation}

In the section, and throughout the paper, we will try to follow notational conventions used in \citet{Wickramasuriya2017} as much as possible.  A {\em hierarchical time series} is a collection of $n$ variables where some variables are aggregates of other variables.  For example in the hierarchy depicted in Figure~\ref{fig1} below, the variable labelled $Tot$ is the sum of the series $A$
and series $B$, the series $A$ is the sum of series $AA$ and series $AB$ and the series $B$ is the sum of the series $BA$ and $BB$.  The {\em bottom level series} are defined as those $m$ variables that cannot be formed as aggregates of other variables, in the example in Figure~\ref{fig1} these are the series $AA$, $AB$, $BA$ and $BB$. 

\begin{figure}[H]
  \begin{center}
    \leaf{AA} \leaf{AB} 
    \branch{2}{A}
    \leaf{BA} \leaf{BB}
    \branch{2}{B}
    \branch{2}{Tot}
    \qobitree
  \end{center}
  \caption{Two level hierarchical diagram.}\label{fig1}
\end{figure}

We let $\bm{y}_t \in \mathbb{R}^n$ be a vector comprised of observations of all variables in the hierarchy at time $t$,  and $\bm{b}_t \in \mathbb{R}^m$ is a vector comprised of observations of all bottom-level series at time $t$. The hierarchical structure of the data imply the following holds for all $t$
\begin{equation}
  \bm{y}_t = \bm{Sb}_t,
\end{equation}
where $\bm{S}$ is an $n \times m$ constant matrix that encodes the aggregation constraints.  For the hierarchy in Figure~\ref{fig1}, $\bm{y}_t = [y_{Tot,t},y_{A,t}, y_{B,t},y_{C,t},y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}]'$, $\bm{b}_t = [y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}]'$, $m=4$, $n=7$, and 
$$ 
  \bm{S} = \begin{pmatrix} 
               1& 1 &1 &1  \\ 
               1 &1 & 0 &0 \\   
               0&0  & 1 & 1 \\ 
               & \multicolumn{2}{c}{\bm{I}_4} &   
           \end{pmatrix}, 
$$ 

where $\bm{I}_4$ is a $4\times 4$ identity matrix.  

\section{Coherent forecasts}\label{sec:definitions}

It is desirable that forecasts, whether point forecasts or probabilistic forecasts, should in some sense respect aggregation constraints.  We follow other authors in using the nomenclature {\em coherence} to describe this property.   
%While coherent point forecasts have been discussed many times previously, the definitions of coherence previously given are vague and are not easily extended to the situation of probabilistic forecasting.

We now provide new definitions for coherent forecasts in terms of vector spaces that give a geometric understanding of the problem thus facilitating the development of the probabilistic forecast reconciliation in section~\ref{sec:reconciliation}. 

\begin{definition}[Coherent subspace]\label{def:cohspace}
Let an $n$-dimensional time series $\bm{y}_t \in \mathbb{R}^n$ be subject to the linear aggregation constraint $\bm{y}_t = \bm{S}\bm{b}_t$, where $\bm{b}_t \in \mathbb{R}^m$ and $\bm{S}$ is an $n \times m$ constant matrix. The $m$-dimensional subspace $\mathfrak{s}\subset \mathbb{R}^n$ that is spanned by the columns of $\bm{S}$, i.e. $\mathfrak{s}=\mbox{span}(\bm{S})$, is defined as the {\em coherent space}. 
\end{definition}

We also denote the $n\times(n-m)$ orthogonal complement of $\bm{S}$ as $\bm{S}_{\perp}$, where $\mathfrak{s}_\perp=\mbox{span}\left(\bm{S}_{\perp}\right)$ is the nullspace of $\bm{S}$. Also at times is  will be useful to think of pre-multiplication by $\bm{S}$ as a linear mapping from $\mathbb{R}^m$ to $\mathbb{R}^n$ in which case we use the notation $s(.)$.  Although the codomain of $s(.)$ is $\mathbb{R}^n$ its image is the coherent space $\mathfrak{s}$ as depicted in Figure~\ref{fig2}.

\begin{figure}[H]
  \begin{center}
    \begin{tikzpicture}[
    >=stealth,
    bullet/.style={
      fill=black,
      circle,
      minimum width=1.5cm,
      inner sep=0pt
    },
    projection/.style={
      ->,
      thick,
      label,
      shorten <=2pt,
      shorten >=2pt
    },
    every fit/.style={
      ellipse,
      draw,
      inner sep=0pt
    }
    ]
    
    \node at (2,3) {$s$};
    
    \node at (0,5) {$\mathbb{R}^m$(domain of $s$)};
    \node at (4,5) {$\mathbb{R}^n$(codomain of $s$)};
    \node at (4.5,1.0) {$\mathfrak{s}$(image of $s$)};
    %\node[bullet,label=below:$f(x)$] at (4,2.5){};
    
    
    \draw (0,2.5) ellipse (1.02cm and 2.2cm);
    \draw (4,2.5) ellipse (1.02cm and 2.2cm);
    \draw (4,2.5) ellipse (0.51cm and 1.1cm);
    
    
    \draw[projection, label=below:$f$] (0.3,2.5) -- (3.8,2.5) ;
    
    \end{tikzpicture}
    \newline
  \end{center}
  \caption{The domain, codomain and image of the mapping $s$.}\label{fig2}
\end{figure}

\begin{definition}[Coherent Point Forecasts]\label{def:cohpoint}
  Let $\breve{\bm{y}}_{t+h|t} \in \mathbb{R}^n$ be a point forecast of the values of all series in the hierarchy at time $t+h$,  made using information up to and including time $t$.  Then $\breve{\bm{y}}_{t+h|t}$ is \emph{coherent} if $\breve{\bm{y}}_{t+h|t} \in \mathfrak{s}$. 
\end{definition}

\begin{definition}[Coherent Probabilistic Forecasts]\label{def:cohprob}
  Let $(\mathbb{R}^m, \mathscr{F}^{\mathbb{R}^m}, \nu)$ be a probability triple, where $\mathscr{F}^{\mathbb{R}^m}$ is the usual $\sigma$-algebra on $\mathbb{R}^m$. Let $\breve{\nu}$ be a probability measure on $\mathfrak{s}$ with $\sigma$-algebra $\mathscr{F}^{\mathfrak{s}}$.  Here $\mathscr{F}^{\mathfrak{s}}$ is formed as collection of sets $s(\mathcal{B})$, where $s(\mathcal{B})$ denotes the image of the set $\mathcal{B}\in \mathscr{F}^{\mathbb{R}^m}$ under the mapping $s(.)$.  Let the measure $\breve{\nu}$ have the property
  $$
      \breve{\nu}(s(\mathcal{B})) = \nu(\mathcal{B}) \quad \forall  \mathcal{B} \in \mathscr{F}^{\mathbb{R}^m},
    $$  
  A probabilistic forecast is coherent if uncertainty in $\bm{y_{t+h|h}}$ conditional on all information up to time $t$ is characterised by the probability triple $(\mathfrak{s},\mathscr{F}^{\mathfrak{s}},\breve{\nu})$.
\end{definition}

%Definition~\ref{def:cohprob} implies the probability measure on $\mathbb{C}^m$ is equivalent to the probability measure on $(\mathbb{R}^m, \bm{\mathscr{F}}^m)$. Hence, there is no density anywhere outside the linear subspace $\mathbb{C}^m$. That is, a \textit{coherent probability density forecast} is any density $\bm{f}(\breve{\bm{y}}_{t+h})$ such that $\bm{f}(\breve{\bm{y}}_{t+h})=0$ for all $\breve{\bm{y}}_{t+h} \in \bm{\mathbb{N}}^{n-m}$. 

%The following example will help to understand these definitions more clearly.

%Consider a simple hierarchy with two bottom-level series $A$ and $B$ that add up to the top level series $Tot$. Suppose the forecasts of these series at time $t+h$ are given by $\breve{\bm{y}}_{t+h} = [\breve{y}_{Tot,t+h},\breve{y}_{A,t+h}, \breve{y}_{B,t+h}]$. Due to the aggregation constraint of the hierarchy we have $\breve{y}_{Tot,t+h}=\breve{y}_{A,t+h}+\breve{y}_{B,t+h}$. This implies that, even though  $\breve{\bm{y}}_{t+h} \in \mathbb{R}^3$, the points actually lie in $\mathbb{C}^2$, which is a two dimensional subspace within that $\mathbb{R}^3$ space. Therefore, any $\breve{\bm{y}}_{t+h} \in \bm{\mathbb{N}}$ is impossible, so that $f(\breve{\bm{y}}_{t+h})=0$ for any $\breve{\bm{y}}_{t+h} \in \bm{\mathbb{N}}$.

These definitions of the coherent space $\mathfrak{s}$ and coherent point and probabilistic forecasts are defined in terms of the mapping $s(.)$ and may give the impression that the bottom level series play an important role in the definition.  However, alternative definitions could be formed using any set of basis vectors that spans $\mathfrak{s}$. For example, consider the most simple three variable hierarchy where $y_{1,t}=y_{2,t}+y_{3,t}$.  In this case the matrix $\bm{S}$ has columns  $(1,1,0)'$ and $(1,0,1)'$ spanning $\mathfrak{s}$ and premultiplying by $\bm{S}$ transforms arbitrary values of $y_{2,t}$ and $y_{3,t}$ into a coherent vector for the full hierarchy.  However the columns $(1,0,1)'$ and $(0,1,-1)'$ also span $\mathfrak{s}$ and define a mapping that transforms arbitrary values of $y_{1,t}$ and $y_{2,t}$ into a coherent vector for the full hierarchy.  The definitions above could be made in terms of any series and not just the bottom level series.  In general, we call the series (or linear combinations thereof) used in the definitions of coherence as \textit{basis series}.  Unless stated otherwise, we will always assume that the basis series are the bottom level series as in Definition~\ref{def:cohpoint} and Defintion~\ref{def:cohprob}, since this facilitates comparison with existing approaches in the literature.

%Because the basis is not unique for a given coherent subspace, Definition~\ref{def:cohprob} is not unique, and one can redefine the coherent probabilistic forecasts with respect to any basis. However, we stick to Definition~\ref{def:cohprob} and consider the basis defined by the columns of $\bm{S}$ in what follows.

%Definitions~\ref{def:cohpoint} and~\ref{def:cohprob} facilitate extension to probabilistic forecast reconciliation which we discuss in the next section. 
To  the best of our knowledge, the only other definition of coherent probabilistic forecasts is given by \citet{BenTaieb2017} who define coherent probabilistic forecasts in terms of convolutions. According to their definition, probabilistic forecasts are coherent when a convolution of forecast distributions of disaggregate series is identical to the forecast distribution of the corresponding aggregate series.  Their definition is consistent with our definition, our reason for providing a different definition is that the geometric understanding of coherence will facilitate our definitions of point and probabilistic forecast reconciliation to which we now turn our attention.

\section{Forecast reconciliation}\label{sec:reconciliation}

Initially we define point forecast reconciliation, before extending the idea to the probabilistic setting.

\subsection{Point forecast reconciliation}


  Let $\hat{\bm{y}}_{t+h|t} \in \mathbb{R}^n$ be any set of incoherent point forecasts at time $t+h$ using information up to and including time $t$.  Let $\bm{G}$ be an $m\times n$ matrix and $g:\mathbb{R}^n \rightarrow \mathbb{R}^m$ be the mapping corresponding to pre-multiplication by $\bm{G}$.
  \begin{definition}\label{def:reconpoint}
  The point forecast $\tilde{\bm{y}}_{t+h|t}$ ``reconciles'' $\hat{\bm{y}}_{t+h|t}$ with respect to the mapping $g(.)$ iff
  \begin{equation}
    \tilde{\bm{y}}_{t+h|t}=\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t}\,.
  \end{equation} 
    
\end{definition}

In definition~\ref{def:reconpoint}  $\bm{S}\bm{G}$ is a projection matrix that maps $\mathbb{R}^n$ onto $\mathfrak{s}$. We note that a similar characterisation of reconciliation has been used in previous studies but where the notation $\bm{P}$ is used in place of $\bm{G}$. In principle the definition can be generalised where $g(.)$ is a non-linear mapping, a possibility we consider in Section~\ref{nonlinear} {\color{red}[AP: I WILL ADD THIS LATER]}.

Definition~\ref{def:reconpoint}, defines an entire family of forecast reconciliation methods including many methods currently extant in the literature. To provide a geometric interpretation, we introduce an $n \times (n-m)$ matrix $\bm{R}$ whose columns span the null space $\mathfrak{s}_{\perp}$.  For example, a straightforward choice of $\bm{R}$ for the most simple three variable hierarchy where $y_{1,t}=y_{2,t}+y_{3,t}$, is the vector $(1,-1,-1)$ which is orthogonal (in the Euclidean sense) to the columns of $\bm{S}$.  In this case, the matrix $\bm{R}$ can be interpreted as a `restrictions' matrix since it has the property that $\bm{R}'\bm{y}=\bm{0}$ for coherent $\bm{y}$. In general, the columns of any given $\bm{R}$ matrix can also be thought of as `directions' along which incoherent point forecasts are projected onto the coherent space, and these directions need not be orthogonal to $\mathfrak{s}$.  A schematic of this geometric interpretation is provided in Figure~\ref{fig:pointfr_sch}. 
%$$ 
%\bm{S} = 
%\begin{pmatrix} 
%1& 1 \\ 1 & 0 \\ 0&1 
%\end{pmatrix} 
%\quad \text{and} \quad 
%\bm{R} = 
%\begin{pmatrix}  
%1 \\ -1 \\ -1 
%\end{pmatrix}.
%$$ 
%
%Further let $\{\bm{s}_1,\dots,\bm{s}_m\}$ and $\{\bm{r}_1,\dots,\bm{r}_{n-m}\}$ denote the columns of $\bm{S}$ and $\bm{R}$ respectively. Then $\bm{B}=\{\bm{s}_1,\dots,\bm{s}_m, \bm{r}_1,\dots,\bm{r}_{n-m}\}$ is a basis for $\mathbb{R}^n$. Now, using the insights of Definition~\ref{def:reconpoint}, we can use the following steps to reconcile the point forecasts.

%\subsubsection*{Step 1: Change of basis}

\begin{figure}
	\input{Figs/pointforerec_schematic.tex}
	\caption{Summary of probabilistic point reconciliation.  The mapping $G(.)$ projects the unreconciled forecast $\hat{\bm y}$ onto   Note that since the smallest hierarchy involves three dimensions, this figure is only a schematic}\label{fig:pointfr_sch} 
\end{figure}

To illustrate further note that the columns of $\bm{S}$ and $\bm{R}$ provide a basis for $\mathbb{R}^n$.  As such any incoherent set of point forecasts $\hat{\bm{y}}_{t+h|t} \in \mathbb{R}^n$, can be expressed in terms of coordinates in the basis defined by $\bm{S}$ and $\bm{R}$. Let $\tilde{\bm{b}}_{t+h|t}$ and $\tilde{\bm{a}}_{t+h|t}$ be the coordinates corresponding to $\bm{S}$ and $\bm{R}$, respectively. The process of reconciliation involves setting $\tilde{\bm{b}}_{t+h|t}$ to be the values of the reconciled bottom-level series and setting $\tilde{\bm{a}}_{t+h}=\bm{0}$ to ensure coherence. From properties of linear algebra it follows that 
%\[
%(\bm{S} ~ \vdots~ \bm{R})
%(\tilde{\bm{b}}'_{t+h},~ \tilde{\bm{t}}'_{t+h})'
%= \hat{\bm{y}}_{t+h},
%\]
\[
\hat{\bm{y}}_{t+h|t} = (\bm{S} ~ \bm{R})
\begin{pmatrix}
\tilde{\bm{b}}_{t+h|t}\\ \tilde{\bm{a}}_{t+h|t}
\end{pmatrix}= \bm{S}\tilde{\bm{b}}_{t+h|t} +  \bm{R}\tilde{\bm{a}}_{t+h|t},
\]
while setting $\tilde{\bm{a}}_{t+h|t}=\bm{0}$ gives the reconciled point forecast
\[
\tilde{\bm{y}}_{t+h|t} = \bm{S}\tilde{\bm{b}}_{t+h|t}
\]

%\begin{equation}\label{4.3}
%(\tilde{\bm{b}}'_{t+h}, ~ \tilde{\bm{t}}'_{t+h})' =
%(\bm{S} ~ \vdots~ \bm{R})^{-1}
%\hat{\bm{y}}_{t+h}.
%\end{equation}

In order to find $\tilde{\bm{b}}_{t+h|t}$ we require the inverse $(\bm{S} ~ \bm{R})^{-1}$ which is given by
\begin{equation}
(\bm{S} ~  \bm{R})^{-1} = \begin{pmatrix}
(\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot \\ (\bm{S}'_\bot \bm{R})^{-1}\bm{S}'_\bot
\end{pmatrix}\,,
\end{equation}
where $\bm{S}_{\bot}$ and $\bm{R}_{\bot}$ be the orthogonal complements of $\bm{S}$ and $\bm{R}$
respectively. Thus it follows that $\tilde{\bm{b}}_{t+h}=(\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot \hat{\bm{y}}_{t+h}$ and $\tilde{\bm{y}}_{t+h|t}=\bm{S}(\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot \hat{\bm{y}}_{t+h|t}$.  Here $(\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot$ is equivalent to $\bm{G}$ in Definition~\ref{def:reconpoint}.

Point reconciliation methods will always minimise the distance between unreconciled and reconciled forecasts, however the specific distance will depend on the choice of ${\bm R}$.  For example \cite{Hyndman2011} consider $\tilde{\bm{y}}^{OLS}_{t+h}=\bm{S}(\bm{S}' \bm{S})^{-1}\bm{S}' \hat{\bm{y}}_{t+h}$ which minimises the Euclidean distance between $\hat{\bm y}$ and $\tilde{\bm y}$. \cite{Wickramasuriya2017} consider $\tilde{\bm{y}}^{MinT}_{t+h}=\bm{S}(\bm{S}' \bm{W}^{-1}_{h}\bm{S})^{-1}\bm{S}'\bm{W}^{-1}_{h} \hat{\bm{y}}_{t+h}$, where ${\bm W}$ is an estimate of the variance covariance matrix of the unreconciled errors.  This 
minimises the Mahalonobis distance between $\hat{\bm y}$ and $\tilde{\bm y}$.  Bottom up methods minimise distance between reconciled and unreconciled forecasts only along dimensions corresponding to the bottom level series.  As such bottom up methods are at should be thought of as a boundary case of reconciliation methods, since they ultimately do not use information at all levels of the hierarchy. 

%\begin{equation} \label{4.4}
%\begin{pmatrix}
%\tilde{\bm{b}}_{t+h} \\ \cdots \\ \tilde{\bm{t}}_{t+h}
%\end{pmatrix} = \begin{pmatrix}
%(\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot \\ \cdots \\ (\bm{S}'_\bot \bm{R})^{-1}\bm{S}'_\bot
%\end{pmatrix}\hat{\bm{y}}_{t+h}.
%\end{equation}
%From \eqref{4.4} it follows that
%\begin{equation}
%\tilde{\bm{b}}_{t+h}=(\bm{R}'_\bot %\bm{S})^{-1}\bm{R}'_\bot \hat{\bm{y}}_{t+h}
%\end{equation}

%\subsubsection*{Step 2: Obtaining reconciled point forecasts for the whole hierarchy}
%
%This step directly follows from the definition for coherent forecasts. To obtain reconciled point forecasts for the entire hierarchy, we map $\tilde{\bm{b}}_{t+h} \in \mathbb{R}^n$ to the $\mathbb{C}^m$ through $\bm{S}$. Thus we have, 
%\begin{equation}
%\tilde{\bm{y}}_{t+h}=\bm{S}(\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot \hat{\bm{y}}_{t+h}, \qquad \tilde{\bm{y}}_{t+h} \in \mathbb{C}^m<\mathbb{R}^n.
%\end{equation}

%Finding a suitable $\bm{R}$ with respect to a certain loss function will lead to optimally reconciled point forecasts of the hierarchy. %If $\bm{P}=(\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot$, then the definition for linear reconciliation of point forecasts in previous studies coincides with our explanation.


%In our context, we need to find $\bm{R}$ such that $\bm{R}'_\bot \bm{S}$ is invertible; i.e, $(\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot \bm{S}=\bm{I}$. This condition coincides with the unbiased condition $\bm{SPS}=\bm{S}$ proposed by \citet{Hyndman2011}

%\citet{Hyndman2011} proposed to choose
%\begin{equation}
%\tilde{\bm{b}}^{OLS}_{t+h}=(\bm{S}' \bm{S})^{-1}\bm{S}' \hat{\bm{y}}_{t+h},
%\end{equation}
%where, in this context, $\bm{R}'_\bot = \bm{S}'$. Thus the reconciled point forecasts for the entire hierarchy are given by
%\begin{equation}
%\tilde{\bm{y}}^{OLS}_{t+h}=\bm{S}(\bm{S}' \bm{S})^{-1}\bm{S}' \hat{\bm{y}}_{t+h}.
%\end{equation}
%They referred this to as the OLS solution and the loss function they considered is equivalent to the Euclidean norm between $\hat{\bm{y}}_{t+h}$ and $\tilde{\bm{y}}_{t+h}$; i.e. $<\hat{\bm{y}}_{T+h}, \tilde{\bm{y}}_{T+h}>$.
%
%According to a recent study by \citet{Wickramasuriya2017}, selecting $\bm{R}'_\bot = \bm{S}'\bm{W}^{-1}_{h}$ will minimize the trace of mean squared reconciled forecast errors under the property of unbiasedness, where $\bm{W}^{-1}_{h}$ is the variance of the incoherent forecast errors. This will result in
%\begin{equation}
%\tilde{\bm{b}}^{MinT}_{t+h}=(\bm{S}'\bm{W}^{-1}_{h} \bm{S})^{-1}\bm{S}'\bm{W}^{-1}_{h} \hat{\bm{y}}_{t+h},
%\end{equation}
%and thus,
%\begin{equation}
%\tilde{\bm{y}}^{MinT}_{t+h}=\bm{S}(\bm{S}' \bm{W}^{-1}_{h}\bm{S})^{-1}\bm{S}'\bm{W}^{-1}_{h} \hat{\bm{y}}_{t+h}.
%\end{equation}
%
%They referred this to as the MinT solution. The loss function they considered is equivalent to the Mahalanobis distance between $\hat{\bm{y}}_{T+h}$ and $\tilde{\bm{y}}_{T+h}$. i.e. $<\hat{\bm{y}}_{T+h}, \tilde{\bm{y}}_{T+h}>_{\bm{W}_h}$.

\subsection{Probabilistic forecast reconciliation}

We now extend the methodology of point forecast reconciliation to probabilistic forecasts

%For probabilistic forecasts, reconciliation implies finding the probability measure of the coherent forecasts using the information from an incoherent probabilistic forecast measure. A more formal definition is given below.

  Let $(\mathbb{R}^n, \mathscr{F}^{\mathbb{R}^n}, \hat{\nu})$ be an probability triple, that is not necessarily coherent and that characterises forecast uncertainty for all variables in the hierarchy at time $t+h$ conditional on all information up to time $t$.  Let $g:\mathbb{R}^n \rightarrow \mathbb{R}^m $ be a linear mapping.  Let $(\mathbb{R}^m, \mathscr{F}^{\mathbb{R}^m}, \nu)$ be a probability triple defined on $\mathbb{R}^m$ where $\nu$ has the following property:
  \begin{equation}
  \nu(\mathcal{B}) = \hat{\nu}(g^{-1}(\mathcal{B})), \quad \forall \mathcal{B} \in \mathscr{F}^{\mathbb{R}^m}\,.
  \end{equation}
  Here $g^{-1}(\mathcal{B}):=\left\{\breve{\bm{y}}\in \mathbb{R}^n:g(\breve{\bm{y}})\in \mathcal{B} \right\}$ is the pre-image of $\mathcal{B}$, that is the set of all points in $\mathbb{R}^n$ that $g(.)$ maps to a point in $\mathcal{B}$.
  \begin{definition} \label{def:reconprob}
  We define the reconciled probability measure of $\hat{\nu}$ with respect to the mapping $g(.)$ as a probability measure $\tilde{\nu}$ on $\mathfrak{s}$ with $\sigma$-algebra $\mathscr{F}_\mathfrak{s}$ where the following holds
  \begin{equation}
  \tilde{\nu}(s(\mathcal{B})) = \nu(\mathcal{B})= \hat{\nu}(g^{-1}(\mathcal{B})) \qquad \forall \mathcal{B} \in \mathscr{F}^{\mathbb{R}^m}\,.
  \end{equation}
\end{definition}

This definition extends the notion of forecast reconciliation to the probabilistic setting.   Under point reconciliation methods, the reconciled point forecast is equal to the unreconciled point forecast after the latter is passed through two linear mappings. Similarly, probabilistic forecast reconciliation assigns the same probability to two sets where the points in one set are obtained by passing all points in the other set through two linear mappings.  This is depicted schematically in Figure~\ref{fig:probfr_sch}.

\begin{figure}
	\input{Figs/probforerec_schematic.tex}
	\caption{Summary of probabilistic forecast reconciliation.  The probability that ${\bm y}_{t+h}$ lies in the red line segment under the reconciled probabilistic forecast is defined to be equal to the probability that ${\bm y}_{t+h}$ lies in the shaded blue area under the unreconciled probabilistic forecast.  Note that since the smallest hierarchy involves three dimensions, this figure is only a schematic}\label{fig:probfr_sch} 
\end{figure}

We now discuss how this definition can be used in practice to obtain reconciled probabilistic forecasts for hierarchical time series.   Recall that the case of forecast reconciliation could be broken down into three steps.  In the first, $\hat{\bm{y}}_{t+h|t}$ is transformed into coordinates $\tilde{\bm{b}}_{t+h|t}$ and $\tilde{\bm{a}}_{t+h|t}$ via a change of basis.  In the second, $\tilde{\bm{a}}_{t+h|t}$ is discarded and $\tilde{\bm{b}}_{t+h|t}$ are kept as the bottom level reconciled forecasts.  In the third, reconciled forecasts for the entire hierarchy are recovered via $\tilde{\bm{y}}_{t+h|t}=\bm{S}\tilde{\bm{b}}_{t+h|t}$.  We now outline these three steps for probabilistic forecasts and demonstrate how they correspond to Definition~\ref{def:reconprob}.

While $\hat{\nu}$ is a probability measure for an $n$-vector $\hat{\bm{y}}_{t+h|t}$,  probability statements in terms of a different coordinate system can be made via an appropriate change of basis. Letting $f(.)$ be generic notation for a probability density functions and following the notation from our definition of point forecast reconciliation where $\hat{\bm{y}}=\bm{S}\tilde{\bm{b}}_{t+h|t}+\bm{R}\tilde{\bm{a}}_{t+h|t}$ we obtain

\begin{equation}
f(\hat{\bm{y}}_{t+h|t})=f(\bm{S}\tilde{\bm{b}}_{t+h|t}+\bm{R}\tilde{\bm{a}}_{t+h|t})|\left(\bm{S}~\bm{R}\right)|
\end{equation}

The expression $\hat{\nu}(g^{-1}(\mathcal{B}))$ in Definition~\ref{def:reconprob} is equivalent to the probability statement $\mbox{Pr}(\hat{\bm{y}}_{t+h|t}\in g^{-1}(\mathcal{B}))$.  After the change of basis this is equivalent to $\mbox{Pr}(\tilde{\bm{b}}\in \mathcal{B})$ which implies

\begin{align}
\mbox{Pr}(\hat{\bm{y}}_{t+h|t}\in g^{-1}(\mathcal{B}))&=\int\limits_{g^{-1}(\mathcal{B})}f(\hat{\bm{y}}_{t+h|t})d\hat{\bm{y}}_{t+h|t}\\
&=\int\limits_{\mathcal{B}}\int f(\bm{S}\tilde{\bm{b}}_{t+h|t}+\bm{R}\tilde{\bm{a}}_{t+h|t})|\left(\bm{S}~\bm{R}\right)|d\tilde{\bm{a}}_{t+h|t}d\tilde{\bm{b}}_{t+h|t}
\end{align}

After integrating out over $\tilde{\bm{a}}_{t+h|t}$, a step analogous to setting $\tilde{\bm{a}}_{t+h|t}=0$ for point forecasting, we obtain an expressions that gives the probability the reconciled bottom level series lies in the region $\mathcal{B}$. This corresponds to $\nu(\mathcal{B})$ in Definition~\ref{def:reconprob}.  To make a valid probability statement about the entire hierarchy we simply use the bottom level probabilistic forecasts together with Definition~\ref{def:cohprob}. 

%Recall that $\hat{\bm{y}}_{t+h}$ is a set of incoherent point forecasts and the coordinates of $\hat{\bm{y}}_{t+h}$ with respect to the basis $\bm{B}$ are given by \eqref{4.3}. Suppose $\hat{\bm{f}}(\cdot)$ is the probability density of $\hat{\bm{y}}_{t+h}$. Our goal is to reconcile $\hat{\bm{f}}(\cdot)$ such that the density lives on $\mathbb{C}^m$. In order to obtain this reconciled density, we need to project $\hat{\bm{f}}(\hat{\bm{y}}_{t+h})$ onto $\mathbb{C}^m$ along the direction of $\mathbb{N}^{n-m}$.

%Let the density of $\hat{\bm{y}}_{t+h}$ with respect to basis $\bm{B}$ be denoted by $\bm{f_B}(\cdot)$. Then it follows from  \eqref{4.3}, and standard results for densities of transformed variables, that
%\begin{equation}\label{4.5}
%\bm{f_B}(\tilde{\bm{b}}_{t+h},\tilde{\bm{t}}_{t+h})=\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) \,\Big|\bm{S} ~ \vdots~ \bm{R}\Big|,
%\end{equation}
%where $|\cdot|$ denotes the determinant of a matrix. Now that we have the density of $(\tilde{\bm{b}}'_{t+h} , \tilde{\bm{t}}'_{t+h})'$, the marginal density of $\tilde{\bm{b}}_{t+h}$ can be obtained by integrating \eqref{4.5} over the range of $\tilde{\bm{t}}_{t+h}$. This will result in the reconciled density of the bottom-level series $\tilde{\bm{b}}_{t+h}$,
%\begin{equation}\label{4.6}
%\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})=\int_{\lim(\tilde{\bm{t}}_{t+h})}\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h})\,  \Big|\bm{S} ~ \vdots~ \bm{R}\Big| \, d\tilde{\bm{t}}_{t+h}.
%\end{equation}
%
%Finally to get the reconciled density of the whole hierarchy, we simply follow Definition~\ref{def:cohprob} to obtain
%\begin{equation}\label{4.7}
%\tilde{\bm{f}}(\tilde{\bm{y}}_{t+h})=\bm{S}\circ \tilde{\bm{f}}(\tilde{\bm{b}}_{t+h}).
%\end{equation}
%This final step will transform every point in the density $\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})$ to the space $\mathbb{C}^m<\mathbb{R}^n$. The following example illustrates how this method can be used to reconcile an incoherent Gaussian forecast distribution.

\subsubsection*{Example: Gaussian Distributions}

Suppose an unreconciled probabilistic forecast is Gaussian with mean $\hat{\bm \mu}$ and variance-covariance matrix $\hat{\bm \Sigma}$.  The subscripts $t+h|t$ are suppressed for brevity.  The unreconciled density

\begin{equation}
  f(\hat{\bm{y}})=(2\pi)^{-n/2}|\hat{\bm{\Sigma}}|^{-1/2}\exp\left\{-\frac{1}{2}\left[(\hat{\bm y}-\hat{\bm \mu})'\hat{\bm{\Sigma}}^{-1}(\hat{\bm y}-\hat{\bm \mu})\right]\right\}
\end{equation}

After a change in basis

\begin{equation}
f(\tilde{\bm b},\tilde{\bm a})=(2\pi)^{-\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{-\frac{1}{2}}\Big|(\bm{S} ~  \bm{R})\Big|\exp\left\{-\frac{1}{2}q\right\}\,,
\end{equation}
where
\begin{equation}
q=(\bm{S}\tilde{\bm{b}}+\bm{R}\tilde{\bm{a}}-\bm{\hat{\mu}})' \bm{\hat{\Sigma}}^{-1}(\bm{S}\tilde{\bm{b}}+\bm{R}\tilde{\bm{a}}-\bm{\hat{\mu}})
\end{equation}


The quadratic form $q$ can be rearranged as
\begin{align*}
q& = 
\left((\bm{S} ~  \bm{R})\bt-\bm{\hat{\mu}}\right)' \bm{\hat{\Sigma}}^{-1}\left((\bm{S} ~ \bm{R})\bt-\bm{\hat{\mu}}\right),\\
& =
\left(\bt-(\bm{S} ~ \bm{R})^{-1}\bm{\hat{\mu}}_{t+h}\right)' \Big[(\bm{S}  \bm{R})\bm{\hat{\Sigma}_{t+h}}(\bm{S} ~ \bm{R})'\Big]^{-1}
\left(\bt-(\bm{S} ~ \bm{R})^{-1}\bm{\hat{\mu}}_{t+h}\right)\,.
\end{align*}

Recall that
$$
(\bm{S} ~ \bm{R})^{-1} =
\begin{pmatrix}(\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot  \\ (\bm{S}'_\bot \bm{R})^{-1}\bm{S}'_\bot \end{pmatrix} :=
\begin{pmatrix}
\bm{G} \\\bm{H}
\end{pmatrix}\,.
$$
Then $q$ can be rearranged further as
\begin{align*}
q& =%\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|\PQ \Big|}
%\exp \Big\{-\frac{1}{2}
\left[\bt-\PQ\bm{\hat{\mu}}_{t+h}\right]'%\\[-0.5cm]
\left[\PQ\bm{\hat{\Sigma}_{t+h}}\PQ'\right]^{-1}\left[\bt-\PQ\bm{\hat{\mu}}_{t+h}\right] %\Big\},
\\[0.5cm]
 & =%\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\PQ\bm{\hat{\Sigma}_{t+h}}\PQ'\Big|^{\frac{1}{2}}}
%\exp \Big\{-\frac{1}{2} 
\begin{pmatrix}\tilde{\bm{b}} - \bm{G}\bm{\hat{\mu}}\\ \tilde{\bm{a}}- \bm{H}\bm{\hat{\mu}}\end{pmatrix}' %&\\[-0.5cm]
%& \hspace*{7cm}
 \left[\PQ\bm{\hat{\Sigma}_{t+h}}\PQ'\right]^{-1}\begin{pmatrix}\tilde{\bm{b}} - \bm{G}\bm{\hat{\mu}}\\ \tilde{\bm{a}}- \bm{H}\bm{\hat{\mu}}\end{pmatrix} %\Big\}.
\end{align*}
%Since $\Big[\PQ\bm{\hat{\Sigma}_{t+h}}\PQ'\Big] = \begin{pmatrix}
%\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}' & \bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{Q}' 
%\bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{P}' & \bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{Q}'
%\end{pmatrix}$ we have

Similar manipulations on determinant of the covariance matrix lead to the following expression for the density

\begin{align*}
f(\tilde{\bm{b}},\tilde{\bm{a}})&
=(2\pi)^{-\frac{n}{2}}\left|
\begin{pmatrix}
  \bm{G}\bm{\hat{\Sigma}}\bm{G}' & \bm{G}\bm{\hat{\Sigma}}\bm{H}' \\
  \bm{H}\bm{\hat{\Sigma}}\bm{G}' & \bm{H}\bm{\hat{\Sigma}}\bm{H}'
\end{pmatrix}
\right|^{-\frac{1}{2}}
\exp \left\{-\frac{1}{2} \begin{pmatrix}\tilde{\bm{b}} - \bm{G}\bm{\hat{\mu}}\\ \tilde{\bm{a}}- \bm{H}\bm{\hat{\mu}}\end{pmatrix}'\right.\\
&\hspace*{7cm}
\left.\begin{pmatrix}
\bm{G}\bm{\hat{\Sigma}}\bm{G}' & \bm{G}\bm{\hat{\Sigma}}\bm{H}' \\
\bm{H}\bm{\hat{\Sigma}}\bm{G}' & \bm{H}\bm{\hat{\Sigma}}\bm{H}'
\end{pmatrix}^{-1}
\begin{pmatrix}\tilde{\bm{b}} - \bm{G}\bm{\hat{\mu}}\\ \tilde{\bm{a}}- \bm{H}\bm{\hat{\mu}}\end{pmatrix} \right\}.
\end{align*}
Marginalising out $\tilde{\bm{a}}$, leads to the following bottom level reconciled forecasts. 
\begin{equation}\label{ex:2.1}
f(\tilde{\bm{b}})=(2\pi)^{-\frac{m}{2}}\Big|\bm{G}\bm{\hat{\Sigma}}\bm{G}'\Big|^{\frac{1}{2}}
\exp \Big\{-\frac{1}{2} (\tilde{\bm{b}} - \bm{G}\bm{\hat{\mu}})' (\bm{G}\bm{\hat{\Sigma}}\bm{G}')^{-1}(\tilde{\bm{b}} - \bm{G}\bm{\hat{\mu}}) \Big\}.
\end{equation}

%Equation \eqref{ex:2.1} implies $\tilde{\bm{b}}_{t+h} \sim \mathcal{N}(\bm{P}\bm{\hat{\mu}}_{t+h}, \bm{P}\hat{\bm{\Sigma}}_{t+h}\bm{P}')$, where $\bm{P} = (\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot$. Then from \eqref{4.7} it follows that
%\begin{equation}\label{eq:gaussianreconciled}
%\tilde{\bm{f}}(\tilde{\bm{y}}_{t+h})=\tilde{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}).
%\end{equation}
Which implies that the reconciled probabilistic forecast for the bottom level series is $\tilde{\bm{b}}_{t+h} \sim \mathcal{N}(\bm{G}\bm{\hat{\mu}}_{t+h}, \bm{G}\hat{\bm{\Sigma}}_{t+h}\bm{G}')$.  The reconciled probabilistic forecasts for the whole hierarchy follow a degenerate Gaussian distribution with mean  $\bm{SG}\bm{\hat{\mu}}$ and rank deficient covariance matrix $\bm{SG}\hat{\bm{\Sigma}}_{t+h}\bm{G}'\bm{S}'$.
\subsection{Elliptical Distributions}
We now show that if the mapping $g(.)$ in definition $\ref{def:reconpoint}$ is extended to allow for a translation by ${\bm d}$, i.e. $g({\bm y})={\bm G}{\bm y}+{\bm d}$, then the true predictive distribution can be recovered for elliptical distributions.  Here, for any square matrix ${\bm C}$, ${\bm C}^{1/2}$ and ${\bm C}^{-1/2}$ are defined to satisfy ${\bm C}^{1/2}\left({\bm C}^{1/2}\right)'={\bm C}$ and ${\bm C}^{-1/2}\left({\bm C}^{-1/2}\right)'={\bm C}^{-1}$, for example ${\bm C}^{1/2}$ may be obtained via the Cholesky or eigenvalue decompositions. 
\begin{theo}[Reconciliation for Elliptical Distributions]
	Let an unreconciled probabilistic forecast come from the elliptical class with location parameter $\hat{\bm \mu}$ and scale matrix $\hat{\bm \Sigma}$.  Let the true predictive distribution of ${\bm y}_{t+h|t}$ also belong to the elliptical class with location parameter ${\bm \mu}$ and scale matrix ${\bm \Sigma}$.  Then the affine reconciliation mapping $g(\breve{\bm y})={\bm G}_{opt}\breve{\bm y}+{\bm d}_{opt}$ with $\bm{G}_{opt}={\bm\Omega}^{1/2}{\bm\Sigma}^{-1/2}$ and ${\bm d}_{opt}={\bm S}{\bm G}_{opt}\left({\bm \mu}-\hat{\bm \mu}\right)$ recovers the true predictive density, where $\bm\Omega$ is the true variance covariance matrix of the predictive distribution for the bottom level. 
\end{theo}
\begin{proof}
   Since elliptical distributions are closed under affine transformations, and are closed under marginalisation, reconciliation of an elliptical distribution yields an elliptical distribution (although the unreconciled and unreconciled distributions may be different members of the class of elliptical distributions).  The scale matrix of the reconciled forecast is given by $\bm{S}\bm{G}_{opt}{\bm \Sigma}{\bm G}_{opt}'{\bm S}'$ while the location matrix is given by $\bm{S}\bm{G}_{opt}\hat{\bm \mu}+{\bm d}_{opt}$.  The reconciled scale matrix is 
   \begin{align*}
   \tilde{\bm \Sigma}_{opt}=&{\bm S}{\bm\Omega}^{1/2}{\bm\Sigma}^{-1/2}{\bm \Sigma}{\bm\Sigma}^{-1/2}{\bm\Omega}^{1/2}{\bm S}'\\
   =&{\bm S}{\bm \Omega}{\bm S}'\\
   =&{\bm \Sigma}
   \end{align*}
   For the choices of ${\bf G}_{opt}$ and ${\bf d}_{opt}$ given above, the reconciled location matrix is 
   \begin{align*}
   \tilde{\bm \mu}_{opt}=&{\bm S}{\bm G}_{opt}\hat{\bm \mu}+{\bm S}{\bm G}_{opt}\left({\bm \mu}-\hat{\bm \mu}\right)\\
   =&{\bm S}{\bm G}_{opt}\hat{\bm \mu}-{\bm S}{\bm G}_{opt}\hat{\bm \mu}+{\bm S}{\bm G}_{opt}{\bm \mu}\\
   =&{\bm S}{\bm G}_{opt}{\bm \mu}\\
   =&{\bm \mu}\\
   \end{align*}
   The final equality above holds since ${\bm S}{\bm G}_{opt}$ projects any vector onto the coherent subspace, however since ${\bm \mu}$ is the mean of the true data generating process, it must already lie on the coherent subspace and ${\bm S}{\bm G}_{opt}{\bm \mu}={\bm \mu}$.
\end{proof}	
It should be noted that ${\bm G}_{opt}$ and ${\bm d}_{opt}$ depend on the true location and scale of the predictive distribution and are thus infeasible in practice.  However, these expressions do provide some insight on why some choices for ${\bm G}$ in the point forecast reconciliation literature may also work well for probabilistic forecasts.  To illustrate, first rearrange the equation for the optimal value of  $\bm{G}_{opt}={\bm\Omega}^{1/2}\hat{\bm\Sigma}^{-1/2}$ as ${\bm\Omega}^{1/2}=\bm{G}_{opt}\hat{\bm\Sigma}^{1/2}$. For arbitrary ${\bm G}\hat{\bm\Sigma}^{1/2}$ is an approximation for ${\bm\Omega}^{1/2}$.  This approximation has similarities with the way bottom level estimates are produced for point forecast reconciliation.  Rather than mapping a vector of point forecasts to reconcilied bottom level forecasts, the columns of  $\hat{\bm\Sigma}^{1/2}$ are mapped to an approximation of the columns of ${\bm\Omega}^{1/2}$.  The value for $\tilde{\bm \mu}_{opt}$ includes a projection of $\hat{\bm \mu}$ onto $\mathfrak{s}$. While it is infeasible to obtain the translation ${\bm d}$, it is worthwhile noting that this measures the difference between the reconciled mean and true mean. 

\section{Evaluation of hierarchical probabilistic forecasts}\label{sec:evaluation}

The necessary final step in hierarchical forecasting is to make sure that our forecast distributions are accurately predicting the uncertain future. In general, forecasters prefer to maximize the sharpness of the forecast distribution subject to the calibration \citep{Gneiting2014}. Therefore the probabilistic forecasts should be evaluated with respect to these two properties.

Calibration refers to the statistical compatibility between probabilistic forecasts and realizations. In other words, random draws from a perfectly calibrated forecast distribution should be equivalent in distribution to the realizations. On the other hand, sharpness refers to the spread or the concentration of the prediction distributions and it is a property of the forecasts only. The more concentrated the forecast distributions, the sharper the forecasts \citep{Gneiting2008}. However, independently assessing the calibration and sharpness will not help to properly evaluate the probabilistic forecasts. Therefore we need to assess these properties simultaneously using scoring rules.

Scoring rules are summary measures obtained based on the relationship between the forecast distributions and the realizations. In some studies, researchers take the scoring rules to be positively oriented, in which case the scores should be maximized \citep{Gneiting2007}. However, scoring rules have also been defined to be negatively oriented, and then the scores should be minimized \citep{Gneiting2014}. We consider negatively oriented scoring rules to evaluate probabilistic forecasts in hierarchical time series.

Let $\breve{\bm{Y}}$ and $\bm{Y}$ be $n$-dimensional random vectors from the forecast distribution $\bm{F}$ and the true distribution $\bm{G}$, respectively. Further let $\bm{y}$ be an $n$-dimensional realization from $\bm{G}$. Then a scoring rule is a numerical value $S(\breve{\bm{Y}},\bm{y})$ assigned to each pair $(\breve{\bm{Y}},\bm{y})$. It is a ``proper'' scoring rule if 
\begin{equation}\label{eq:(3.1.)}
\E_{\bm{G}}[S(\bm{Y},\bm{y})] \le \E_{\bm{G}}[S(\breve{\bm{Y}},\bm{y})],
\end{equation}
where $\E_{\bm{G}}[S(\bm{Y,y})]$ is the expected score under the true distribution $\bm{G}$ \citep{Gneiting2008, Gneiting2014}.

Table~\ref{table:scoringrules} summarizes a few existing proper scoring rules.

\begin{table}[!bh]
  \caption{Scoring rules to evaluate multivariate forecast densities. Here, $\breve{\bm{y}}_{T+h}$ and $\breve{\bm{y}}^*_{T+h}$ are two independent random vectors from the coherent forecast distribution $\breve{\bm{F}}$ with density function $\breve{\bm{f}}(\cdot)$ at time $T+h$, and $\bm{y}_{T+h}$ is the vector of realizations. Further, $\breve{Y}_{T+h,i}$ and $\breve{Y}_{T+h,j}$ are the $i$th and $j$th components of the vector $\breve{\bm{Y}}_{T+h}$. The variogram score is given for order $p$, where $w_{ij}$ denote non-negative weights.}\label{table:scoringrules}
  \centering\small\setstretch{1.3}
  \begin{tabular}{@{}lp{8.1cm}l@{}}
    \toprule
    \textbf{Scoring rule}  & \textbf{Expression} & \textbf{Reference}           \\
    \midrule
    \text{Log score}       &
    $\text{LS}(\breve{\bm{F}},\bm{y}_{T+h}) = -\log {\breve{\bm{f}}(\bm{y}_{T+h})}$ &
    \citet{Gneiting2007}  \\\\[-0.2cm]
    \text{Energy score}    &
    $\text{ES}(\breve{\bm{Y}}_{T+h},\bm{y}_{T+h}) =
    \E_{\breve{\bm{F}}}
    \|\breve{\bm{Y}}_{T+h}-\bm{y}_{T+h}\|^\alpha -$ \par\hfill
    $\frac{1}{2}\E_{\breve{\bm{F}}}\|\breve{\bm{Y}}_{T+h}-\breve{\bm{Y}}^*_{T+h}\|^\alpha$, \,\, $\alpha \in (0,2]$ &
    \citet{Gneiting2008}  \\\\[-0.2cm]
    \text{Variogram score} &
    $\text{VS}(\breve{\bm{F}}, \bm{y}_{T+h}) =
    \sum\limits_{i=1}^{n}
    \sum\limits_{j=1}^{n}
    w_{ij}\Big(|y_{T+h,i} - y_{T+h,j}|^p -$ \par\hfill
    $\E_{\breve{\bm{F}}}|\breve{Y}_{T+h,i}-\breve{Y}_{T+h,j}|^p\Big)^2$     &
    \citet{SCHEUERER2015} \\
    \bottomrule
  \end{tabular}
\end{table}

Even though the log score can be used to evaluate simulated forecast densities with large samples \citep{Jordan2017}, it is more convenient to use if we can assume a parametric forecast density for the hierarchy. However, the degeneracy of coherent forecast densities is problematic when using log scores. We will discuss this further in the next subsection.

For the energy score with $\alpha=2$, it can be easily shown that
\begin{equation} \label{eq:(5.1)}
\text{ES}(\breve{\bm{Y}_{T+h},y_{T+h}}) = \|\bm{y}_{T+h}-\breve{\bm{\mu}}_{T+h}\|^2,
\end{equation}
where $\breve{\bm{\mu}}_{T+h} =\E_{\bm{F}}(\breve{\bm{Y}}_{T+h}) $. Therefore, in the limiting case, the energy score only measures the accuracy of the forecast mean, and not the entire distribution. Similarly, \citet{Pinson2013a} argued that the energy score given in Table~\ref{table:scoringrules} has a very low discrimination ability for incorrectly specified covariances, even though it discriminates the misspecified means well.

In contrast, \citet{SCHEUERER2015} have shown that the variogram score has a higher discrimination ability of misspecified means, variances and correlation structures than the energy score. For a finite sample of size $B$ from the multivariate forecast density $\breve{\bm{F}}$, the empirical variogram score is defined as
\begin{equation}
\text{VS}(\breve{\bm{F}}, \bm{y}_{T+h}) = \displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n}w_{ij}\left(|y_{T+h,i} - y_{T+h,j}|^p - \frac{1}{B} \displaystyle\sum_{k=1}^{B} |\breve{Y}^k_{T+h,i}-\breve{Y}^k_{T+h,j}|^p\right)^2.
\end{equation}
\citet{SCHEUERER2015} recommend using $p=0.5$. 

%For example, assume you have a Gaussian coherent predictive density $\bm{\tilde{f}}_{T+h}$. Then the log score is given by

%\begin{equation}
%LS(\bm{\tilde{f}_{T+h},y_{T+h}}) = -\log \left(\frac{1}{(2\pi)^{\frac{m}{2}}|\bm{\tilde{\Sigma}}_{T+h}|^{\frac{1}{2}}}\exp\left[-\frac{1}{2}(\bm{y}_{T+h}-\bm{\tilde{\mu}}_{T+h})^T %\bm{\tilde{\Sigma}_{T+h}}^{-1}(\bm{y}_{T+h}-\bm{\tilde{\mu}}_{T+h})\right]\right),
%\end{equation}

%
%where $\bm{\tilde{\mu}}_{T+h}$ and $\bm{\tilde{\Sigma}}_{T+h}$ are the coherent mean and variance-covariance matrix of the predictive Gaussian density and both lies in the linear %subspace spanned by $\mathbb{C}^m$. Then $\bm{\tilde{\Sigma}}_{T+h}$ is always a singular matrix and hence the determinant will be zero. Therefore the density function of %degenerate Gaussian distribution is undefined. However, we could use the pseudo determinant, i.e. product of positive eigenvalues and the pseudo inverse to calculate the log score %to compare the predictive ability of coherent forecast densities.

\subsection{Evaluating coherent forecast densities}

Any coherent hierarchical forecast density is a degenerate density. To the best of our knowledge, there is no proper multivariate scoring rule available to evaluate degenerate densities. Further, it can easily be seen that some of the existing scoring rules breakdown under degeneracy. 

For example, consider the log score in the univariate case. Suppose the true density is degenerate at $x=0$, i.e. $f(x)=\mathbb{1}\{x=0\}$. \todo{I think you need Dirac delta functions here.}  Now consider two predictive densities $p_1(x)$ and $p_2(x)$. Let $p_1(x)$ be equivalent to the true density, i.e. $p_1(x)=\mathbb{1}\{x=0\}$, and let $p_2(x) \sim \mathcal{N}(0,\sigma^2)$ with $\sigma^2 < (2\pi)^{-1}$. The expected log score of $p_1$ is
$$
\E_f[S(f,f)] = \E_f[S(p_1,f)] = -\log[p_1(x=0)]=0,
$$
while that of $p_2$ is
$$
\E_f[S(p_2,f)] = -\log[p_2(x=0)]<0.
$$
Therefore $S(f,f) > S(p_2,f)$, and there exists at least one forecast density which breaks the condition \eqref{eq:(3.1.)} for a proper scoring rule. This implies that the log score cannot be used to evaluate the degenerate densities.

However, even though the coherent distribution of the entire hierarchy is degenerate, the density of the basis set of series is non-degenerate since these series are linearly independent. Further, if we can correctly specify the forecast distribution of the basis set of series, then we have almost obtained the correct forecast distribution of the whole hierarchy. Therefore, we propose to evaluate the forecast distributions using only the basis set of series. Then we can use any of the multivariate scoring rules discussed above without incurring problems due to degeneracy.

For example, since the bottom-level series forms a set of basis series for a given hierarchy, we can evaluate the coherent forecast distribution using only the bottom-level series instead of evaluating the whole distribution. In particular, if our purpose is to compare two coherent forecast densities, we can compare them using only the bottom-level forecast densities.

\subsection{Comparison of coherent and incoherent forecast densities}

It is also important to assess how the coherent or reconciled forecast densities improve the predictive ability compared to the incoherent forecasts. Clearly, we cannot use multivariate scoring rules, even for the basis set of series, since the coherent and incoherent forecast densities lie in two different metric spaces.

However we could compare the individual margins of the forecast density of the hierarchy using univariate proper scoring rules. The widely used Continuous Ranked Probability Score (CRPS) could then be used. This is defined as
\begin{equation} \label{eq:(3.6)}
\text{CRPS}(\breve{F}_i,y_{T+h,i}) = \E_{\breve{F}_i}|\breve{Y}_{T+h,i}-y_{T+h,i}| - \frac{1}{2}\E_{\breve{F}_i}|\breve{Y}_{T+h,i}-\breve{Y}^*_{T+h,i}|,
\end{equation}
where $\breve{Y}_{T+h,i}$ and $\breve{Y}^*_{T+h,i}$ are two independent copies from the $i$th reconciled marginal forecast distribution $\tilde{F}_i$ of the hierarchy, and $y_{T+h,i}$ is the $i$th realization from the true marginal distribution $G_i$. We could also use univariate log scores, for which we could assume a parametric forecast distribution.

\section{Probabilistic forecast reconciliation in the Gaussian framework}\label{sec:gaussian}

An important special case for probabilistic forecasting arises when we can assume a multivariate Gaussian distribution. That is, suppose all the historical data in the hierarchy follows a multivariate Gaussian distribution, $\bm{y}_T \sim \mathcal{N}(\bm{\mu}_T, \bm{\Sigma}_T)$, where both $\bm{\mu}_T$ and $\bm{\Sigma}_T$ live in $\mathbb{C}^m$ by nature of the hierarchical structure of the data. We are interested in estimating the predictive Gaussian distribution of $\bm{Y}_{T+h}| \bm{\mathcal{I}}_T$, where $\bm{\mathcal{I}}_T= \{\bm{y}_1,\bm{y}_2,\dots.,\bm{y}_T\}$, which should also live in $\mathbb{C}^m$.

It is well known that the optimal point forecasts with respect to the minimal mean square error are given by the conditional expectations, $\E[Y_{T+h,i}\mid y_{1,i},\dots,y_{T,i}],$ $i=1,\dots,n$. Suppose we independently fit time series models for each series in the hierarchy. Then the point forecasts, $\hat{Y}_{T+h,i}$, from the estimated models are unbiased and consistent estimators of $\E[Y_{T+h,i}\mid y_{1,i},\dots,y_{T,i}]$, assuming the parameter estimates of the fitted models are unbiased and asymptotically consistent. \todo{Only for linear models?}

For example, suppose the data from $i$th series follows a ARMA$(p,q)$ model. i.e.,
$$
Y_{t,i}=\alpha_1Y_{t-1,i}+\dots+\alpha_pY_{t-p,i}+\varepsilon_t + \beta_1\varepsilon_{t-1,i}+\dots+\beta_q\varepsilon_{t-q,i},
$$
where $\varepsilon_t \sim \mathcal{NID}(0, \sigma_i^2)$. Then,
$$
\E[Y_{T+h,i}\mid y_{1,i},\dots,y_{T,i}] = \alpha_1Y_{T+h-1,i}+\dots+\alpha_pY_{T+h-p,i}+ \beta_1\varepsilon_{T+h-1,i}+\dots+\beta_q\varepsilon_{T+h-q,i}.
$$

Since $\bm{\alpha} = (\alpha_1,\dots,\alpha_p)'$ and $\bm{\beta} = (\beta_1,\dots,\beta_q)'$ are unknown in practice and thus estimated using the maximum likelihood method. Let $\bm{\hat{\alpha}}$ and $\bm{\hat{\beta}}$ denote the maximum likelihood estimates of $\bm{\alpha}$ and $\bm{\beta}$ respectively. \citet{Yao2006} showed that $\bm{\hat{\alpha}}$ and $\bm{\hat{\beta}}$ are asymptotically consistent estimators. Thus the point forecasts from this estimated model, $\hat{Y}_{T+h,i}$, will also be a consistent estimator for $\E[Y_{T+h,i}\mid y_{1,i},\dots,y_{T,i}]$. i.e.,
\begin{equation} \label{eq:(6.01)}
\hat{Y}_{T+h,i} \overset{p}{\to} \E[Y_{T+h,i}\mid y_{1,i},\dots,y_{T,i}] \quad \text{as} \quad T \to \infty.
\end{equation}

Let $\hat{\bm{Y}}_{T+h}=(\hat{Y}_{T+h,1},\dots,\hat{Y}_{T+h,n})'$ and suppose \eqref{eq:(6.01)} holds for $i=1,\dots,n$. Then from Slutsky's theorem it follows that
\begin{equation}\label{eq:(6.02)}
\hat{\bm{Y}}_{T+h} \overset{p}{\to} \E[\bm{Y}_{T+h}|\bm{\mathcal{I}}_T] \quad \text{as} \quad T \to \infty.
\end{equation}
Further, let the forecast error due to $\hat{\bm{Y}}_{T+h}$ be given by
\begin{equation}
\hat{\bm{e}}_{T+h} = \bm{Y}_{T+h}-\hat{\bm{Y}}_{T+h},
\end{equation}
and consider the variance of $\hat{\bm{e}}_{T+h}$,
\begin{align*}
\E[(\bm{Y}_{T+h} - \hat{\bm{Y}}_{T+h})(\bm{Y}_{T+h} - \hat{\bm{Y}}_{T+h})'|\bm{\mathcal{I}}_T] = &
\E[(\bm{Y}_{T+h} - \E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T) + \E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T)- \hat{\bm{Y}}_{T+h})\\
& \quad
(\bm{Y}_{T+h} - \E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T) + \E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T) - \hat{\bm{Y}}_{T+h})'|\bm{\mathcal{I}}_T],\\
= &
\E[(\bm{Y}_{T+h} - \E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T))(\bm{Y}_{T+h} - \E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T))'|\bm{\mathcal{I}}_T]\\
& \quad
+ \E[\E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T) - \hat{\bm{Y}}_{T+h})(\E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T) - \hat{\bm{Y}}_{T+h})'|\bm{\mathcal{I}}_T]\\
& \quad
+  \E[(\bm{Y}_{T+h} - \E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T))(\E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T) - \hat{\bm{Y}}_{T+h})'|\bm{\mathcal{I}}_T]\\
&  \quad
+ \E[\E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T) - \hat{\bm{Y}}_{T+h})(\bm{Y}_{T+h} - \E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T))'|\bm{\mathcal{I}}_T].
\end{align*}

From \eqref{eq:(6.02)} it immediately follows that
\begin{align*}
&\E[(\bm{Y}_{T+h} - \hat{\bm{Y}}_{T+h})(\bm{Y}_{T+h} - \hat{\bm{Y}}_{T+h})'|\bm{\mathcal{I}}_T] \overset{p}{\to} \E[(\bm{Y}_{T+h} - \E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T))(\bm{Y}_{T+h} - \E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T))'|\bm{\mathcal{I}}_T].
\end{align*}
That is,
\begin{equation}
\bm{W}_{T+h} \overset{p}{\to} \text{Var}(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T) \quad \text{as} \quad T \to \infty,
\end{equation}
where $\E[(\bm{Y}_{T+h} - \hat{\bm{Y}}_{T+h})(\bm{Y}_{T+h} - \hat{\bm{Y}}_{T+h})'|\bm{\mathcal{I}}_T] = \bm{W}_{T+h}$.

Even though $\hat{\bm{Y}}_{T+h}$ and $\bm{W}_{T+h}$ are asymptotically consistent estimators for $\E(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T)$ and $\text{Var}(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T)$ respectively, they are not coherent since they do not lie in the coherent subspace. Thus the Gaussian forecast distribution with mean $\hat{\bm{Y}}_{T+h}$ and variance $\bm{W}_{T+h}$ will be incoherent, and we denote it by
\begin{equation}\label{eq:(6.03)}
\widehat{\bm{Y}_{T+h,i}|\bm{\mathcal{I}}_T} \sim \mathcal{N}(\hat{\bm{Y}}_{T+h}, \bm{W}_{T+h})
\end{equation}

Since our primary objective is to find the coherent forecast density of the hierarchy, we need to reconcile \eqref{eq:(6.03)}. Using \eqref{eq:gaussianreconciled}, the reconciled Gaussian forecast distribution is then given by
\begin{equation}\label{eq:(6.04)}
\widetilde{\bm{Y}_{T+h,i}|\bm{\mathcal{I}}_T} \sim \mathcal{N}(\bm{SP}\hat{\bm{Y}}_{T+h}, \bm{SP}\bm{W}_{T+h}\bm{P}'\bm{S}'),
\end{equation}
where $\bm{P} = (\bm{R}'_\bot \bm{S})^{-1}\bm{R}'_\bot$.

\textbf{Result 1}: Choosing $\bm{R}'_\bot = \bm{S}'\bm{W}_{T+h}^{-1}$ will ensure that at least the mean of the predictive Gaussian distribution is optimally reconciled with respect to the energy score.

Result 1 can be easily shown as follows. From \eqref{eq:(5.1)}, the energy score at the upper limit of $\alpha=2$ is given by $\|\bm{y}_{T+h}-\bm{SP}\hat{\bm{y}}_{T+h}\|^2$. Then the expectation of the energy score with respect to the true distribution is equivalent to the trace of mean squared forecast error; i.e.,
$$
\E_{\bm{G}}[eS(\bm{\tilde{Y}_{T+h},y_{T+h}})]= \text{Tr}\{\E_{\bm{y}_{T+h}}[(\bm{Y}_{T+h}-\bm{SP}\hat{\bm{Y}}_{T+h})(\bm{Y}_{T+h}-\bm{SP}\hat{\bm{Y}}_{T+h})'|\mathcal{I}_{T}]\}.
$$

From Theorem 1 of \citet{Wickramasuriya2017} it immediately follows that $\bm{P} = (\bm{S}'\bm{W}_{T+h}^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_{T+h}^{-1}$ minimizes the expected energy score, if we constrain the reconciled forecasts to be unbiased. Thus we have $\bm{R}'_\bot = \bm{S}'\bm{W}_{T+h}^{-1}$.

It should be noted that $\bm{W}_{T+h}$ can be estimated in different ways, which yields different estimates of $\bm{R}'_\bot$. Table~\ref{table:2} summarizes some of these methods.

\begin{table}
  \caption{Several possible estimates of $\bm{R}'_\bot$. For $n<T$, $\bm{\hat{W}}_{T+1}^{sam}$ is an unbiased and consistent estimator for $\bm{W}_{T+1}$. $\bm{\hat{W}}_{T+1}^{shr}$ is a shrinkage estimator which is more suitable for large dimensions. $\bm{\hat{W}}_{T+1}^{shr}$ was proposed by \citet{Schafer2005} and also used by \citet{Wickramasuriya2017}, where Diag($\bm{A}$) denotes the diagonal matrix of $\bm{A}$, $\tau = \frac{\sum_{i \ne j}\hat{\text{Var}}(\hat{r}_{ij})}{\sum_{i \ne j}\hat{r}_{ij}^2}$, and $\hat{r}_{ij}$ is the $ij$th element of the sample correlation matrix.}\label{table:2}
  \centering\setstretch{1.5}
  \begin{tabular}{lll}
    \toprule
    \textbf{Method} & \textbf{Estimate of $\bm{W}_{h}$} & \textbf{Estimate of $\bm{R}'_\bot$}      \\
    \midrule
    OLS             &
    $\bm{I}$  &
    $\bm{S}'$  \\
    MinT(Sample)    &
    $\bm{\hat{W}}_{T+1}^{sam}$ &
    $\bm{S}'(\bm{\hat{W}}_{T+1}^{sam})^{-1}$ \\
    MinT(Shrink)    &
    $\bm{\hat{W}}_{T+1}^{shr} = \tau\text{Diag}(\bm{\hat{W}}_{T+1}^{sam}) + (1-\tau)\bm{\hat{W}}_{T+1}^{sam}$ &
    $\bm{S}'(\bm{\hat{W}}_{T+1}^{shr})^{-1}$ \\
    MinT(WLS)       &
    $\bm{\hat{W}}_{T+1}^{wls} = \text{Diag}(\bm{\hat{W}}_{T+1}^{shr})$ &
    $\bm{S}'(\bm{\hat{W}}_{T+1}^{wls})^{-1}$ \\
    \bottomrule
  \end{tabular}
\end{table}

All of these forecasting methods are well-established in the context of point forecast reconciliation \citep{Hyndman2011, Hyndman2016,Wickramasuriya2017}. Here, we are showing how these reconciliation methods can be used in the context of probabilistic forecast reconciliation, at least in the Gaussian framework.

\subsubsection*{Simulations}

We consider the hierarchy given in Figure~\ref{fig1}, comprising two aggregation levels with four bottom-level series. Each bottom-level series will be generated first, and then summed to obtain the data for the upper-level series. In practice, hierarchical time series tend to contain much noisier series at lower levels of aggregation. In order to replicate this feature in our simulations, we follow the data generating process proposed by \citet{Wickramasuriya2017}.

Suppose $\{w_{AA,t},w_{AB,t},w_{BA,t},w_{BB,t}\}$ are generated from ARIMA$(p,d,q)$ processes, where $(p,q)$ and $d$ take integers from $\{1,2\}$ and $\{0,1\}$ respectively with equal probability. Further, the contemporaneous errors $\{\varepsilon_{AA,t},\varepsilon_{AB,t},\varepsilon_{BA,t},\varepsilon_{BB,t}\} \sim \mathcal{N}(\bm{0}, \bm{\Sigma})$. The parameters for the AR and MA components will be randomly and uniformly generated from $[0.3,0.5]$ and $[0.3,0.7]$ respectively. Then the bottom-level series $\{y_{AA,t},y_{AB,t},y_{BA,t},y_{BB,t}\}$ will be obtained as:
\begin{align*}
y_{AA,t} &= w_{AA,t} + u_t - 0.5v_t,\\
y_{AB,t} &= w_{AB,t} - u_t - 0.5v_t,\\
y_{BA,t} &= w_{BA,t} + u_t + 0.5v_t,\\
y_{BB,t} &= w_{BB,t} - u_t + 0.5v_t,
\end{align*}
where $u_t \sim \mathcal{N}(0,\sigma^2_u)$ and $v_t \sim \mathcal{N}(0,\sigma^2_v)$.
To obtain the aggregate series at level 1, we add the bottom-level series:
\begin{align*}
y_{A,t} &= w_{AA,t} + w_{AB,t} - v_t,\\
y_{B,t} &= w_{BA,t} + w_{BB,t} + v_t,
\end{align*}
and the total series is obtained using
$$y_{Tot,t} = w_{AA,t} + w_{AB,t} + w_{BA,t} + w_{BB,t}.$$

To ensure noisier disaggregate series than aggregate series, we choose $\bm{\Sigma}, \sigma^2_u$ and $\sigma^2_v$ such that
$$
\text{Var}(\varepsilon_{AA,t}+\varepsilon_{AB,t}+\varepsilon_{BA,t}+\varepsilon_{BB,t}) \le \text{Var}(\varepsilon_{AA,t}+\varepsilon_{AB,t}-v_t) \le \text{Var}(\varepsilon_{AA,t}+u_t-0.5v_t).
$$
Therefore
$$
\bm{l}_1\bm{\Sigma} \bm{l}_1' \le \bm{l}_2\bm{\Sigma} \bm{l}_2' + \sigma^2_v \le  \bm{l}_3\bm{\Sigma} \bm{l}_3' + \sigma^2_u + \frac{1}{4}\sigma^2_v,
$$
where $\bm{l}_1 = (1,1,1,1)'$, $\bm{l}_2 = (1,1,0,0)'$ and $\bm{l}_3 = (1,0,0,0)'$, and hence
$$\bm{l}_1\bm{\Sigma} \bm{l}_1' - \bm{l}_2\bm{\Sigma} \bm{l}_2' \le \sigma^2_v \le \frac{4}{3}(\sigma^2_u + \bm{l}_3\bm{\Sigma} \bm{l}_3' - \bm{l}_2\bm{\Sigma} \bm{l}_2').$$
To satisfy these constraints, we choose
$\bm{\Sigma} =
\begin{pmatrix}
5.0 & 3.1 & 0.6 & 0.4 \\
3.1 & 4.0 & 0.9 & 1.4 \\
0.6 & 0.9 & 2.0 & 1.8 \\
0.4 & 1.4 & 1.8 & 3.0 \\
\end{pmatrix}$,
$\sigma^2_u = 19$ and $\sigma^2_u = 18$ in our simulation setting.

We generate data for the hierarchy with sample size $T=501$. Univariate ARIMA models were fitted for each series independently using the first 500 observations, and 1-step ahead base (incoherent) forecasts were calculated. We use the \textit{forecast} package \citep{hyndman2017forecasting} in R \citep{Rcore} for model fitting and forecasting. The different estimates of $\bm{W}_{T+1}$ and the corresponding $\bm{R}'_\bot$ from Table~\ref{table:2} were obtained. This process was replicated using $1000$ different data sets from the same data generating processes.

To assess the predictive performance of different forecasting methods, we use scoring rules as discussed in Section~\ref{sec:evaluation}. To faciliate comparisons, we report skill scores \citep{Gneiting2007}. For a given forecasting method, evaluated by a particular scoring rule $S(\cdot)$, the skill score is calculated as
\begin{equation}
Ss[S_B(\cdot)] = \frac{S_B(\bm{Y},\bm{y})^{\text{ref}} - S_B(\breve{\bm{Y}},\bm{y})}{S_B(\bm{Y},\bm{y})^{\text{ref}}}\times 100\%,
\end{equation}
where $S_B(\cdot)$ is the average score over $B$ samples and $S_B(\bm{Y},\bm{y})^{\text{ref}}$ is the average score for the reference forecasting method. Thus $Ss[S_B(\cdot)]$ gives the percentage improvement of the preferred forecasting method relative to the reference method. Any negative value of $Ss[S_B(\cdot)]$ indicates that the method we compared is worse than the reference method, whereas any positive value indicates that method is superior to the reference method.

In Table~\ref{table:3}, we compare different reconciliation methods over the conventional bottom-up method. We use bottom-level probabilistic forecasts and calculate the percentage skill score based on energy score, log score and variogram score for each reconciliation method with reference to the bottom-up method.

We also evaluate the predictive ability of coherent forecasts over incoherent forecasts in Tables~\ref{table:4} and~\ref{table:5}. Here we use percentage skill score based on CRPS and univariate log score for coherent probabilistic forecasts of each individual series with reference to incoherent forecasts.

It is clearly evident from the results in Table~\ref{table:3} that the multivariate reconciled forecasts for the bottom-level series from MinT(Shrink) and MinT(Sample) out-perform the bottom-up forecasts. Further, these two methods produce probabilistic forecasts with the best predictive ability in comparison to incoherent forecasts (from Tables~\ref{table:4} and~\ref{table:5}). Moreover, it turns out that OLS and bottom-up methods produce the worst forecasts.

\begin{table}
  \caption{Comparison of incoherent forecasts using bottom-level series. The ``Skill score'' columns give the percentage skill score with reference to the bottom-up forecasting method. Entries in these columns show the percentage increase of score for different reconciliation methods relative to the bottom-up method.}\label{table:3}
  \centering\small
  \begin{tabular}{@{}lSSSSSS@{}}
    \toprule
    Forecasting &
    \multicolumn{2}{c}{\text{Energy score}} &
    \multicolumn{2}{c}{\text{Log score}} &
    \multicolumn{2}{c}{\text{Variogram score}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7}
    method &
    \text{Mean score} & \text{Skill score} &
    \text{Mean score} & \text{Skill score} &
    \text{Mean score} & \text{Skill score}\\
    \midrule
    MinT(Shrink) &  7.47 &  10.11 &  11.34 &     6.44 & 3.05 &   4.69 \\
    MinT(Sample) &  7.47 &  10.11 &  11.33 &     6.52 & 3.05 &   4.69 \\
    MinT(WLS)    &  7.91 &   4.81 &  12.64 &    -4.29 & 3.23 &  -0.94 \\
    OLS          & 10.14 & -22.02 & 135.13 & -1014.93 & 4.60 & -43.75 \\
    Bottom-up    &  8.31 &        &  12.12 &          & 3.20 &        \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \caption{Comparison of incoherent vs coherent forecasts for the aggregate series using Skill scores. The ``Incoherent'' row shows the average scores for incoherent forecasts. Each entry above this row represents the percentage skill score with reference to the incoherent forecasts. Entries show the percentage increase in score for different forecasting methods relative to the incoherent forecasts.}\label{table:4}
  \centering\small
  \begin{tabular}{@{}lSSSSSS@{}}
    \toprule
    Forecasting &
    \multicolumn{2}{c}{\text{Total}} &
    \multicolumn{2}{c}{\text{Series - A}} &
    \multicolumn{2}{c}{\text{Series - B}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7}
    method      &  CRPS   & LogS    & CRPS   & LogS     & CRPS   & LogS     \\
    \midrule
    MinT(Shrink) & 1.12   & 0.34    & 10.07  & 2.93     & 5.41   & 1.52     \\
    MinT(Sample) & 1.12   & 0.34    & 10.07  & 2.93     & 5.41   & 1.52     \\
    MinT(WLS)    & -2.61  & -2.02   & 5.28   & -4.40    & 2.70   & -4.24    \\
    OLS          & -38.06 & -698.99 & -24.70 & -1368.33 & -24.86 & -1159.09 \\
    Bottom-up    & -89.55 & -21.83  & -8.87  & -2.35    & -9.46  & -2.73    \\
    \midrule
    \textit{Incoherent} & $\mathbi{2.68}$ & $\mathbi{2.97}$ & $\mathbi{4.17}$ & $\mathbi{3.41}$ & $\mathbi{3.70}$ & $\mathbi{3.30}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \caption{Comparison of incoherent vs coherent forecasts for the individual bottom-level series using Skill scores.}\label{table:5}
  \centering\tabcolsep=0.08cm\small
  \begin{tabular}{@{}lSSSSSSSS@{}}
    \toprule
    Forecasting &
    \multicolumn{2}{c}{\text{Series - AA}} &
    \multicolumn{2}{c}{\text{Series - AB}} &
    \multicolumn{2}{c}{\text{Series - BA}} &
    \multicolumn{2}{c}{\text{Series - BB}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(l){8-9}
    method       & CRPS   & LogS    & CRPS   & LogS    & CRPS   & LogS    & CRPS   & LogS \\
    \midrule
    MinT(Shrink) & 8.71   & 2.71    & 10.57  & 3.04    & 5.95   & 1.86    & 7.91   & 2.46 \\
    MinT(Sample) & 8.71   & 2.71    & 10.57  & 3.04    & 5.95   & 1.86    & 8.19   & 2.46 \\
    MinT(WLS)    & 5.54   & 0.30    & 5.96   & 0.30    & 2.43   & -0.62   & 5.08   & 0.62 \\
    OLS          & -22.43 & -931.63 & -22.49 & -886.32 & -26.01 & -834.67 & -23.45 & -812.92 \\
    \midrule
    \textit{Incoherent} & $\mathbi{3.79}$ & $\mathbi{3.32}$ & $\mathbi{3.69}$ & $\mathbi{3.29}$ & $\mathbi{3.46}$ & $\mathbi{3.23}$ & $\mathbi{3.54}$ & $\mathbi{3.25}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Conclusions}\label{sec:conclusions}

Although the problem of hierarchical point forecasts is well studied in the literature, there is a lack of attention in the context of probabilistic forecasts. Thus we attempted to fill this gap in the literature by providing substantial theoretical background to the problem. We initially provided rigorous definitions for the coherent point and probabilistic forecasts using the principles of measure theory. Due to the aggregation nature of hierarchy, the probability density is a degenerate density. Thus the forecast distribution that we opt to find should also lie in a lower dimensional subspace of $\mathbb{R}^{n}$.

As it was well established that the reconciliation outperforms other conventional point forecasting methods in the hierarchical literature, we proposed to use reconciliation in probabilistic framework to obtain coherent degenerate densities. We provided a distinct definition for density forecast reconciliation and how it can be used to reconcile incoherent densities in practice.

Assuming a multivariate Gaussian distribution for the hierarchy, we showed how to obtain reconciled Gaussian forecast densities, utilizing available information in the hierarchy. An extensive Monte Carlo simulation study further showed that the MinT reconciliation method \citep{Wickramasuriya2017} is useful in producing improved coherent probabilistic forecasts at least in the Gaussian framework.


\newpage
\printbibliography
\end{document}

