
\documentclass[a4paper, 11pt]{article}
\usepackage[a4paper, inner=3cm, outer=3cm, top=2cm, bottom=3cm]{geometry}
\usepackage{amsmath, amssymb, fixmath, qtree, bm, calrsfs, enumitem,multirow, caption, textcmds,siunitx,adjustbox, mathrsfs, float, amstext}
\usepackage[bb=boondox]{mathalfa}
\usepackage{tikz}
\usepackage[natbib=true,backend=bibtex,style=authoryear-comp,dashed=false,maxbibnames=99,firstinits=true,maxcitenames=3,url=true,doi=false,isbn=false]{biblatex}
\usepackage[pdftex,colorlinks=true]{hyperref}
\hypersetup{citecolor=blue,linkcolor=blue,urlcolor=blue}
\DeclareNameAlias{sortname}{last-first}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type


\bibliography{References_paper1}
\begin{document}


\title{Probabilistic Forecasts in Hierarchical Time Series}
\maketitle


\section{Introduction.}
\begin{itemize}
	\item Introduction to hierarchical time series including a literature review in the context of point forecasts. 
	\item Importance of probabilistic forecasts in time series.
	\item Lack of attention in the context of probabilistic forecasts in hierarchical literature.
	
\end{itemize}

\section{Notations}
We first introduce notations where possible we follow \citet{Wickramasuriya2017}. Suppose $\mathbold{y}_t \in \bm{\mathbb{R}}^n$ comprising all observations of the whole hierarchy at time $t$ and $\mathbold{b}_t \in \bm{\mathbb{R}}^m$ comprising only the bottom level observations at  time $t$. Then due to the aggregation nature of the hierarchy we have, 

\begin{equation}
\mathbold{y}_t = \mathbold{Sb}_t,
\end{equation}
where $\mathbold{S}$ is a $n \times m$ constant matrix whose columns span the linear subspace for which all constraints hold. Since this linear subspace is equivalent to the column space of $\bm{S}$, we denote it as $\mathscr{C}(\bm{S})$. Further the null space of $\bm{S}$ which is orthogonal to this linear subspace is denoted by $\mathscr{N}(\bm{S})$. 
To understand the notations clearly, consider the hierarchy given in Figure 1.\\



\begin{figure}
	\begin{center}
		\leaf{AA} \leaf{AB} \leaf{AC}
		\branch{3}{A}
		\leaf{BA} \leaf{BB}
		\branch{2}{B}
		\branch{2}{Tot}
		\qobitree
	\end{center}
	\caption{Two level hierarchical diagram}
\end{figure}
\noindent
In any hierarchy, the most aggregated level is termed as level 0, the second most aggregated level is termed as level 1 and so on. This example consists of two levels. At a particular time $t$, let $y_{T,t}$ denote the observation at level 0; $y_{A,t}, y_{B,t} $ denote observations at level 1; and $y_{AA,t}, y_{AB,t}, y_{AC,t}, y_{BA,t}, y_{BB,t}$ denote observations at level 2. Then $\mathbold{y}_t = [y_{T,t},y_{A,t}, y_{B,t},y_{C,t},y_{AA,t}, y_{AB,t}, y_{AC,t}, y_{BA,t}, y_{BB,t}]^T$, \\$\mathbold{b}_t = [y_{AA,t}, y_{AB,t}, y_{AC,t}, y_{BA,t}, y_{BB,t}]^T$, $m=7$, n=11$, $ and $$ \mathbold{S} = \begin{pmatrix} 1& 1 &1 &1 &1 \\ 1 &1 &1 & 0 &0 \\   0&0 &0 & 1 & 1 \\ & & I_5 &  & \end{pmatrix}, $$ where $I_5$ is a $5$-dimension identity matrix. \\  


\section{Coherent forecasts}

The main purpose of this section is to provide formal definitions for coherent forecasts. We start with redefining the coherent point forecasts using the properties of vector spaces which is then followed by the definition of coherent probabilistic forecasts. 
\\

\noindent
\textbf{Definition 3.1: Coherent Point Forecasts}\\
\noindent
Suppose $\bm{\breve{y}}_{t+h} \in \mathbb{R}^n$ consists point forecasts of each series in the hierarchy at time $t+h$.  $\bm{\breve{y}}_{t+h}$ is said to be \textit{Coherent} if it lies in a $m$-dimensional subspace of $\bm{\mathbb{R}}^n$ which is spanned by the columns of $\bm{S}$. \\

\noindent
\textbf{Definition 3.2: Coherent Probabilistic Forecasts}\\
\noindent
Let $(\bm{\mathbb{R}}^m, \bm{\mathscr{F}}^m, \nu^m)$ be a probability measure space where $\mathscr{F}^m$ is a sigma algebra on $\bm{\mathbb{R}}^m$. $\breve{\nu}(.)$ on $(\mathscr{C}(\bm{S}), \mathscr{F}_{\bm{S}})$ is said to be coherent probability measure iff $$\breve{\nu}(\bm{S}(\bm{A})) = \nu^m(\bm{A}) \quad \forall \quad \bm{A} \in \mathscr{F}^m,$$ where $\bm{S}(\bm{A})$ denotes the image of subset $\bm{A}$ under $\bm{S}$. 

\usetikzlibrary{arrows,positioning,shapes,fit,calc}

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[
		>=stealth,
		bullet/.style={
			fill=black,
			circle,
			minimum width=1.5cm,
			inner sep=0pt
		},
		projection/.style={
			->,
			thick,
			label,
			shorten <=2pt,
			shorten >=2pt
		},
		every fit/.style={
			ellipse,
			draw,
			inner sep=0pt
		}
		]
		
		\node at (2,3) {$\bm{S}$};
		
		\node at (0,5) {$\bm{\mathbb{R}}^m$(domain of $\bm{S}$)};
		\node at (4,5) {$\bm{\mathbb{R}}^n$(codomain of $\bm{S}$)};
		\node at (4,1.0) {$\mathscr{C}(\bm{S})$};
		%\node[bullet,label=below:$f(x)$] at (4,2.5){};
		
		
		\draw (0,2.5) ellipse (1.02cm and 2.2cm);
		\draw (4,2.5) ellipse (1.02cm and 2.2cm);
		\draw (4,2.5) ellipse (0.51cm and 1.1cm);
		
		
		\draw[projection, label=below:$f$] (0.3,2.5) -- (3.8,2.5) ;
		
		\end{tikzpicture}
		\newline
	\end{center}
	\caption{Any set $\bm{A} \in \bm{\mathbb{R}}$ will be mapped to the $\mathscr{C}(\bm{S})$ through the mapping $\bm{S}$}
\end{figure}
\noindent
Definition 3.2 implies the probability measure on $\mathscr{C}(\bm{S})$ is equivalent to the probability measure on $(\bm{\mathbb{R}}^m, \bm{\mathscr{F}}^m)$. Hence, there is no density anywhere outside the linear subspace spanned by the columns of $\bm{S}$. That is, a \textit{Coherent probability density forecast} is any density $\bm{f}(\bm{\breve{y}}_{t+h})$ such that $\bm{f}(\bm{\breve{y}}_{t+h})=0$ for all $\bm{\breve{y}}_{t+h}$ in the null space of $\bm{S}$. Following example will help to understand these definitions more clearly.\\

\noindent
\textbf{Example 1}\\
\noindent
Consider a simple hierarchy with two bottom level series A and B that add up to the top level series T. Suppose the forecasts at time $t+h$ of these series are given by $\mathbold{\breve{y}}_{t+h} = [\breve{y}_{T,t+h},\breve{y}_{A,t+h}, \breve{y}_{B,t+h}]$ and $\mathbold{\breve{y}}_{t+h} \in \bm{\mathbb{R}}^3$. Due to the aggregation constraint of the hierarchy we have $\breve{y}_{T,t+h}=\breve{y}_{A,t+h}+\breve{y}_{B,t+h}$. This implies, even though  $\mathbold{\breve{y}}_{t+h}$ is in $\bm{\mathbb{R}}^3$, the points actually lie in a two dimensional subspace within that $\bm{\mathbb{R}}^3$ space. This subspace is equivalent to $\mathscr{C}(\bm{S})$ for this simple hierarchy. Therefore, for any $\mathbold{\breve{y}}_{t+h} \in \mathscr{N}(\bm{S})$ have a zero probability. I.e. $f(\mathbold{\breve{y}}_{t+h})=0$ for any $\mathbold{\breve{y}}_{t+h} \in \mathscr{N}(\bm{S})$.\\

\noindent
By the definition of coherent forecasts, it is clear that there are only $m$ number of linear independent series in the whole hierarchy. We refer to these as $m$ dimensional \textit{basis set of series} since these series generates a \textit{basis} that spans the $\mathscr{C}(\bm{S})$. Other $n-m$ series of the hierarchy can be linearly determined by these basis set of series. This implies that any coherent density is a degenerate density. The $m$ number of bottom level series of a given hierarchy can be considered as a basis set of series. Then the columns of $\bm{S}$ is a basis that generates through these bottom level series. This basis spans the linear subspace where the degenerate density lives on.\\

\noindent
However bottom level series are not the only set of basis series and we can find many other basis set of series which generates basis that spans the same $\mathscr{C}(\bm{S})$ for a given hierarchy. If we go back to the hierarchy in example 1, instead of two bottom level series $(\bm{A,B})$ we can take $(\bm{T,A})$ as the basis set of series and the basis that generates through $(\bm{T,A})$ will be $\{\begin{pmatrix}
1&0&1 \end{pmatrix}^T, \begin{pmatrix}
0&1&-1 \end{pmatrix}^T\}$. Another possible basis would be the singular value decomposition of $\bm{S}$.\\

\noindent
An important thing to notice here is all of these basis spans the same linear subspace equivalent to the $\mathscr{C}(\bm{S})$. Therefore the definition $(3.2)$ is not unique and one can redefine the coherent probabilistic forecasts with respect to any basis. However we stick to the definition $(3.2)$ and consider the basis defined by the columns of $\bm{S}$ that generates through the bottom levels of the hierarchy in what follows.\\ 

\noindent
It is also worth to mention that the definition $(3.1)$ and $(3.2)$ facilitate extension to the forecast reconciliation which we talk about in the next section. In contrast to our definition, \citet{BenTaieb2017} defines the coherent probabilistic forecasts in terms of convolution. According to their definition, if the forecasts are coherent, then the convolution of forecast distributions of disaggregate series is same as the forecast distribution of the corresponding aggregate series. This is the only study that copes with probabilistic forecasts in hierarchical literature thus far. 

\section{Forecast reconciliation}
\noindent
Bottom-up, top-down and middle-out methods are the most traditional forecasting methods that were used to produce coherent point forecasts in early studies on hierarchical literature. In bottom-up approach, forecasts of the lowest level are first generated and they are simply aggregated to forecast the upper levels of the hierarchy \citep{Dunn1976}. 
In contrast, the top-down approach involves forecasting the most aggregated series first and then disaggregating these forecasts down the hierarchy based on the proportions of observed data \citep{Gross1990}. A compromise between these two approaches is the middle-out method which entails forecasting each series of a selected middle level in the hierarchy and then forecasting upper levels by the bottom-up method and lower levels by the top-down method.\\ 

\noindent
These three approaches use only a part of the information available when producing coherent forecasts. This might result inaccurate forecasts. For example, if the bottom level series are highly volatile or too noisy and hence challenging to forecast, then the resulting forecasts from bottom-up approach would be inaccurate.\\

\noindent
As an alternative to these conventional methods, \citet{Hyndman2011} propose to use information from all the levels of the hierarchy and reconcile them to obtain coherent point forecasts. In this approach, independent forecasts of all series are initially obtained. It is very unlikely that these forecasts are coherent. Then, these forecasts are optimally combined through a regression model to obtain coherent forecasts. This study has given arise to the concept of point forecast reconciliation in hierarchical time series. In general, any forecasting method that use a set of incoherent forecasts and revise them to obtain coherent forecasts is referred to as forecast reconciliation method. For example, Minimum Trace reconciliation \citep{Wickramasuriya2017}, GTOP \citep{VanErven2015a}. It is important to notice that bottom-up, top-down and middle-out methods are not reconciliation methods since they only use forecasts from a part of the levels in the hierarchy when producing coherent forecasts. \\


\subsection{Point forecast reconciliation}

Even though the point forecast reconciliation has a well established literature, we would like to redefine this using concepts in linear algebra as a groundwork for the probabilistic forecast reconciliation. \\


\noindent
\textbf{Definition 4.1}\\
\noindent
Let $\bm{g}:\bm{\mathbb{R}}^n \rightarrow \bm{\mathbb{R}}^m $ and $\hat{\bm{y}}_{t+h} \in \bm{\mathbb{R}}^n$ be any set of incoherent forecasts at time $t+h$. Then $\tilde{\bm{b}}_{t+h}$ is said to be reconciled bottom level forecasts if 
\begin{equation}
\tilde{\bm{b}}_{t+h}=\bm{g}(\hat{\bm{y}}_{t+h}),
\end{equation}
\noindent
where $\bm{g}(\hat{\bm{y}}_{t+h})$ is the image of $\hat{\bm{y}}_{t+h}$ under $\bm{g}$ on $\bm{\mathbb{R}}^m$. The reconciled forecasts for the whole hierarchy is then given by $\tilde{\bm{y}}_{t+h}=\bm{S}(\tilde{\bm{b}}_{t+h})$ such that $\tilde{\bm{y}}_{t+h} \in \mathscr{C}(\bm{S})$, where $\bm{S}(\tilde{\bm{b}}_{t+h})$ is the image of $\tilde{\bm{b}}_{t+h}$ under $\bm{S}$ on the $\mathscr{C}(\bm{S}) < \bm{\mathbb{R}}^n$.\\

\noindent
In the following content we explain more on how this definition can be used in practice to reconcile point forecasts in hierarchical time series. Let $\bm{R} \in \bm{\mathbb{R}}^{n \times n-m}$ consists the columns that spans $\mathscr{N}(\bm{S})$ which is orthogonal to $\mathscr{C}(\bm{S})$. $\mathscr{N}(\bm{S})$ is also equivalent to the $\mathscr{C}(\bm{R})$. Note that $\bm{R}$ is also not unique and one example is a matrix whose columns represent the aggregation constraints for the hierarchy. Then for the hierarchy in example 1, $$ \mathbold{S} = \begin{pmatrix} 1& 1 \\ 1 & 0 \\ 0&1 \end{pmatrix} \quad \text{and} \quad \mathbold{R} = \begin{pmatrix}  1 \\ -1 \\ -1 \end{pmatrix}.$$ 
\noindent
Further let $\{\bm{s}_1,...,\bm{s}_m\}$ and $\{\bm{r}_1,...,\bm{r}_{n-m}\}$ denote the columns of $\bm{S}$ and $\bm{R}$ respectively. Then $\bm{B}=\{\bm{s}_1,...,\bm{s}_m, \bm{r}_1,...,\bm{r}_{n-m}\}$ is a basis for $\bm{\mathbb{R}}^n$. Now, using the insights of definition 4.1, we can follow the below steps to reconcile the point forecasts.\\

\noindent
\textbf{Step 1: Obtaining reconciled bottom level point forecasts}\\

\noindent
For a given incoherent set of point forecasts $\hat{\bm{y}}_{t+h} \in \bm{\mathbb{R}}^n$, first we find the coordinates of $\hat{\bm{y}}_{t+h}$ with respect to the basis $\bm{B}$. Let $\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T$ denote these coordinates. Note that $\tilde{\bm{b}}_{t+h}$ is a basis set of series which is really the reconciled bottom level series that generates $\{\bm{s}_1,...,\bm{s}_m\}$ and $\tilde{\bm{t}}_{t+h}$ is another basis set of series that generates $\{\bm{r}_1,...,\bm{r}_{n-m}\}$. Then from basic properties of linear algebra it follows that, 

\begin{flalign}\label{4.1}
\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T = \hat{\bm{y}}_{t+h},
\end{flalign}
\begin{flalign}\label{4.2}
\hat{\bm{y}}_{t+h} = \bm{S}\tilde{\bm{b}}_{t+h} +  \bm{R}\tilde{\bm{t}}_{t+h},
\end{flalign}
\noindent
and
\begin{flalign}\label{4.3}
\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T  = \begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1}\hat{\bm{y}}_{t+h}.
\end{flalign}

\noindent
In order to find $(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1}$, let $\bm{S}_{\bot}$ and $\bm{R}_{\bot}$ be the orthogonal complements of $\bm{S}$ and $\bm{R}$ 
respectively. Then $(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1}$ is given by, 

\begin{flalign}
\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1} = \begin{pmatrix}
(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot\\ \cdots \\ (\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot
\end{pmatrix}.
\end{flalign}

\noindent
Thus we have, 
\begin{flalign} \label{4.4}
\begin{pmatrix}
\tilde{\bm{b}}_{t+h}\\ \cdots \\ \tilde{\bm{t}}_{t+h}
\end{pmatrix} = \begin{pmatrix}
(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot\\ \cdots \\ (\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot
\end{pmatrix}\hat{\bm{y}}_{t+h}.
\end{flalign}

\noindent
From (\ref{4.4}) it follows that, 
\begin{flalign}
\tilde{\bm{b}}_{t+h}=(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot \hat{\bm{y}}_{t+h} 
\end{flalign}
\\

\noindent
\textbf{Step 2: Obtaining reconciled point forecasts for the whole hierarchy}\\

\noindent
This step directly follows by the definition for coherent forecasts. That is, to obtain reconciled point forecasts for the entire hierarchy, we map $\tilde{\bm{b}}_{t+h} \in \bm{\mathbb{R}}^n$ to the $\mathscr{C}(\bm{S})$ through $\bm{S}$. Thus we have, 
\begin{flalign}
\tilde{\bm{y}}_{t+h}=\bm{S}(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot \hat{\bm{y}}_{t+h}, \quad \tilde{\bm{y}}_{t+h} \in \mathscr{C}(\bm{S})<\bm{\mathbb{R}}^n.
\end{flalign}

\noindent
Finding a suitable $\bm{R}_\bot$ with respect to a certain loss function will result optimally reconciled point forecasts of the hierarchy. Notice that, if the mapping $\bm{g}$ in definition 4.1 is considered to be linear we have,
\begin{flalign}
\bm{g}=(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot.
\end{flalign}
\noindent
In previous studies on hierarchical point forecasting, $\bm{g}$ is considered as a $m \times n$ matrix $\bm{P}$ and thus $\tilde{\bm{y}}_{t+h}=\bm{S}\bm{P} \hat{\bm{y}}_{t+h}$. 
In other words, $\bm{g}$ linearly projects incoherent point forecasts onto the $\mathscr{C}(\bm{S})$. Further in our context, we need to find $\bm{R}_\bot$ such that $\bm{R}^T_\bot \bm{S}$ is invertible. i.e, $(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot \bm{S}=\bm{I}$. This condition coincides with the unbiased condition $\bm{SPS}=\bm{S}$ proposed by \citet{Hyndman2011}. 

\noindent
In their study, \citet{Hyndman2011} proposed to choose,
\begin{flalign*}
\tilde{\bm{b}}^{OLS}_{t+h}=(\bm{S}^T \bm{S})^{-1}\bm{S}^T \hat{\bm{y}}_{t+h},
\end{flalign*}
\noindent
where in this context, $\bm{R}^T_\bot = \bm{S}^T$. Thus the reconciled point forecasts for the entire hierarchy is given by, 
\begin{flalign}
\tilde{\bm{y}}^{OLS}_{t+h}=\bm{S}(\bm{S}^T \bm{S})^{-1}\bm{S}^T \hat{\bm{y}}_{t+h}.
\end{flalign}
\noindent
They referred this to as OLS solution and the loss function they considered is equivalent to the euclidean norm between $\hat{\bm{y}}_{t+h}$ and $\tilde{\bm{y}}_{t+h}$, i.e. $<\hat{\bm{y}}_{T+h}, \tilde{\bm{y}}_{T+h}>$.\\

\noindent
According to a recent study by \citet{Wickramasuriya2017}, choosing $\bm{R}^T_\bot = \bm{S}^T\hat{\bm{W}}^{-1}_{T+h}$ will minimize the trace of mean squared reconciled forecasts errors under the property of unbiasedness. $\hat{\bm{W}}^{-1}_{T+h}$ is the variance of the incoherent forecast errors. This will result, 
\begin{flalign*}
\tilde{\bm{b}}^{MinT}_{t+h}=(\bm{S}^T\hat{\bm{W}}^{-1}_{T+h} \bm{S})^{-1}\bm{S}^T\hat{\bm{W}}^{-1}_{T+h} \hat{\bm{y}}_{t+h},
\end{flalign*}
and thus,
\begin{flalign}
\tilde{\bm{y}}^{MinT}_{t+h}=\bm{S}(\bm{S}^T \hat{\bm{W}}^{-1}_{T+h}\bm{S})^{-1}\bm{S}^T\hat{\bm{W}}^{-1}_{T+h} \hat{\bm{y}}_{t+h}.
\end{flalign}

\noindent
They referred this to as MinT solution. It is also worth to notice that the loss function they considered is equivalent to the Mahalanobis distance between $\hat{\bm{y}}_{T+h}$ and $\tilde{\bm{y}}_{T+h}$. i.e. $<\hat{\bm{y}}_{T+h}, \tilde{\bm{y}}_{T+h}>_{\hat{\bm{W}}}$.  
 

\subsection{Probabilistic forecast reconciliation}

In terms of probabilistic forecasts, the reconciliation implies finding the probability measure of the coherent forecasts using the information of incoherent probabilistic forecast measure. A more formal definition is given below. \\

\noindent
\textbf{Definition 4.2}\\
\noindent
Suppose $(\bm{\mathbb{R}}^n, \mathscr{F}^n, \hat{\nu})$ be an incoherent probability measure space and $(\bm{\mathbb{R}}^m, \mathscr{F}^m, \nu^m)$ be a probability measure space defined on $\bm{\mathbb{R}}^m$. Let $\bm{g}:\mathbb{R}^n \rightarrow \mathbb{R}^m $. Then the probability measure on reconciled bottom levels is such that, 
\begin{flalign}
\nu^m(\bm{A}) = \hat{\nu}(\bm{g}^{-1}(\bm{A})), \quad \forall \quad \bm{A} \in \mathscr{F}^m.
\end{flalign}
\noindent
Further the reconciled probability measure of the whole hierarchy is given by, 
\begin{flalign}
\tilde{\nu}(\bm{S}(\bm{A})) = \hat{\nu}(\bm{g}^{-1}(\bm{A})), \quad \forall \quad \bm{A} \in \mathscr{F}^m,
\end{flalign}
\noindent
where, $\bm{S}:\mathbb{R}^m \rightarrow \mathbb{R}^n$ and $\tilde{\nu}(.)$ is the probability measure on the measure space $(\mathscr{C}(\bm{S}), \mathscr{F}_S)$.\\

\noindent
Since the above definition seems not to be straight forward in reconciling incoherent forecasts, the following content explains how this can be used in practice to obtain reconciled probabilistic forecasts for hierarchical time series. \\

\noindent
Recall that $\hat{\bm{y}}_{t+h}$ is a set of incoherent point forecasts and the coordinates of that with respect to the basis $\bm{B}$ is given by (\ref{4.3}). Suppose $\hat{\bm{f}}(.)$ is the probability density of $\hat{\bm{y}}_{t+h}$. Our goal is to reconcile $\hat{\bm{f}}(.)$ such that the density lives on the $\mathscr{C}(\bm{S})$. In order to obtain this reconciled density, we need to project $\hat{\bm{f}}(\hat{\bm{y}}_{t+h})$ onto the $\mathscr{C}(\bm{S})$ along the direction of $\mathscr{C}(\bm{R})$. \\

\noindent
Let the density of coordinates of $\hat{\bm{y}}_{t+h}$ with respect to basis $\bm{B}$ is denoted by $\bm{f_B}(.)$. Then it follows from  (\ref{4.3}) and the facts on density of transformed variables, 

\begin{flalign}\label{4.5}
\bm{f_B}(\tilde{\bm{b}}_{t+h},\tilde{\bm{t}}_{t+h})=\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) \quad \Big|\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array}\Big|,
\end{flalign}
\noindent
where $|.|$ denote the determinant of a matrix. Now that we have the density of $\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T $, the marginal density of $(\tilde{\bm{b}}_{t+h})$ can be obtained by integrating (\ref{4.5}) over the range of $\tilde{\bm{t}}_{t+h}$. This will result the reconciled density of the bottom level series $(\tilde{\bm{b}}_{t+h})$. i.e.,
\begin{flalign}\label{4.6}
\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})=\int_{lim(\tilde{\bm{t}}_{t+h})}\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) \quad \Big|\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array}\Big| \quad d\tilde{\bm{t}}_{t+h}.
\end{flalign}

\noindent
Finally to get the reconciled density of the whole hierarchy, we simply follow the definition $(3.2)$ and have, 
\begin{flalign}\label{4.7}
\tilde{\bm{f}}(\tilde{\bm{y}}_{t+h})=\bm{S}\circ \tilde{\bm{f}}(\tilde{\bm{b}}_{t+h}).
\end{flalign}

\noindent
This final step will transform every point in the density $\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})$ to the $\mathscr{C}(\bm{S})<\bm{\mathbb{R}}^n$\\

\noindent
\textbf{Example 2}\\

\noindent  
Suppose $\hat{\bm{Y}}_{t+h} \sim \mathscr{N}(\hat{\bm{\mu}}_{t+h}, \hat{\bm{\Sigma}}_{t+h}) \leftrightarrow^d \hat{\bm{f}}(\hat{\bm{y}}_{t+h})$. Then from (\ref{4.5}) it follows,

\begin{flalign*}
\bm{f_B}(\tilde{\bm{b}}_{t+h},\tilde{\bm{t}}_{t+h})=\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) \quad \Big|\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array}\Big| = \frac{\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) }{\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|}. 
\end{flalign*}
\noindent
By substituting the Gaussian distribution function to $\bm{f_B}(.)$ we get, 

\begin{flalign*}
\bm{f_B}(.)&=\frac{\exp\left\{-\frac{1}{2}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}-\bm{\hat{\mu}}_{t+h})^T \bm{\hat{\Sigma}_{t+h}}^{-1}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}-\bm{\hat{\mu}}_{t+h})\right\}}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|},\\
&= \frac{\exp\left\{-\frac{1}{2}\Big(\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T-\bm{\hat{\mu}}_{t+h}\Big)^T \bm{\hat{\Sigma}_{t+h}}^{-1}\Big(\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T-\bm{\hat{\mu}}_{t+h}\Big)\right\}}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|},\\
\end{flalign*}

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|}
\exp \Big\{&-\frac{1}{2}\Big(\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T-\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1}\bm{\hat{\mu}}_{t+h}\Big)^T\\
&\Big[\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^T\Big]^{-1}\\ 
&\Big(\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T-\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1}\bm{\hat{\mu}}_{t+h}\Big) \Big\}.
\end{flalign*}
\noindent
Recall that, $\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1} = \begin{pmatrix}
(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot\\ \cdots \\ (\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot
\end{pmatrix} = \begin{pmatrix}
\bm{P}\\\bm{Q}
\end{pmatrix},$(say). Then, 

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix} \Big|}
\exp \Big\{&-\frac{1}{2}\Big[\begin{pmatrix}\tilde{\bm{b}}_{t+h}\\ \tilde{\bm{t}}_{t+h}\end{pmatrix}-\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\mu}}_{t+h}\Big]^T\Big[\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big]^{-1}\\ 
&\Big[\begin{pmatrix}\tilde{\bm{b}}_{t+h}\\ \tilde{\bm{t}}_{t+h}\end{pmatrix}-\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\mu}}_{t+h}\Big] \Big\},
\end{flalign*}

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big|^{\frac{1}{2}}}
\exp \Big\{&-\frac{1}{2} \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix}^T \Big[\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big]^{-1}\\
& \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix} \Big\}.
\end{flalign*}
\noindent
Since, $\Big[\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big] = \begin{pmatrix}
\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T\\
\bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T
\end{pmatrix}$ we have, 

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\begin{pmatrix}
	\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T\\
	\bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T
	\end{pmatrix}\Big|^{\frac{1}{2}}}
\exp \Big\{&-\frac{1}{2} \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix}^T\\ 
&\Big[\begin{pmatrix}
\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T\\
\bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T
\end{pmatrix}\Big]^{-1} \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix} \Big\}.
\end{flalign*}
\noindent
$\bm{f_B}(.)$ gives the joint distribution of $\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T $, which is a multivariate Gaussian distribution. Then from (\ref{4.6}) and the properties of marginalization of multivariate Gaussian distribution it follows,

\begin{flalign}\label{ex:2.1}
\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T\Big|^{\frac{1}{2}}}
\exp \Big\{-\frac{1}{2} (\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h})^T (\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T)^{-1}(\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}) \Big\}
\end{flalign}

\noindent
(\ref{ex:2.1} ) implies $\tilde{\bm{b}}_{t+h} \sim \mathscr{N}(\bm{P}\hat{\bm{\mu}}_{t+h}, \bm{P}\hat{\bm{\Sigma}}_{t+h}\bm{P}^T)$ where $\bm{P} = (\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot$. Then from (\ref{4.7}) it follows that 

\begin{flalign}
\tilde{\bm{f}}(\tilde{\bm{y}}_{t+h})=\tilde{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}).
\end{flalign}
\noindent
That is the reconciled Gaussian forecast distribution of the whole hierarchy is $\mathscr{N}(\bm{SP}\hat{\bm{\mu}}_{t+h}, \bm{SP}\hat{\bm{\Sigma}}_{t+h}\bm{P}^T\bm{S}^T)$.



\section{Evaluation of hierarchical probabilistic forecasts}

The necessary final step in hierarchical forecasting is to make sure that our forecast distributions are accurate enough to predict the uncertain future. In general, forecasters prefer to maximize the sharpness of the predictive distribution subject to the calibration \citep{Gneiting2014}. Therefore the probabilistic forecasts should be evaluated with respect to these two properties.\\

\noindent
Calibration refers to the statistical compatibility between probabilistic forecasts and realizations. In other words, random draws from a perfectly calibrated predictive distribution should be equivalent to the realizations. The uniformity in Probability Integral Transformation (PIT) of predictive densities evaluated at realizations implies well calibrated probabilistic forecasts. Therefore, assessing the uniformity in PIT of predictive densities, by inspecting their histograms and correlograms can be used as a simple tool for evaluating the calibration of probabilistic forecasts.  \\

\noindent
On the other hand, sharpness refers to the spread or the concentration of prediction distributions and it is a property of forecasts only. The more concentrated the predictive distributions, the sharper the forecasts are \citep{Gneiting2008}. The sharpness of density forecasts can be assessed in terms of the prediction intervals. Sharper density forecasts will have shorter prediction intervals. However, independently assessing the calibration and sharpness will not help to properly evaluate the probabilistic forecasts. Therefore to assess these properties simultaneously, we use scoring rules. \\

\noindent
Scoring rules are summary measures obtained based on the relationship between predictive distribution and the realizations. In some studies, researchers take the scoring rules to be positively oriented which they would wish to maximize \citep{Gneiting2007}. However, scoring rules were also defined to be negatively oriented which forecasters wish to minimize \citep{Gneiting2014}. We consider these negatively oriented scoring rules to evaluate probabilistic forecasts in hierarchical time series. \\

\noindent
Let $\bm{Y}$ be a $n$-dimensional random vector from the predictive distribution $\mathbold{F}$ and $\mathbold{y}$ be $n$-dimension vector of realizations. Scoring rule is a numerical value $S(\mathbold{Y,y})$ assign to each pair $(\mathbold{Y,y})$. Suppose the true distribution of $\mathbold{y}$ is given by $\mathbold{G}$. Then the proper scoring rule is defined as,
\begin{equation}\label{eq:(3.1.)}
S(\mathbold{G,G}) \le S(\mathbold{Y,G}),
\end{equation}
\noindent
where $S(\mathbold{Y,G})=E_{\mathbold{G}}[S(\mathbold{Y,y})]$ is the expected score under the true distribution $\mathbold{G}$ \citep{Gneiting2008, Gneiting2014}. \\

\noindent
Let $\bm{\tilde{Y}}_{T+h}$ and $\bm{\tilde{Y}}'_{T+h}$ be two independent random vectors from the coherent forecast distribution $\tilde{\bm{F}}$ with the density function $\tilde{\bm{f}}(.)$ at time $t+h$ and $\bm{y}_{T+h}$ is the vector of realizations. Further $\tilde{Y}_{T+h,i}$ and $\tilde{Y}_{T+h,j}$ are $i^{\text{th}}$ and $j^{\text{th}}$ components of the vector $\tilde{\bm{Y}}_{T+h}$. Then the following table summarizes few existing proper scoring rules. 


\begin{center}
	\captionof{table}{\textit{Scoring rules to evaluate multivariate forecast densities }} 
	
	
	\resizebox{\linewidth}{!}{
		
		\begin{tabular}{ L | L | L}
			\hline
			\hline
			\textbf{Scoring rule} & \textbf{Expression} & \textbf{Reference}\\
			\\
			\hline
			\hline \\
			\text{Log score} & LS(\tilde{\bm{F}},\bm{y}_{T+h}) = -log {\tilde{\bm{f}}(\bm{y}_{T+h})} & \text{\citet{Gneiting2007}} \\
			\\ 
			\hline \\
			\text{Energy score} & eS(\bm{\tilde{Y}_{T+h},y_{T+h}}) = E_{\tilde{\bm{F}}}||\tilde{\bm{Y}}_{T+h}-\bm{y}_{T+h}||^\alpha - \frac{1}{2}E_{\tilde{\bm{F}}}||\tilde{\bm{Y}}_{T+h}-\tilde{\bm{Y}}'_{T+h}||^\alpha, \alpha \in (0,2] & \text{\citet{Gneiting2008}} \\
			\\
			\hline
			\text{Variogram score} & VS(\tilde{\bm{F}}, \bm{y}_{T+h}) = \displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n}w_{ij}\left(|y_{T+h,i} - y_{T+h,j}|^p - E_{\tilde{\bm{F}}}|\tilde{Y}_{T+h,i}-\tilde{Y}_{T+h,j}|^p\right)^2 & \text{\citet{SCHEUERER2015}} \\
			\hline
		\end{tabular}
	
		}
\end{center}

\noindent
Log scores can be used to evaluate the hierarchical probabilistic forecasts if it is reasonable to assume a parametric forecast density for the hierarchy. However the \qq{degenerecy} of the coherent forecast densities would be problematic when using log scores. We will discuss more about this in the next sub section.\\

\noindent
In the energy score, for $\alpha=2$, it can be easily shown that
\begin{equation} \label{eq:(5.1)}
eS(\bm{\tilde{Y}_{T+h},y_{T+h}}) = ||\bm{y}_{T+h}-\tilde{\bm{\mu}}_{T+h}||^2,
\end{equation}

\noindent
where $\tilde{\bm{\mu}}_{T+h} =E_{\bm{F}}(\tilde{\bm{Y}}_{T+h}) $. Therefore in the limiting case, the energy score only measures the accuracy of the forecast mean, but not the entire distribution. Further \citet{Pinson2013a} argued that the Energy score given in table 1 has a very low discrimination ability for incorrectly specified covariances, even though it discriminates the misspecified means well. \\

\noindent
The variogram score given in Table $1$ is for order $p$ where, $w_{ij}$ are non-negative weights. \citet{SCHEUERER2015} have shown that the variogram score has a high discrimination ability of misspecified means, variance and correlation structure than the Energy score. Further they suggested the variogram score with $p=0.5$ is more powerful.\\

\noindent
For a possible finite sample of size $B$ from the multivariate forecast density $\tilde{\bm{F}}$, the variogram score is defined as, 

\begin{flalign}
VS(\tilde{\bm{F}}, \bm{y}_{T+h}) = \displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n}w_{ij}\left(|y_{T+h,i} - y_{T+h,j}|^p - \frac{1}{m} \displaystyle\sum_{k=1}^{B} |\tilde{Y}^k_{T+h,i}-\tilde{Y}^k_{T+h,j}|^p\right)^2.
\end{flalign}
  



%For example, assume you have a Gaussian coherent predictive density $\bm{\tilde{f}}_{T+h}$. Then the log score is given by, 

%\begin{equation} 
%LS(\bm{\tilde{f}_{T+h},y_{T+h}}) = -\log \left(\frac{1}{(2\pi)^{\frac{m}{2}}|\bm{\tilde{\Sigma}}_{T+h}|^{\frac{1}{2}}}\exp\left[-\frac{1}{2}(\bm{y}_{T+h}-\bm{\tilde{\mu}}_{T+h})^T %\bm{\tilde{\Sigma}_{T+h}}^{-1}(\bm{y}_{T+h}-\bm{\tilde{\mu}}_{T+h})\right]\right),
%\end{equation}

%\noindent
%where $\bm{\tilde{\mu}}_{T+h}$ and $\bm{\tilde{\Sigma}}_{T+h}$ are the coherent mean and variance-covariance matrix of the predictive Gaussian density and both lies in the linear %subspace spanned by $\mathscr{C}(\bm{S})$. Then $\bm{\tilde{\Sigma}}_{T+h}$ is always a singular matrix and hence the determinant will be zero. Therefore the density function of %degenerate Gaussian distribution is undefined. However, we could use the pseudo determinant, i.e. product of positive eigenvalues and the pseudo inverse to calculate the log score %to compare the predictive ability of coherent forecast densities.  



\subsection{Evaluating coherent forecast densities}

As it was mentioned in the previous section, any coherent hierarchical forecast density is a degenerate density. To the best of our knowledge, there is no proper multivariate scoring rule that has been introduced in literature to evaluate densities with degeneracy. Further it can be easily seen that some of the existing scoring rules breakdown under the degeneracy. For example take the log score in the univariate case. Suppose the true density is degenerate at $x=0$, i.e. $f(x)=\mathbb{1}\{x=0\}$.  Now consider two predictive densities $p_1(x)$ and $p_2(x)$. Let $p_1(x)$ is equivalent to the true density, i.e. $p_1(x)=\mathbb{1}\{x=0\}$ and $p_2(x) =^d N(0,\sigma^2)$ with $\sigma^2 < (2\pi)^{-1}$. The expected log score of $p_1$ is:
$$S(f,f) = S(p_1,f) -ln[p_1(x=0)]=0,$$
\noindent
and that of $p_2$ is:
$$S(p_2,f) = -ln[p_2(x=0)]<0.$$ 
\noindent
Therefore $S(f,f) > S(p_2,f)$ and hence there exist at least one forecast density which breaks the condition (\ref{eq:(3.1.)}) for proper scoring rule. This implies log score cannot be used to evaluate the degenerate densities.  \\

\noindent
This suggest that it is necessary to have a rule of thumb to use these scoring rules in order to evaluate coherent forecast densities. First we should notice that, even though the coherent distribution of the entire hierarchy is degenerate, the density of the basis set of series is non-degenerate since these series are linearly independent. Further, if we can correctly specify the forecast distribution of these basis set of series, then we have almost obtained the correct forecast distribution of the whole hierarchy. Therefore, we propose to evaluate the predictive ability of only the basis set of series of the coherent forecast density by using any of the above discussed multivariate scoring rules. This will also avoid the impact of degeneracy for the scoring rules. \\

\noindent
For example, since the bottom level series is a set of basis series for a given hierarchy, we can evaluate the predictive ability of the bottom level series of the coherent forecast distribution instead of evaluating the whole distribution. Further if our purpose is to compare two coherent forecast densities, we can compare the forecast ability of only the bottom level forecast densities. 


\subsection{Comparison of coherent and incoherent forecast densities}

It is also important to assess how the coherent or reconciled forecast densities improve the predictive ability compared to the incoherent forecasts. Clearly we cannot use multivariate scoring rules, even for the basis set of series, since the coherent and incoherent forecast densities lie in two different matrix spaces.\\

\noindent
However you could compare individual margins of the forecast density of the hierarchy using univariate proper scoring rules. Most widely used Continuous Ranked Probability Score (CRPS) for evaluating univariate forecast densities would be helpful for this. \\

\begin{equation} \label{eq:(3.6)}
CRPS(\tilde{F}_i,y_{T+h,i}) = E_{\tilde{F}_i}|\tilde{Y}_{T+h,i}-y_{T+h,i}| - \frac{1}{2}E_{\tilde{F}_i}|\tilde{Y}_{T+h,i}-\tilde{Y}'_{T+h,i}|,
\end{equation}
       
\noindent
where $\tilde{Y}_{T+h,i}$ and $\tilde{Y}'_{T+h,i}$ are two independent copies from the $i^{\text{th}}$ reconciled marginal forecast distribution $\tilde{F}_i$ of the hierarchy and $y_{T+h,i}$ is the $i^{\text{th}}$ realization from the true marginal distribution $G_i$. 
 



\subsection{Simulation study}

\section{Conclusions.}




   



\newpage
\printbibliography
\end{document}


