
\documentclass[a4paper, 11pt]{article}
\usepackage[a4paper, inner=3cm, outer=3cm, top=2cm, bottom=3cm]{geometry}
\usepackage{amsmath, amssymb, fixmath, qtree, bm, calrsfs, enumitem,multirow, caption, textcmds,siunitx,adjustbox, mathrsfs, float, amstext}
\usepackage[bb=boondox]{mathalfa}
\usepackage{tikz}
\usepackage[natbib=true,backend=bibtex,style=authoryear-comp,dashed=false,maxbibnames=99,firstinits=true,maxcitenames=3,url=true,doi=false,isbn=false]{biblatex}
\usepackage[pdftex,colorlinks=true]{hyperref}
\hypersetup{citecolor=blue,linkcolor=blue,urlcolor=blue}
\DeclareNameAlias{sortname}{last-first}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type


\bibliography{References_paper1}
\begin{document}


\title{Probabilistic Forecasts in Hierarchical Time Series}
\maketitle


\section{Introduction.}
\begin{itemize}
	\item Introduction to hierarchical time series including a literature review in the context of point forecasts. 
	\item Importance of probabilistic forecasts in time series.
	\item Lack of attention in the context of probabilistic forecasts in hierarchical literature.
	
\end{itemize}

\section{Notations}
We first introduce notations where possible we follow \citet{Wickramasuriya2017}. Suppose $\mathbold{y}_t \in \bm{\mathbb{R}}^n$ comprising all observations of the whole hierarchy at time $t$ and $\mathbold{b}_t \in \bm{\mathbb{R}}^m$ comprising only the bottom level observations at  time $t$. Then due to the aggregation nature of the hierarchy we have, 

\begin{equation}
\mathbold{y}_t = \mathbold{Sb}_t,
\end{equation}
where $\mathbold{S}$ is a $n \times m$ constant matrix whose columns span the linear subspace for which all constraints hold. Since this linear subspace is equivalent to the column space of $\bm{S}$, we denote it as $\mathscr{C}(\bm{S})$. Further the null space of $\bm{S}$ which is orthogonal to this linear subspace is denoted by $\mathscr{N}(\bm{S})$. 
To understand the notations clearly, consider the hierarchy given in Figure 1.\\



\begin{figure}
	\begin{center}
		\leaf{AA} \leaf{AB} 
		\branch{2}{A}
		\leaf{BA} \leaf{BB}
		\branch{2}{B}
		\branch{2}{Tot}
		\qobitree
	\end{center}
	\caption{Two level hierarchical diagram}
\end{figure}
\noindent
In any hierarchy, the most aggregated level is termed as level 0, the second most aggregated level is termed as level 1 and so on. This example consists of two levels. At a particular time $t$, let $y_{T,t}$ denote the observation at level 0; $y_{A,t}, y_{B,t} $ denote observations at level 1; and $y_{AA,t}, y_{AB,t}, y_{AC,t}, y_{BA,t}, y_{BB,t}$ denote observations at level 2. Then $\mathbold{y}_t = [y_{T,t},y_{A,t}, y_{B,t},y_{C,t},y_{AA,t}, y_{AB,t}, y_{AC,t}, y_{BA,t}, y_{BB,t}]^T$, \\$\mathbold{b}_t = [y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}]^T$, $m=4$, $n=7$, and $$ \mathbold{S} = \begin{pmatrix} 1& 1 &1 &1  \\ 1 &1 & 0 &0 \\   0&0  & 1 & 1 \\ & & I_4 &   \end{pmatrix}, $$ where $I_4$ is a $4$-dimension identity matrix. \\  


\section{Coherent forecasts}

The main purpose of this section is to provide formal definitions for coherent forecasts. We start with redefining the coherent point forecasts using the properties of vector spaces which is then followed by the definition of coherent probabilistic forecasts. 
\\

\noindent
\textbf{Definition 3.1: Coherent Point Forecasts}\\
\noindent
Suppose $\bm{\breve{y}}_{t+h} \in \mathbb{R}^n$ consists point forecasts of each series in the hierarchy at time $t+h$.  $\bm{\breve{y}}_{t+h}$ is said to be \textit{Coherent} if it lies in a $m$-dimensional subspace of $\bm{\mathbb{R}}^n$ which is spanned by the columns of $\bm{S}$. \\

\noindent
\textbf{Definition 3.2: Coherent Probabilistic Forecasts}\\
\noindent
Let $(\bm{\mathbb{R}}^m, \bm{\mathscr{F}}^m, \nu^m)$ be a probability measure space where $\mathscr{F}^m$ is a sigma algebra on $\bm{\mathbb{R}}^m$. $\breve{\nu}(.)$ on $(\mathscr{C}(\bm{S}), \mathscr{F}_{\bm{S}})$ is said to be coherent probability measure iff $$\breve{\nu}(\bm{S}(\bm{A})) = \nu^m(\bm{A}) \quad \forall \quad \bm{A} \in \mathscr{F}^m,$$ where $\bm{S}(\bm{A})$ denotes the image of subset $\bm{A}$ under $\bm{S}$. 

\usetikzlibrary{arrows,positioning,shapes,fit,calc}

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[
		>=stealth,
		bullet/.style={
			fill=black,
			circle,
			minimum width=1.5cm,
			inner sep=0pt
		},
		projection/.style={
			->,
			thick,
			label,
			shorten <=2pt,
			shorten >=2pt
		},
		every fit/.style={
			ellipse,
			draw,
			inner sep=0pt
		}
		]
		
		\node at (2,3) {$\bm{S}$};
		
		\node at (0,5) {$\bm{\mathbb{R}}^m$(domain of $\bm{S}$)};
		\node at (4,5) {$\bm{\mathbb{R}}^n$(codomain of $\bm{S}$)};
		\node at (4,1.0) {$\mathscr{C}(\bm{S})$};
		%\node[bullet,label=below:$f(x)$] at (4,2.5){};
		
		
		\draw (0,2.5) ellipse (1.02cm and 2.2cm);
		\draw (4,2.5) ellipse (1.02cm and 2.2cm);
		\draw (4,2.5) ellipse (0.51cm and 1.1cm);
		
		
		\draw[projection, label=below:$f$] (0.3,2.5) -- (3.8,2.5) ;
		
		\end{tikzpicture}
		\newline
	\end{center}
	\caption{Any set $\bm{A} \in \bm{\mathbb{R}}$ will be mapped to the $\mathscr{C}(\bm{S})$ through the mapping $\bm{S}$}
\end{figure}
\noindent
Definition 3.2 implies the probability measure on $\mathscr{C}(\bm{S})$ is equivalent to the probability measure on $(\bm{\mathbb{R}}^m, \bm{\mathscr{F}}^m)$. Hence, there is no density anywhere outside the linear subspace spanned by the columns of $\bm{S}$. That is, a \textit{Coherent probability density forecast} is any density $\bm{f}(\bm{\breve{y}}_{t+h})$ such that $\bm{f}(\bm{\breve{y}}_{t+h})=0$ for all $\bm{\breve{y}}_{t+h}$ in the null space of $\bm{S}$. Following example will help to understand these definitions more clearly.\\

\noindent
\textbf{Example 1}\\
\noindent
Consider a simple hierarchy with two bottom level series A and B that add up to the top level series T. Suppose the forecasts at time $t+h$ of these series are given by $\mathbold{\breve{y}}_{t+h} = [\breve{y}_{T,t+h},\breve{y}_{A,t+h}, \breve{y}_{B,t+h}]$ and $\mathbold{\breve{y}}_{t+h} \in \bm{\mathbb{R}}^3$. Due to the aggregation constraint of the hierarchy we have $\breve{y}_{T,t+h}=\breve{y}_{A,t+h}+\breve{y}_{B,t+h}$. This implies, even though  $\mathbold{\breve{y}}_{t+h}$ is in $\bm{\mathbb{R}}^3$, the points actually lie in a two dimensional subspace within that $\bm{\mathbb{R}}^3$ space. This subspace is equivalent to $\mathscr{C}(\bm{S})$ for this simple hierarchy. Therefore, for any $\mathbold{\breve{y}}_{t+h} \in \mathscr{N}(\bm{S})$ have a zero probability. I.e. $f(\mathbold{\breve{y}}_{t+h})=0$ for any $\mathbold{\breve{y}}_{t+h} \in \mathscr{N}(\bm{S})$.\\

\noindent
By the definition of coherent forecasts, it is clear that there are only $m$ number of linear independent series in the whole hierarchy. We refer to these as $m$ dimensional \textit{basis set of series} since these series generates a \textit{basis} that spans the $\mathscr{C}(\bm{S})$. Other $n-m$ series of the hierarchy can be linearly determined by these basis set of series. This implies that any coherent density is a degenerate density. The $m$ number of bottom level series of a given hierarchy can be considered as a basis set of series. Then the columns of $\bm{S}$ is a basis that generates through these bottom level series. This basis spans the linear subspace where the degenerate density lives on.\\

\noindent
However bottom level series are not the only set of basis series and we can find many other basis set of series which generates basis that spans the same $\mathscr{C}(\bm{S})$ for a given hierarchy. If we go back to the hierarchy in example 1, instead of two bottom level series $(\bm{A,B})$ we can take $(\bm{T,A})$ as the basis set of series and the basis that generates through $(\bm{T,A})$ will be $\{\begin{pmatrix}
1&0&1 \end{pmatrix}^T, \begin{pmatrix}
0&1&-1 \end{pmatrix}^T\}$. Another possible basis would be the singular value decomposition of $\bm{S}$.\\

\noindent
An important thing to notice here is all of these basis spans the same linear subspace equivalent to the $\mathscr{C}(\bm{S})$. Therefore the definition $(3.2)$ is not unique and one can redefine the coherent probabilistic forecasts with respect to any basis. However we stick to the definition $(3.2)$ and consider the basis defined by the columns of $\bm{S}$ that generates through the bottom levels of the hierarchy in what follows.\\ 

\noindent
It is also worth to mention that the definition $(3.1)$ and $(3.2)$ facilitate extension to the forecast reconciliation which we talk about in the next section. In contrast to our definition, \citet{BenTaieb2017} defines the coherent probabilistic forecasts in terms of convolution. According to their definition, if the forecasts are coherent, then the convolution of forecast distributions of disaggregate series is same as the forecast distribution of the corresponding aggregate series. This is the only study that copes with probabilistic forecasts in hierarchical literature thus far. 

\section{Forecast reconciliation}
\noindent
Bottom-up, top-down and middle-out methods are the most traditional forecasting methods that were used to produce coherent point forecasts in early studies on hierarchical literature. In bottom-up approach, forecasts of the lowest level are first generated and they are simply aggregated to forecast the upper levels of the hierarchy \citep{Dunn1976}. 
In contrast, the top-down approach involves forecasting the most aggregated series first and then disaggregating these forecasts down the hierarchy based on the proportions of observed data \citep{Gross1990}. A compromise between these two approaches is the middle-out method which entails forecasting each series of a selected middle level in the hierarchy and then forecasting upper levels by the bottom-up method and lower levels by the top-down method.\\ 

\noindent
These three approaches use only a part of the information available when producing coherent forecasts. This might result inaccurate forecasts. For example, if the bottom level series are highly volatile or too noisy and hence challenging to forecast, then the resulting forecasts from bottom-up approach would be inaccurate.\\

\noindent
As an alternative to these conventional methods, \citet{Hyndman2011} propose to use information from all the levels of the hierarchy and reconcile them to obtain coherent point forecasts. In this approach, independent forecasts of all series are initially obtained. It is very unlikely that these forecasts are coherent. Then, these forecasts are optimally combined through a regression model to obtain coherent forecasts. This study has given arise to the concept of point forecast reconciliation in hierarchical time series. In general, any forecasting method that use a set of incoherent forecasts and revise them to obtain coherent forecasts is referred to as forecast reconciliation method. For example, Minimum Trace reconciliation \citep{Wickramasuriya2017}, GTOP \citep{VanErven2015a}. It is important to notice that bottom-up, top-down and middle-out methods are not reconciliation methods since they only use forecasts from a part of the levels in the hierarchy when producing coherent forecasts. \\


\subsection{Point forecast reconciliation}

Even though the point forecast reconciliation has a well established literature, we would like to redefine this using concepts in linear algebra as a groundwork for the probabilistic forecast reconciliation. \\


\noindent
\textbf{Definition 4.1}\\
\noindent
Let $\bm{g}:\bm{\mathbb{R}}^n \rightarrow \bm{\mathbb{R}}^m $ and $\hat{\bm{y}}_{t+h} \in \bm{\mathbb{R}}^n$ be any set of incoherent forecasts at time $t+h$. Then $\tilde{\bm{b}}_{t+h}$ is said to be reconciled bottom level forecasts if 
\begin{equation}
\tilde{\bm{b}}_{t+h}=\bm{g}(\hat{\bm{y}}_{t+h}),
\end{equation}
\noindent
where $\bm{g}(\hat{\bm{y}}_{t+h})$ is the image of $\hat{\bm{y}}_{t+h}$ under $\bm{g}$ on $\bm{\mathbb{R}}^m$. The reconciled forecasts for the whole hierarchy is then given by $\tilde{\bm{y}}_{t+h}=\bm{S}(\tilde{\bm{b}}_{t+h})$ such that $\tilde{\bm{y}}_{t+h} \in \mathscr{C}(\bm{S})$, where $\bm{S}(\tilde{\bm{b}}_{t+h})$ is the image of $\tilde{\bm{b}}_{t+h}$ under $\bm{S}$ on the $\mathscr{C}(\bm{S}) < \bm{\mathbb{R}}^n$.\\

\noindent
In the following content we explain more on how this definition can be used in practice to reconcile point forecasts in hierarchical time series. Let $\bm{R} \in \bm{\mathbb{R}}^{n \times n-m}$ consists the columns that spans $\mathscr{N}(\bm{S})$ which is orthogonal to $\mathscr{C}(\bm{S})$. $\mathscr{N}(\bm{S})$ is also equivalent to the $\mathscr{C}(\bm{R})$. Note that $\bm{R}$ is also not unique and one example is a matrix whose columns represent the aggregation constraints for the hierarchy. Then for the hierarchy in example 1, $$ \mathbold{S} = \begin{pmatrix} 1& 1 \\ 1 & 0 \\ 0&1 \end{pmatrix} \quad \text{and} \quad \mathbold{R} = \begin{pmatrix}  1 \\ -1 \\ -1 \end{pmatrix}.$$ 
\noindent
Further let $\{\bm{s}_1,...,\bm{s}_m\}$ and $\{\bm{r}_1,...,\bm{r}_{n-m}\}$ denote the columns of $\bm{S}$ and $\bm{R}$ respectively. Then $\bm{B}=\{\bm{s}_1,...,\bm{s}_m, \bm{r}_1,...,\bm{r}_{n-m}\}$ is a basis for $\bm{\mathbb{R}}^n$. Now, using the insights of definition 4.1, we can follow the below steps to reconcile the point forecasts.\\

\noindent
\textbf{Step 1: Obtaining reconciled bottom level point forecasts}\\

\noindent
For a given incoherent set of point forecasts $\hat{\bm{y}}_{t+h} \in \bm{\mathbb{R}}^n$, first we find the coordinates of $\hat{\bm{y}}_{t+h}$ with respect to the basis $\bm{B}$. Let $\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T$ denote these coordinates. Note that $\tilde{\bm{b}}_{t+h}$ is a basis set of series which is really the reconciled bottom level series that generates $\{\bm{s}_1,...,\bm{s}_m\}$ and $\tilde{\bm{t}}_{t+h}$ is another basis set of series that generates $\{\bm{r}_1,...,\bm{r}_{n-m}\}$. Then from basic properties of linear algebra it follows that, 

\begin{flalign}\label{4.1}
\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T = \hat{\bm{y}}_{t+h},
\end{flalign}
\begin{flalign}\label{4.2}
\hat{\bm{y}}_{t+h} = \bm{S}\tilde{\bm{b}}_{t+h} +  \bm{R}\tilde{\bm{t}}_{t+h},
\end{flalign}
\noindent
and
\begin{flalign}\label{4.3}
\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T  = \begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1}\hat{\bm{y}}_{t+h}.
\end{flalign}

\noindent
In order to find $(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1}$, let $\bm{S}_{\bot}$ and $\bm{R}_{\bot}$ be the orthogonal complements of $\bm{S}$ and $\bm{R}$ 
respectively. Then $(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1}$ is given by, 

\begin{flalign}
\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1} = \begin{pmatrix}
(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot\\ \cdots \\ (\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot
\end{pmatrix}.
\end{flalign}

\noindent
Thus we have, 
\begin{flalign} \label{4.4}
\begin{pmatrix}
\tilde{\bm{b}}_{t+h}\\ \cdots \\ \tilde{\bm{t}}_{t+h}
\end{pmatrix} = \begin{pmatrix}
(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot\\ \cdots \\ (\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot
\end{pmatrix}\hat{\bm{y}}_{t+h}.
\end{flalign}

\noindent
From (\ref{4.4}) it follows that, 
\begin{flalign}
\tilde{\bm{b}}_{t+h}=(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot \hat{\bm{y}}_{t+h} 
\end{flalign}
\\

\noindent
\textbf{Step 2: Obtaining reconciled point forecasts for the whole hierarchy}\\

\noindent
This step directly follows by the definition for coherent forecasts. That is, to obtain reconciled point forecasts for the entire hierarchy, we map $\tilde{\bm{b}}_{t+h} \in \bm{\mathbb{R}}^n$ to the $\mathscr{C}(\bm{S})$ through $\bm{S}$. Thus we have, 
\begin{flalign}
\tilde{\bm{y}}_{t+h}=\bm{S}(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot \hat{\bm{y}}_{t+h}, \quad \tilde{\bm{y}}_{t+h} \in \mathscr{C}(\bm{S})<\bm{\mathbb{R}}^n.
\end{flalign}

\noindent
Finding a suitable $\bm{R}_\bot$ with respect to a certain loss function will result optimally reconciled point forecasts of the hierarchy. Notice that, if the mapping $\bm{g}$ in definition 4.1 is considered to be linear we have,
\begin{flalign}
\bm{g}=(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot.
\end{flalign}
\noindent
In previous studies on hierarchical point forecasting, $\bm{g}$ is considered as a $m \times n$ matrix $\bm{P}$ and thus $\tilde{\bm{y}}_{t+h}=\bm{S}\bm{P} \hat{\bm{y}}_{t+h}$. 
In other words, $\bm{g}$ linearly projects incoherent point forecasts onto the $\mathscr{C}(\bm{S})$. Further in our context, we need to find $\bm{R}_\bot$ such that $\bm{R}^T_\bot \bm{S}$ is invertible. i.e, $(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot \bm{S}=\bm{I}$. This condition coincides with the unbiased condition $\bm{SPS}=\bm{S}$ proposed by \citet{Hyndman2011}. 

\noindent
In their study, \citet{Hyndman2011} proposed to choose,
\begin{flalign*}
\tilde{\bm{b}}^{OLS}_{t+h}=(\bm{S}^T \bm{S})^{-1}\bm{S}^T \hat{\bm{y}}_{t+h},
\end{flalign*}
\noindent
where in this context, $\bm{R}^T_\bot = \bm{S}^T$. Thus the reconciled point forecasts for the entire hierarchy is given by, 
\begin{flalign}
\tilde{\bm{y}}^{OLS}_{t+h}=\bm{S}(\bm{S}^T \bm{S})^{-1}\bm{S}^T \hat{\bm{y}}_{t+h}.
\end{flalign}
\noindent
They referred this to as OLS solution and the loss function they considered is equivalent to the euclidean norm between $\hat{\bm{y}}_{t+h}$ and $\tilde{\bm{y}}_{t+h}$, i.e. $<\hat{\bm{y}}_{T+h}, \tilde{\bm{y}}_{T+h}>$.\\

\noindent
According to a recent study by \citet{Wickramasuriya2017}, choosing $\bm{R}^T_\bot = \bm{S}^T\hat{\bm{W}}^{-1}_{T+h}$ will minimize the trace of mean squared reconciled forecasts errors under the property of unbiasedness. $\hat{\bm{W}}^{-1}_{T+h}$ is the variance of the incoherent forecast errors. This will result, 
\begin{flalign*}
\tilde{\bm{b}}^{MinT}_{t+h}=(\bm{S}^T\hat{\bm{W}}^{-1}_{T+h} \bm{S})^{-1}\bm{S}^T\hat{\bm{W}}^{-1}_{T+h} \hat{\bm{y}}_{t+h},
\end{flalign*}
and thus,
\begin{flalign}
\tilde{\bm{y}}^{MinT}_{t+h}=\bm{S}(\bm{S}^T \hat{\bm{W}}^{-1}_{T+h}\bm{S})^{-1}\bm{S}^T\hat{\bm{W}}^{-1}_{T+h} \hat{\bm{y}}_{t+h}.
\end{flalign}

\noindent
They referred this to as MinT solution. It is also worth to notice that the loss function they considered is equivalent to the Mahalanobis distance between $\hat{\bm{y}}_{T+h}$ and $\tilde{\bm{y}}_{T+h}$. i.e. $<\hat{\bm{y}}_{T+h}, \tilde{\bm{y}}_{T+h}>_{\hat{\bm{W}}}$.  
 

\subsection{Probabilistic forecast reconciliation}

In terms of probabilistic forecasts, the reconciliation implies finding the probability measure of the coherent forecasts using the information of incoherent probabilistic forecast measure. A more formal definition is given below. \\

\noindent
\textbf{Definition 4.2}\\
\noindent
Suppose $(\bm{\mathbb{R}}^n, \mathscr{F}^n, \hat{\nu})$ be an incoherent probability measure space and $(\bm{\mathbb{R}}^m, \mathscr{F}^m, \nu^m)$ be a probability measure space defined on $\bm{\mathbb{R}}^m$. Let $\bm{g}:\mathbb{R}^n \rightarrow \mathbb{R}^m $. Then the probability measure on reconciled bottom levels is such that, 
\begin{flalign}
\nu^m(\bm{A}) = \hat{\nu}(\bm{g}^{-1}(\bm{A})), \quad \forall \quad \bm{A} \in \mathscr{F}^m.
\end{flalign}
\noindent
Further the reconciled probability measure of the whole hierarchy is given by, 
\begin{flalign}
\tilde{\nu}(\bm{S}(\bm{A})) = \hat{\nu}(\bm{g}^{-1}(\bm{A})), \quad \forall \quad \bm{A} \in \mathscr{F}^m,
\end{flalign}
\noindent
where, $\bm{S}:\mathbb{R}^m \rightarrow \mathbb{R}^n$ and $\tilde{\nu}(.)$ is the probability measure on the measure space $(\mathscr{C}(\bm{S}), \mathscr{F}_S)$.\\

\noindent
Since the above definition seems not to be straight forward in reconciling incoherent forecasts, the following content explains how this can be used in practice to obtain reconciled probabilistic forecasts for hierarchical time series. \\

\noindent
Recall that $\hat{\bm{y}}_{t+h}$ is a set of incoherent point forecasts and the coordinates of that with respect to the basis $\bm{B}$ is given by (\ref{4.3}). Suppose $\hat{\bm{f}}(.)$ is the probability density of $\hat{\bm{y}}_{t+h}$. Our goal is to reconcile $\hat{\bm{f}}(.)$ such that the density lives on the $\mathscr{C}(\bm{S})$. In order to obtain this reconciled density, we need to project $\hat{\bm{f}}(\hat{\bm{y}}_{t+h})$ onto the $\mathscr{C}(\bm{S})$ along the direction of $\mathscr{C}(\bm{R})$. \\

\noindent
Let the density of coordinates of $\hat{\bm{y}}_{t+h}$ with respect to basis $\bm{B}$ is denoted by $\bm{f_B}(.)$. Then it follows from  (\ref{4.3}) and the facts on density of transformed variables, 

\begin{flalign}\label{4.5}
\bm{f_B}(\tilde{\bm{b}}_{t+h},\tilde{\bm{t}}_{t+h})=\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) \quad \Big|\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array}\Big|,
\end{flalign}
\noindent
where $|.|$ denote the determinant of a matrix. Now that we have the density of $\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T $, the marginal density of $(\tilde{\bm{b}}_{t+h})$ can be obtained by integrating (\ref{4.5}) over the range of $\tilde{\bm{t}}_{t+h}$. This will result the reconciled density of the bottom level series $(\tilde{\bm{b}}_{t+h})$. i.e.,
\begin{flalign}\label{4.6}
\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})=\int_{lim(\tilde{\bm{t}}_{t+h})}\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) \quad \Big|\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array}\Big| \quad d\tilde{\bm{t}}_{t+h}.
\end{flalign}

\noindent
Finally to get the reconciled density of the whole hierarchy, we simply follow the definition $(3.2)$ and have, 
\begin{flalign}\label{4.7}
\tilde{\bm{f}}(\tilde{\bm{y}}_{t+h})=\bm{S}\circ \tilde{\bm{f}}(\tilde{\bm{b}}_{t+h}).
\end{flalign}

\noindent
This final step will transform every point in the density $\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})$ to the $\mathscr{C}(\bm{S})<\bm{\mathbb{R}}^n$. Following example illustrates how this method can be used to reconcile an incoherent Gaussian forecast distribution.\\

\noindent
\textbf{Example 2}\\

\noindent  
Suppose $\hat{\bm{Y}}_{t+h} \sim \mathscr{N}(\hat{\bm{\mu}}_{t+h}, \hat{\bm{\Sigma}}_{t+h}) \leftrightarrow^d \hat{\bm{f}}(\hat{\bm{y}}_{t+h})$. Then from (\ref{4.5}) it follows,

\begin{flalign*}
\bm{f_B}(\tilde{\bm{b}}_{t+h},\tilde{\bm{t}}_{t+h})=\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) \quad \Big|\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array}\Big| = \frac{\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) }{\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|}. 
\end{flalign*}
\noindent
By substituting the Gaussian distribution function to $\bm{f_B}(.)$ we get, 

\begin{flalign*}
\bm{f_B}(.)&=\frac{\exp\left\{-\frac{1}{2}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}-\bm{\hat{\mu}}_{t+h})^T \bm{\hat{\Sigma}_{t+h}}^{-1}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}-\bm{\hat{\mu}}_{t+h})\right\}}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|},\\
&= \frac{\exp\left\{-\frac{1}{2}\Big(\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T-\bm{\hat{\mu}}_{t+h}\Big)^T \bm{\hat{\Sigma}_{t+h}}^{-1}\Big(\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T-\bm{\hat{\mu}}_{t+h}\Big)\right\}}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|},\\
\end{flalign*}

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|}
\exp \Big\{&-\frac{1}{2}\Big(\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T-\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1}\bm{\hat{\mu}}_{t+h}\Big)^T\\
&\Big[\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^T\Big]^{-1}\\ 
&\Big(\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T-\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1}\bm{\hat{\mu}}_{t+h}\Big) \Big\}.
\end{flalign*}
\noindent
Recall that, $$\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1} = \begin{pmatrix}
(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot\\ \cdots \\ (\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot
\end{pmatrix} = \begin{pmatrix}
\bm{P}\\\bm{Q}
\end{pmatrix},$$ where, $\bm{P}=(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot$ and $\bm{Q}=(\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot$. Then, 

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix} \Big|}
\exp \Big\{&-\frac{1}{2}\Big[\begin{pmatrix}\tilde{\bm{b}}_{t+h}\\ \tilde{\bm{t}}_{t+h}\end{pmatrix}-\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\mu}}_{t+h}\Big]^T\Big[\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big]^{-1}\\ 
&\Big[\begin{pmatrix}\tilde{\bm{b}}_{t+h}\\ \tilde{\bm{t}}_{t+h}\end{pmatrix}-\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\mu}}_{t+h}\Big] \Big\},
\end{flalign*}

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big|^{\frac{1}{2}}}
\exp \Big\{&-\frac{1}{2} \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix}^T \Big[\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big]^{-1}\\
& \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix} \Big\}.
\end{flalign*}
\noindent
Since, $\Big[\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big] = \begin{pmatrix}
\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T\\
\bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T
\end{pmatrix}$ we have, 

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\begin{pmatrix}
	\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T\\
	\bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T
	\end{pmatrix}\Big|^{\frac{1}{2}}}
\exp \Big\{&-\frac{1}{2} \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix}^T\\ 
&\Big[\begin{pmatrix}
\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T\\
\bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T
\end{pmatrix}\Big]^{-1} \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix} \Big\}.
\end{flalign*}
\noindent
$\bm{f_B}(.)$ gives the joint distribution of $\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T $, which is a multivariate Gaussian distribution. Then from (\ref{4.6}) and the properties of marginalization of multivariate Gaussian distribution it follows,
\begin{flalign}\label{ex:2.1}
\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T\Big|^{\frac{1}{2}}}
\exp \Big\{-\frac{1}{2} (\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h})^T (\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T)^{-1}(\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}) \Big\}.
\end{flalign}

\noindent
(\ref{ex:2.1}) implies $\tilde{\bm{b}}_{t+h} \sim \mathscr{N}(\bm{P}\hat{\bm{\mu}}_{t+h}, \bm{P}\hat{\bm{\Sigma}}_{t+h}\bm{P}^T)$ where $\bm{P} = (\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot$. Then from (\ref{4.7}) it follows that,
\begin{flalign}
\tilde{\bm{f}}(\tilde{\bm{y}}_{t+h})=\tilde{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}).
\end{flalign}
\noindent
Therefore, the reconciled Gaussian forecast distribution of the whole hierarchy is $\mathscr{N}(\bm{SP}\hat{\bm{\mu}}_{t+h}, \bm{SP}\hat{\bm{\Sigma}}_{t+h}\bm{P}^T\bm{S}^T)$.



\section{Evaluation of hierarchical probabilistic forecasts}

The necessary final step in hierarchical forecasting is to make sure that our forecast distributions are accurate enough to predict the uncertain future. In general, forecasters prefer to maximize the sharpness of the predictive distribution subject to the calibration \citep{Gneiting2014}. Therefore the probabilistic forecasts should be evaluated with respect to these two properties.\\

\noindent
Calibration refers to the statistical compatibility between probabilistic forecasts and realizations. In other words, random draws from a perfectly calibrated predictive distribution should be equivalent to the realizations. On the other hand, sharpness refers to the spread or the concentration of prediction distributions and it is a property of forecasts only. The more concentrated the predictive distributions, the sharper the forecasts are \citep{Gneiting2008}. However, independently assessing the calibration and sharpness will not help to properly evaluate the probabilistic forecasts. Therefore to assess these properties simultaneously, we use scoring rules. \\

\noindent
Scoring rules are summary measures obtained based on the relationship between predictive distribution and the realizations. In some studies, researchers take the scoring rules to be positively oriented which they would wish to maximize \citep{Gneiting2007}. However, scoring rules were also defined to be negatively oriented which forecasters wish to minimize \citep{Gneiting2014}. We consider these negatively oriented scoring rules to evaluate probabilistic forecasts in hierarchical time series. \\

\noindent
Let $\bm{{\breve{Y}}}$ and $\bm{Y}$ be a $n$-dimensional random vectors from the predictive distribution $\mathbold{F}$ and the true distribution $\mathbold{G}$. Further let $\mathbold{y}$ be a $n$-dimensional realization. Then the scoring rule is a numerical value $S(\mathbold{{\breve{Y}},y})$ assign to each pair $(\mathbold{{\breve{Y}},y})$ and the proper scoring rule is defined as,
\begin{equation}\label{eq:(3.1.)}
E_{\mathbold{G}}[S(\mathbold{Y,y})] \le E_{\mathbold{G}}[S(\mathbold{{\breve{Y}},y})],
\end{equation}
\noindent
where $E_{\mathbold{G}}[S(\mathbold{Y,y})]$ is the expected score under the true distribution $\mathbold{G}$ \citep{Gneiting2008, Gneiting2014}. \\

\noindent
Following table summarizes few existing proper scoring rules. 


\begin{center}
	\captionof{table}{\textit{Scoring rules to evaluate multivariate forecast densities }} 
	
	\small
	\resizebox{\linewidth}{!}{
		
		\begin{tabular}{ L | L | L}
			\hline
			\hline
			\textbf{Scoring rule} & \textbf{Expression} & \textbf{Reference}\\
			\\
			\hline
			\hline \\
			\text{Log score} & LS(\breve{\bm{F}},\bm{y}_{T+h}) = -log {\breve{\bm{f}}(\bm{y}_{T+h})} & \text{\citet{Gneiting2007}} \\
			\\ 
			\hline \\
			\text{Energy score} & eS(\bm{\breve{Y}_{T+h},y_{T+h}}) = E_{\breve{\bm{F}}}||\breve{\bm{Y}}_{T+h}-\bm{y}_{T+h}||^\alpha - \frac{1}{2}E_{\breve{\bm{F}}}||\breve{\bm{Y}}_{T+h}-\breve{\bm{Y}}'_{T+h}||^\alpha, \alpha \in (0,2] & \text{\citet{Gneiting2008}} \\
			\\
			\hline
			\text{Variogram score} & VS(\breve{\bm{F}}, \bm{y}_{T+h}) = \displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n}w_{ij}\left(|y_{T+h,i} - y_{T+h,j}|^p - E_{\breve{\bm{F}}}|\breve{Y}_{T+h,i}-\breve{Y}_{T+h,j}|^p\right)^2 & \text{\citet{SCHEUERER2015}} \\
			\hline
		\end{tabular}
	
		}
\end{center}
\textit{NOTE: $\bm{\breve{Y}}_{T+h}$ and $\bm{\breve{Y}}'_{T+h}$ be two independent random vectors from the coherent forecast distribution $\breve{\bm{F}}$ with the density function $\breve{\bm{f}}(.)$ at time $t+h$ and $\bm{y}_{T+h}$ is the vector of realizations. Further $\breve{Y}_{T+h,i}$ and $\breve{Y}_{T+h,j}$ are $i^{\text{th}}$ and $j^{\text{th}}$ components of the vector $\breve{\bm{Y}}_{T+h}$. Further the variogram score given is for order $p$ where, $w_{ij}$ are non-negative weights.}\\

\noindent
Even though the log score can be used evaluate simulated forecast densities with large samples \citep{Jordan2017}, it is more convenient to use if it is reasonable to assume a parametric forecast density for the hierarchy. However, the \qq{degenerecy} of coherent forecast densities would be problematic when using log scores. We will discuss more about this in the next sub section.\\

\noindent
In the energy score, for $\alpha=2$, it can be easily shown that
\begin{equation} \label{eq:(5.1)}
eS(\bm{\breve{Y}_{T+h},y_{T+h}}) = ||\bm{y}_{T+h}-\breve{\bm{\mu}}_{T+h}||^2,
\end{equation}

\noindent
where $\breve{\bm{\mu}}_{T+h} =E_{\bm{F}}(\breve{\bm{Y}}_{T+h}) $. Therefore in the limiting case, the energy score only measures the accuracy of the forecast mean, but not the entire distribution. Further \citet{Pinson2013a} argued that the Energy score given in table 1 has a very low discrimination ability for incorrectly specified covariances, even though it discriminates the misspecified means well. \\

\noindent
However, \citet{SCHEUERER2015} have shown that the variogram score has a high discrimination ability of misspecified means, variance and correlation structure than the Energy score. Further they suggested the variogram score with $p=0.5$ is more powerful.\\
\noindent
For a possible finite sample of size $B$ from the multivariate forecast density $\breve{\bm{F}}$, the variogram score is defined as, 

\begin{flalign}
VS(\breve{\bm{F}}, \bm{y}_{T+h}) = \displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n}w_{ij}\left(|y_{T+h,i} - y_{T+h,j}|^p - \frac{1}{m} \displaystyle\sum_{k=1}^{B} |\breve{Y}^k_{T+h,i}-\breve{Y}^k_{T+h,j}|^p\right)^2.
\end{flalign}
  



%For example, assume you have a Gaussian coherent predictive density $\bm{\tilde{f}}_{T+h}$. Then the log score is given by, 

%\begin{equation} 
%LS(\bm{\tilde{f}_{T+h},y_{T+h}}) = -\log \left(\frac{1}{(2\pi)^{\frac{m}{2}}|\bm{\tilde{\Sigma}}_{T+h}|^{\frac{1}{2}}}\exp\left[-\frac{1}{2}(\bm{y}_{T+h}-\bm{\tilde{\mu}}_{T+h})^T %\bm{\tilde{\Sigma}_{T+h}}^{-1}(\bm{y}_{T+h}-\bm{\tilde{\mu}}_{T+h})\right]\right),
%\end{equation}

%\noindent
%where $\bm{\tilde{\mu}}_{T+h}$ and $\bm{\tilde{\Sigma}}_{T+h}$ are the coherent mean and variance-covariance matrix of the predictive Gaussian density and both lies in the linear %subspace spanned by $\mathscr{C}(\bm{S})$. Then $\bm{\tilde{\Sigma}}_{T+h}$ is always a singular matrix and hence the determinant will be zero. Therefore the density function of %degenerate Gaussian distribution is undefined. However, we could use the pseudo determinant, i.e. product of positive eigenvalues and the pseudo inverse to calculate the log score %to compare the predictive ability of coherent forecast densities.  



\subsection{Evaluating coherent forecast densities}

As it was mentioned in the previous section, any coherent hierarchical forecast density is a degenerate density. To the best of our knowledge, there is no proper multivariate scoring rule in literature to evaluate degenerate densities. Further it can be easily seen that some of the existing scoring rules breakdown under the degeneracy. For example take the log score in the univariate case. Suppose the true density is degenerate at $x=0$, i.e. $f(x)=\mathbb{1}\{x=0\}$.  Now consider two predictive densities $p_1(x)$ and $p_2(x)$. Let $p_1(x)$ is equivalent to the true density, i.e. $p_1(x)=\mathbb{1}\{x=0\}$ and $p_2(x) =^d N(0,\sigma^2)$ with $\sigma^2 < (2\pi)^{-1}$. The expected log score of $p_1$ is:
$$E_f[S(f,f)] = E_f[S(p_1,f)] = -ln[p_1(x=0)]=0,$$
\noindent
and that of $p_2$ is:
$$E_f[S(p_2,f)] = -ln[p_2(x=0)]<0.$$ 
\noindent
Therefore $S(f,f) > S(p_2,f)$ and hence there exist at least one forecast density which breaks the condition (\ref{eq:(3.1.)}) for proper scoring rule. This implies log score cannot be used to evaluate the degenerate densities.  \\

\noindent
This suggest that it is necessary to have a rule of thumb to use these scoring rules in order to evaluate coherent forecast densities. First we should notice that, even though the coherent distribution of the entire hierarchy is degenerate, the density of the basis set of series is non-degenerate since these series are linearly independent. Further, if we can correctly specify the forecast distribution of these basis set of series, then we have almost obtained the correct forecast distribution of the whole hierarchy. Therefore, we propose to evaluate the predictive ability of only the basis set of series of the coherent forecast density by using any of the above discussed multivariate scoring rules. This will also avoid the impact of degeneracy for the scoring rules. \\

\noindent
For example, since the bottom level series is a set of basis series for a given hierarchy, we can evaluate the predictive ability of the bottom level series of the coherent forecast distribution instead of evaluating the whole distribution. Further if our purpose is to compare two coherent forecast densities, we can compare the forecast ability of only the bottom level forecast densities. 


\subsection{Comparison of coherent and incoherent forecast densities}

It is also important to assess how the coherent or reconciled forecast densities improve the predictive ability compared to the incoherent forecasts. Clearly we cannot use multivariate scoring rules, even for the basis set of series, since the coherent and incoherent forecast densities lie in two different matrix spaces.\\

\noindent
However we could compare individual margins of the forecast density of the hierarchy using univariate proper scoring rules. Most widely used Continuous Ranked Probability Score (CRPS) for evaluating univariate forecast densities would be helpful for this. \\

\begin{equation} \label{eq:(3.6)}
CRPS(\breve{F}_i,y_{T+h,i}) = E_{\breve{F}_i}|\breve{Y}_{T+h,i}-y_{T+h,i}| - \frac{1}{2}E_{\breve{F}_i}|\breve{Y}_{T+h,i}-\breve{Y}'_{T+h,i}|,
\end{equation}
       
\noindent
where $\breve{Y}_{T+h,i}$ and $\breve{Y}'_{T+h,i}$ are two independent copies from the $i^{\text{th}}$ reconciled marginal forecast distribution $\tilde{F}_i$ of the hierarchy and $y_{T+h,i}$ is the $i^{\text{th}}$ realization from the true marginal distribution $G_i$. 
 



\section{Simulation study}
Main purpose of this simulation study to establish the importance of reconciliation in probabilistic hierarchical forecasting. We narrow down the simulation setting for the Gaussian framework in this context. Suppose all the historical data in the hierarchy follows a multivariate Gaussian distribution, i.e. $\bm{y}_T \sim \mathscr{N}(\bm{\mu}_T, \bm{\Sigma}_T)$ where both $\bm{\mu}_T$ and $\bm{\Sigma}_T$ lives in $\mathscr{C}(\bm{S})$ by the nature of the hierarchical time series. We are interested in estimating the predictive Gaussian distribution of $\bm{Y}_{T+h}| \bm{\mathcal{I}}_T$ where $\bm{\mathcal{I}}_T= \{\bm{y}_1,\bm{y}_2,....,\bm{y}_T\}$, which should also lives in $\mathscr{C}(\bm{S})$. \\

\noindent
Suppose we independently fit the models for each series in the hierarchy and obtained the point and variance forecasts denoted by $\hat{\bm{\mu}}_{T+h}$ and $\hat{\bm{\Sigma}}_{T+h}$ respectively. Note that the variance forecast can be obtained through the in-sample errors. General convention on Gaussian density estimation is to take the point and variance forecasts as the estimates of mean and variance of the respective predictive distribution \citep{Hall2007}. Thus the incoherent predictive distribution is given by  $\hat{\bm{Y}}_{T+h}| \bm{\mathcal{I}}_T \sim \mathscr{N}(\hat{\bm{y}}_{T+h}, \hat{\bm{\Sigma}}_{T+h})$. We call this distribution is incoherent since it is very unlikely the point and variance forecasts from the independently fitted models to be coherent. Then it directly follows from example 2 that the reconciled Gaussian predictive distribution is given by, $\tilde{\bm{Y}}_{T+h}| \bm{\mathcal{I}}_T \sim \mathscr{N}(\bm{SP}\hat{\bm{y}}_{T+h}, \bm{SP}\hat{\bm{\Sigma}}_{T+h}\bm{P}^T\bm{S}^T)$ where $\bm{P} = (\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot$.\\

\noindent
\textbf{Result 1}: Choosing $\bm{R}^T_\bot = \bm{S}^T\hat{\bm{\Sigma}}_{T+h}^{-1}$ will ensure at least the mean of the predictive Gaussian distribution is optimally reconciled with respect to the energy score.\\

\noindent
Result 1 can be easily shown as follows. From (\ref{eq:(5.1)}) the energy score at the upper limit of $\alpha$ is given by, $||\bm{y}_{T+h}-\bm{SP}\hat{\bm{y}}_{T+h}||^2$. Then the expectation of energy score with respect to the true distribution is equivalent to the trace of mean squared forecast error, i.e. $$E_{\bm{G}}[eS(\bm{\tilde{Y}_{T+h},y_{T+h}})]= Tr\{E_{\bm{y}_{T+h}}[(\bm{y}_{T+h}-\bm{SP}\hat{\bm{y}}_{T+h})(\bm{y}_{T+h}-\bm{SP}\hat{\bm{y}}_{T+h})^T|\mathcal{I}_{T}]\}.$$ 
\noindent
From Theorem 1 of \citet{Wickramasuriya2017} it immediately follows that $\bm{P} = (\bm{S}^T\bm\hat{\bm{\Sigma}}_{T+h}^{-1}\bm{S})^{-1}\bm{S}^T\bm\hat{\bm{\Sigma}}_{T+h}^{-1}$ minimizes the expected energy score constrained on the unbiasedness of reconciled forecasts. Thus we have $\bm{R}^T_\bot = \bm{S}^T\hat{\bm{\Sigma}}_{T+h}^{-1}$. \\

\noindent
It should be noted that $\hat{\bm{\Sigma}}_{T+h}$ can be estimated in different methods which yields different estimation of $\bm{R}^T_\bot$. Following table summarizes these methods. 

\begin{center}
	\captionof{table}{\textit{Summarizing different estimates of $\bm{R}^T_\bot$} }
	
	
	\resizebox{\linewidth}{!}{
		\small
		\small
		\begin{tabular}{c c c }
			\hline\hline\\
			\textbf{Method}& \textbf{Estimate of $\hat{\bm{\Sigma}}_{T+h}$} & \textbf{Estimate of $\bm{R}^T_\bot$} \\  
			\hline \hline\\
			OLS & $\bm{I}$ & $\bm{S}^T$\\
			\hline\\
			
			MinT(Sample) & 	$\hat{\bm{\Sigma}}_{T+1}$ & $\bm{S}^T\hat{\bm{\Sigma}}_{T+1}^{-1}$\\
			& (1-step ahead insample variance covariance matrix) &\\
			\hline\\
			
			MinT(Shrink) & $\hat{\mathbold{\Sigma}}_{T+1}^{shr} = \tau\text{Diag}(\hat{\mathbold{\Sigma}}_{T+1}) + (1-\tau)\hat{\mathbold{\Sigma}}_{T+1}$ & $\bm{S}^T(\hat{\bm{\Sigma}}_{T+1}^{shr})^{-1}$\\
			\hline\\
			
			MinT(WLS) & $\hat{\mathbold{\Sigma}}_{T+1}^{wls} = \text{Diag}(\hat{\mathbold{\Sigma}}_{T+1}^{shr})$ & 	$\bm{S}^T(\hat{\bm{\Sigma}}_{T+1}^{wls})^{-1}$\\
			\hline
			\hline
			
		\end{tabular}
	}
\end{center}
\textit{Note: $\hat{\mathbold{\Sigma}}_{T+1}^{shr}$ is a shrinkage estimator proposed by \citet{Schafer2005} and also used by \citet{Wickramasuriya2017}, where $\tau = \frac{\sum_{i \ne j}\hat{Var}(\hat{r}_{ij})}{\sum_{i \ne j}\hat{r}_{ij}^2}$, $\hat{r}_{ij}$ is the $ij$-th element of sample correlation matrix. Further Diag($\bm{A}$) denote the diagonal matrix of $\bm{A}$}.\\

\noindent
It is worth mentioning that all these forecasting methods were well established in the context of point forecast reconciliation \citep{Hyndman2011, Wickramasuriya2017, Hyndman2016}. However our attempt is to emphasis the use of these reconciliation methods in the context of probabilistic forecasts. \\

\noindent
\textbf{Simulation setup}\\

\noindent
We consider the hierarchy given in figure (1) for this simulation study. This hierarchy consists two aggregation levels with four bottom level series. Each bottom level series will be generated first and add them up to obtain the data for respective upper level series. Hierarchical time series in practice contain much noisier series in the bottom level than in aggregate series. In order to simulate this feature in the hierarchy, we refer to \citet{Wickramasuriya2017} and the data generating process will be given as follows. \\

\noindent
Suppose $\{w_{AA,t},w_{AB,t},w_{BA,t},w_{BB,t}\}$ are generated from $ARIMA(p,d,q)$ processes where, $(p,q)$ and $d$ take integers from $\{1,2\}$ and $\{0,1\}$ respectively with equal probability and the contemporaneous errors $\{\epsilon_{AA,t},\epsilon_{AB,t},\epsilon_{BA,t},\epsilon_{BB,t}\} \sim \mathcal{N}(\bm{0}, \bm{\Sigma})$. Further, the parameters for $AR$ and $MA$ components will be randomly and uniformly generated from $[0.3,0.5]$ and $[0.3,0.7]$ respectively. Then the bottom level series $\{y_{AA,t},y_{AB,t},y_{BA,t},y_{BB,t}\}$ will be obtained as: 
$$y_{AA,t} = w_{AA,t} + u_t - 0.5v_t,$$
$$y_{AB,t} = w_{AB,t} - u_t - 0.5v_t,$$
$$y_{BA,t} = w_{BA,t} + u_t + 0.5v_t,$$
$$y_{BB,t} = w_{BB,t} - u_t + 0.5v_t,$$ 
\noindent
where $u_t \sim N(0,\sigma^2_u)$ and $v_t \sim N(0,\sigma^2_v)$. 

\noindent
To obtain the aggregate series at level 1, we add their respective bottom level series such as: 
$$y_{A,t} = w_{AA,t} + w_{AB,t} - v_t,$$
$$y_{B,t} = w_{BA,t} + w_{BB,t} + v_t,$$
\noindent
and the total series will be obtained as: 
$$y_{Tot,t} = w_{AA,t} + w_{AB,t} + w_{BA,t} + w_{BB,t}.$$
\noindent
To get less noisier aggregate series than disaggregate series, we choose $\bm{\Sigma}, \sigma^2_u$ and $\sigma^2_v$ such that, 

$$Var(\epsilon_{AA,t}+\epsilon_{AB,t}+\epsilon_{BA,t}+\epsilon_{BB,t}) \le Var(\epsilon_{AA,t}+\epsilon_{AB,t}-v_t) \le Var(\epsilon_{AA,t}+u_t-0.5v_t),$$
$$\bm{l}_1\bm{\Sigma} \bm{l}_1^T \le \bm{l}_2\bm{\Sigma} \bm{l}_2^T + \sigma^2_v \le  \bm{l}_3\bm{\Sigma} \bm{l}_3^T + \sigma^2_u + \frac{1}{4}\sigma^2_v,$$
\noindent
where $\bm{l}_1 = \begin{pmatrix} 1&1&1&1 \end{pmatrix}, \bm{l}_2 = \begin{pmatrix} 1&1&0&0 \end{pmatrix}$ and $\bm{l}_3 = \begin{pmatrix} 1&0&0&0 \end{pmatrix}$.
\noindent 
This follows, 
$$\bm{l}_1\bm{\Sigma} \bm{l}_1^T - \bm{l}_2\bm{\Sigma} \bm{l}_2^T \le \sigma^2_v \le \frac{4}{3}(\sigma^2_u + \bm{l}_3\bm{\Sigma} \bm{l}_3^T - \bm{l}_2\bm{\Sigma} \bm{l}_2^T).$$
\noindent
Thus we choose, 
$\mathbold{\Sigma} =  \begin{pmatrix} 5.0 & 3.1 & 0.6 & 0.4\\
3.1 & 4.0 & 0.9 & 1.4\\
0.6 & 0.9 & 2.0 & 1.8\\
0.4 & 1.4 & 1.8 & 3.0\\
\end{pmatrix}, \quad \sigma^2_u = 19$ and $\sigma^2_u = 18$ in our simulation setting. \\

\noindent
As such we generate data for the hierarchy with sample size $T=501$. Then univariate $ARIMA$ models were fitted for each series independently using the first 500 observations and obtain 1-step ahead base (incoherent) forecasts. We use \textit{forecast} package in \textbf{R}-software \citet{hyndman2017forecasting} for model fitting and forecasting. Further, different estimates of $\hat{\Sigma}_{T+1}$ and the corresponding $\bm{R}^T_\bot$ were obtained as summarized in Table 2. This process was then replicated using $1000$ different data sets from the same data generating process. \\

\noindent
To assess the predictive performance of different forecasting methods, we use scoring rules as discussed in section 5. In addition to that we use Skill score \citep{Gneiting2007} for any comparison. For a given forecasting method, evaluated by a particular scoring rule $S(.)$ , the skill score will be calculated as follows, 
\begin{flalign}
Ss[S_B(.)] = \frac{S_B(\mathbold{{\breve{Y}},y}) - S_B(\mathbold{{{Y}},y})^{ref}}{S_B(\mathbold{{{Y}},y})^{ref}}\times 100\%,
\end{flalign} 
\noindent
where $S_B(.)$ is average score over $B$ samples and $S_B(\mathbold{{{Y}},y})^{ref}$ is the average score of the reference forecasting methods. Thus $Ss[S_B(.)]$ gives the percentage improvement of the preferred forecasting method relative to the reference method. \\

\noindent
As it was mentioned before we wish to establish the importance of reconciliation methods from this simulation study. In particular we compare different reconciliation methods over the conventional bottom-up method and the importance of coherent forecasts over incoherent forecasts. For the former comparison we use bottom level probabilistic forecasts and calculated the energy score, log score, variogram score (presented in Table 3) and the skill score is calculated with reference to bottom up method (presented in Table 4). For the latter comparison we calculate the CRPS for each aggregate series (presented in Table 5) and the skill score is calculated with reference to the incoherent forecasts.\\  

\pagebreak
\noindent
\textbf{Comparison of reconciled forecasts using bottom level series}

\begin{center}
	\captionof{table}{\textit{Comparison of reconciled forecasts using bottom level series} }
	
	
	\resizebox{\linewidth}{!}{
		\small
		\scriptsize
		\begin{tabular}{@{}l S[table-format=2.2] S[table-format=2.2]  S[table-format=2.2] @{}}
			
			\hline
			\hline 
			
			
			\text{Forecasting method} &  \text{Energy score} & \text{Log score} & \text{Variogram score} \\ 
			\hline
			\hline
			
			MinT(Shrink) & 7.58  & 11.34  &  2.96  \\ 
			
			MinT(Sample) & 7.60  & 11.34  & 2.97   \\
			
			MinT(WLS) & 7.93  & 12.62  & 3.16   \\ 
			
			OLS & 7.81  & 11.53  & 3.06   \\ 
			
			Bottom up &  8.29 & 12.10  & 3.05   \\
			
			\hline
			\hline 
			
			
		\end{tabular}
	}
\end{center}


\small
\begin{center}
	\captionof{table}{\textit{Comparison of reconciliation methods against Bottom up method} }
	
	
	\resizebox{\linewidth}{!}{
		\scriptsize
		\begin{tabular}{@{}l S[table-format=2.2] S[table-format=2.2]  S[table-format=2.2] @{}}
			
			\hline
			\hline 
			
			
			\text{Forecasting method} &  \text{Energy score} & \text{Log score} & \text{Variogram score} \\ 
			\hline
			\hline
			
			MinT(Shrink) & 8.56  & 6.28  & 2.95  \\ 
			
			MinT(Sample)  & 8.32  & 6.28  & 2.62  \\
			
			MinT(WLS)  & 4.34  & -4.30  & -3.61  \\
			
			OLS  & 5.79  & 4.71  & -0.33  \\
			
			
			\hline
			\hline 
			
			
		\end{tabular}
	}
\end{center}
\textit{Note: Each entry represent the percentage skill score with respect to the bottom up forecasts}\\\\



\noindent
\textbf{Comparison of unreconciled vs. reconciled forecasts for the aggregate series}

\begin{center}
	\captionof{table}{\textit{Comparison of unreconciled vs. reconciled forecasts for the aggregate series}} 
	
	\small
	\resizebox{\linewidth}{!}{
		\scriptsize
		\begin{tabular}{@{}l S[table-format=1.2] S[table-format=1.2]  S[table-format=1.2] S[table-format=1.2] S[table-format=1.2]  S[table-format=1.2] @{}}
			
			\hline
			\hline 
			
			
			\multirow{2}{*}{\text{Forecasting method}} & \multicolumn{2}{ c }{\text{Total}} & \multicolumn{2}{c}{\text{Series - A}} & \multicolumn{2}{c}{\text{Series - B}} \\ \cline{2-7} & \text{CRPS} &  \text{LogS} &   \text{CRPS} &   \text{LogS} &   \text{CRPS} &   \text{LogS} \\ 
			\hline
			\hline
			Unreconciled & 5.57 & 3.99 & 12.68 & 4.87 & 10.35 & 4.66 \\ 
			
			MinT(Shrink) & 5.44 & 3.97 & 10.04 & 4.64 & 8.92 & 4.52 \\
			
			MinT(Sample) & 5.44 & 3.97 & 10.07 & 4.64 & 8.95 & 4.52 \\
			
			MinT(WLS) & 4.26 & 3.70 & 5.45 & 3.94 & 5.06 & 3.86 \\
			
			OLS & 6.50 & 4.18 & 10.91 & 4.72 & 9.29 & 4.56 \\
			
			Bottom up  & 19.95 & 5.33 & 15.21 & 5.05 & 12.40 & 4.84 \\
			
			
			\hline
			\hline
			
			
		\end{tabular}
	}
\end{center}


\pagebreak
\begin{center}
	\captionof{table}{\textit{Comparison of unreconciled vs. reconciled forecasts for the aggregate series using Skill score}} 
	
	
	\resizebox{\linewidth}{!}{
	\scriptsize	
		\begin{tabular}{@{}l S[table-format=1.2] S[table-format=1.2]  S[table-format=1.2] S[table-format=1.2] S[table-format=1.2]  S[table-format=1.2] @{}}
			
			\hline
			\hline 
			
			
			\multirow{2}{*}{\text{Forecasting method}} & \multicolumn{2}{ c }{\text{Total}} & \multicolumn{2}{c}{\text{Series - A}} & \multicolumn{2}{c}{\text{Series - B}} \\ \cline{2-7} & \text{CRPS} &  \text{LogS} &   \text{CRPS} &   \text{LogS} &   \text{CRPS} &   \text{LogS} \\ 
			\hline
			\hline
			
			
			MinT(Shrink) & 2.33 & 0.50 & 20.82 & 4.72 & 13.82 & 3.00 \\
			
			MinT(Sample)  & 2.33 & 0.50 & 20.58 & 4.72 & 13.53 & 3.00 \\
			
			MinT(WLS)  & 23.52 & 7.27 & 57.02 & 19.10 & 51.11 & 17.17 \\ 
			
			OLS  & -16.70 & -4.76 & 13.96 & 3.08 & 10.24 & 2.15 \\
			
			Bottom up   & -258.17 & -33.58 & -19.95 & -3.70 & -19.81 & -3.86 \\
			
			\hline
			\hline
			
			
		\end{tabular}
	}
\end{center}







\section{Conclusions.}




   



\newpage
\printbibliography
\end{document}


