
\documentclass[a4paper, 11pt]{article}
\usepackage[a4paper, inner=3cm, outer=3cm, top=2cm, bottom=3cm]{geometry}
\usepackage{amsmath, amssymb, fixmath, qtree, bm, calrsfs, enumitem,multirow, caption, textcmds,siunitx,adjustbox, mathrsfs, float, amstext}
\usepackage[bb=boondox]{mathalfa}
\usepackage{tikz}
\usepackage[natbib=true,backend=bibtex,style=authoryear-comp,dashed=false,maxbibnames=99,firstinits=true,maxcitenames=3,url=true,doi=false,isbn=false]{biblatex}
\usepackage[pdftex,colorlinks=true]{hyperref}
\hypersetup{citecolor=blue,linkcolor=blue,urlcolor=blue}
\DeclareNameAlias{sortname}{last-first}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\def\mathbi#1{\textit{ #1}}
\def\mathB#1{\textbf{ #1}}


\bibliography{References_paper1}
\begin{document}


\title{Probabilistic Forecasts in Hierarchical Time Series}
\maketitle


\section{Introduction.}
Many research applications often involve a large collection of multiple time series some of which are aggregates of others.
These collection of time series are called hierarchical time series. For example, electricity demand of a country can be disaggregated along a geographical hierarchy. One would be interested in forecasting electricity demand of the whole country along with the demand of states, cities and households. This is referred to as hierarchical forecasting. An important aspect of hierarchical forecasting is to have coherent forecasts across the hierarchy; that is, forecasts at lower levels should add up to their respective upper levels of aggregation.\\ 

\noindent
There is a rich literature on producing coherent point forecasts in hierarchical time series. The traditional approaches involve bottom-up, top-down and middle-out methods. In the bottom-up approach, forecasts of the lowest level are first generated and they are simply aggregated to forecast upper levels of the hierarchy \citep{Dunn1976}.  
In contrast, the top-down approach involves forecasting the most aggregated series first and then disaggregating these forecasts down the hierarchy based on the proportions of observed data \citep{Gross1990}. 
Many studies in the literature have broadly discussed the relative advantages and disadvantages of bottom-up and top-down methods and situations in which they would provide reliable forecasts \citep{Schwarzkopf1988,Kahn1998, Lapide1998,Fliedner2001}.
A compromise between these two approaches is the middle-out method which entails forecasting each series of a selected middle level in the hierarchy and then forecasting upper levels by the bottom-up method and lower levels by the top-down method. Apparently, these three approaches use only a part of the information available when producing coherent forecasts. This might result inaccurate forecasts. For example, if the bottom level series are highly volatile or too noisy and hence challenging to forecast, then the resulting forecasts from bottom-up approach would be inaccurate.\\

\noindent
As an alternative to these conventional methods, \citet{Hyndman2011} propose to use information from all the levels of the hierarchy and reconcile them to obtain coherent point forecasts. In this approach, independent forecasts of all series are initially obtained. It is very unlikely that these forecasts are coherent. Then, these forecasts are optimally combined through a regression model to obtain coherent forecasts. This study has given arise to the concept of point forecast reconciliation in hierarchical time series. In general, any forecasting method that use a set of incoherent forecasts and revise them to obtain coherent forecasts is referred to as forecast reconciliation method. Thus it is important to notice that bottom-up, top-down and middle-out methods are not reconciliation methods since they use only a part of information from the hierarchy to produce coherent forecasts. \\ 

\noindent
In a recent study on point forecast reconciliation, \citet{Wickramasuriya2017} propose to minimize the trace of mean squared coherent forecasts errors to optimally reconcile a set of unbiased, incoherent forecasts. Assuring the resulting reconciled forecasts are unbiased, they found an unique analytical solution to the proposed minimization problem. They referred this to as MinT approach. This solution was further improved to be computationally feasible for high dimensional hierarchies. \\

\noindent
In recent forecasting literature, its being the rationality to provide the associated uncertainty about the future, in addition to the point forecasts. This is referred to as probabilistic forecasts \citep{Gneiting2014}. For example, \citet{McSharry2005} produced probabilistic forecasts in application to electricity demand, \citet{BenTaieb2017} in application to smart meter data, \citet{Pinson2009} in application to wind power generation, \citet{Gel2004}, \citet{Gneiting2005a} and \citet{Gneiting2005} in application to prediction of weather quantities. \\

\noindent      
Although there is a thorough literature on producing coherent point forecasts of hierarchical time series, a very limited attention has been given in the context on probabilistic forecasts. Recently \citet{BenTaieb2017} proposed an algorithm to produce coherent probabilistic forecasts with the application to UK electricity smart meter data. This is the only study that copes with the probabilistic forecasts in hierarchical time series thus far. In their approach, a sample from the mean reconciled predictive distribution of the bottom level will be first generated. This sample will be then aggregated to obtain the coherent probabilistic forecasts of the respective upper levels of the hierarchy. Hence this method is a bottom-up based approach. They further use MinT approach for the mean reconciliation of the bottom level predictive distribution and a copula based approach to model the dependency structure of the hierarchy, when producing the sample of bottom level probabilistic forecasts.\\

\noindent
Our contribution in this paper is to provide a proper theoretical background for the probabilistic hierarchical forecasts. First we redefine the coherency of point forecasts and the reconciliation of a set of incoherent point forecasts by using the concepts on vector spaces and measure theory. This will leads us to provide a proper definition for probabilistic forecast reconciliation. Due to the aggregation structure in hierarchy, the probability distribution is degenerate and hence the forecast distribution should also be degenerate. We discuss in detail how this degeneracy will be taken care of in probabilistic forecast reconciliation and also in the evaluation of their predictive performances. Further we provide theoretical results on probabilistic forecast reconciliation in the Gaussian framework. \\

\noindent
Rest of the paper is structured as follows. We start with introducing the notations in section 2. In section 3 we provide our definitions for coherency which is followed by the definitions for forecast reconciliation in section 4. Section 5 discuss about the methods that can be used for evaluation of probabilistic hierarchical forecasts. Section 6 consists the theoretical results followed by the simulation study on probabilistic forecast reconciliation in Gaussian framework and we enclose with the remarkable conclusions in section 7.     



\section{Notations}
We first introduce notations where possible we follow \citet{Wickramasuriya2017}. Suppose $\mathbold{y}_t \in \bm{\mathbb{R}}^n$ comprising all observations of the whole hierarchy at time $t$ and $\mathbold{b}_t \in \bm{\mathbb{R}}^m$ comprising only the bottom level observations at  time $t$. Then due to the aggregation nature of the hierarchy we have, 

\begin{equation}
\mathbold{y}_t = \mathbold{Sb}_t,
\end{equation}
where $\mathbold{S}$ is a $n \times m$ constant matrix whose columns span the linear subspace for which all constraints hold. Since this linear subspace is equivalent to the column space of $\bm{S}$, we denote it as $\mathscr{C}(\bm{S})$. Further the null space of $\bm{S}$ which is orthogonal to this linear subspace is denoted by $\mathscr{N}(\bm{S})$. 
To understand the notations clearly, consider the hierarchy given in Figure 1.
\begin{figure}[H]
	\begin{center}
		\leaf{AA} \leaf{AB} 
		\branch{2}{A}
		\leaf{BA} \leaf{BB}
		\branch{2}{B}
		\branch{2}{Tot}
		\qobitree
	\end{center}
	\caption{Two level hierarchical diagram}
\end{figure}
\noindent
In any hierarchy, the most aggregated level is termed as level 0, the second most aggregated level is termed as level 1 and so on. This example consists of two levels. At a particular time $t$, let $y_{Tot,t}$ denote the observation at level 0; $y_{A,t}, y_{B,t} $ denote observations at level 1; and $y_{AA,t}, y_{AB,t}, y_{AC,t}, y_{BA,t}, y_{BB,t}$ denote observations at level 2. Then $\mathbold{y}_t = [y_{Tot,t},y_{A,t}, y_{B,t},y_{C,t},y_{AA,t}, y_{AB,t}, y_{AC,t}, y_{BA,t}, y_{BB,t}]^T$, \\$\mathbold{b}_t = [y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}]^T$, $m=4$, $n=7$, and $$ \mathbold{S} = \begin{pmatrix} 1& 1 &1 &1  \\ 1 &1 & 0 &0 \\   0&0  & 1 & 1 \\ & & I_4 &   \end{pmatrix}, $$ where $I_4$ is a $4$-dimension identity matrix. \\  


\section{Coherent forecasts}

The main purpose of this section is to provide formal definitions for coherent forecasts. We start with redefining the coherent point forecasts using the properties of vector spaces which is then followed by the definition of coherent probabilistic forecasts. 
\\

\noindent
\textbf{Definition 3.1: Coherent Point Forecasts}\\
\noindent
Suppose $\bm{\breve{y}}_{t+h} \in \mathbb{R}^n$ consists point forecasts of each series in the hierarchy at time $t+h$.  $\bm{\breve{y}}_{t+h}$ is said to be \textit{Coherent} if it lies in a $m$-dimensional subspace of $\bm{\mathbb{R}}^n$ which is spanned by the columns of $\bm{S}$. \\

\noindent
\textbf{Definition 3.2: Coherent Probabilistic Forecasts}\\
\noindent
Let $(\bm{\mathbb{R}}^m, \bm{\mathscr{F}}^m, \nu^m)$ be a probability measure space where $\mathscr{F}^m$ is a sigma algebra on $\bm{\mathbb{R}}^m$. $\breve{\nu}(.)$ on $(\mathscr{C}(\bm{S}), \mathscr{F}_{\bm{S}})$ is said to be coherent probability measure iff $$\breve{\nu}(\bm{S}(\bm{A})) = \nu^m(\bm{A}) \quad \forall \quad \bm{A} \in \mathscr{F}^m,$$ where $\bm{S}(\bm{A})$ denotes the image of subset $\bm{A}$ under $\bm{S}$. 

\usetikzlibrary{arrows,positioning,shapes,fit,calc}

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[
		>=stealth,
		bullet/.style={
			fill=black,
			circle,
			minimum width=1.5cm,
			inner sep=0pt
		},
		projection/.style={
			->,
			thick,
			label,
			shorten <=2pt,
			shorten >=2pt
		},
		every fit/.style={
			ellipse,
			draw,
			inner sep=0pt
		}
		]
		
		\node at (2,3) {$\bm{S}$};
		
		\node at (0,5) {$\bm{\mathbb{R}}^m$(domain of $\bm{S}$)};
		\node at (4,5) {$\bm{\mathbb{R}}^n$(codomain of $\bm{S}$)};
		\node at (4,1.0) {$\mathscr{C}(\bm{S})$};
		%\node[bullet,label=below:$f(x)$] at (4,2.5){};
		
		
		\draw (0,2.5) ellipse (1.02cm and 2.2cm);
		\draw (4,2.5) ellipse (1.02cm and 2.2cm);
		\draw (4,2.5) ellipse (0.51cm and 1.1cm);
		
		
		\draw[projection, label=below:$f$] (0.3,2.5) -- (3.8,2.5) ;
		
		\end{tikzpicture}
		\newline
	\end{center}
	\caption{Any set $\bm{A} \in \bm{\mathbb{R}}^m$ will be mapped to the $\mathscr{C}(\bm{S})$ through the mapping $\bm{S}$}
\end{figure}
\noindent
Definition 3.2 implies the probability measure on $\mathscr{C}(\bm{S})$ is equivalent to the probability measure on $(\bm{\mathbb{R}}^m, \bm{\mathscr{F}}^m)$. Hence, there is no density anywhere outside the linear subspace spanned by the columns of $\bm{S}$. That is, a \textit{Coherent probability density forecast} is any density $\bm{f}(\bm{\breve{y}}_{t+h})$ such that $\bm{f}(\bm{\breve{y}}_{t+h})=0$ for all $\bm{\breve{y}}_{t+h}$ in the null space of $\bm{S}$. Following example will help to understand these definitions more clearly.\\

\noindent
\textbf{Example 1}\\
\noindent
Consider a simple hierarchy with two bottom level series A and B that add up to the top level series T. Suppose the forecasts at time $t+h$ of these series are given by $\mathbold{\breve{y}}_{t+h} = [\breve{y}_{T,t+h},\breve{y}_{A,t+h}, \breve{y}_{B,t+h}]$ and $\mathbold{\breve{y}}_{t+h} \in \bm{\mathbb{R}}^3$. Due to the aggregation constraint of the hierarchy we have $\breve{y}_{T,t+h}=\breve{y}_{A,t+h}+\breve{y}_{B,t+h}$. This implies, even though  $\mathbold{\breve{y}}_{t+h}$ is in $\bm{\mathbb{R}}^3$, the points actually lie in a two dimensional subspace within that $\bm{\mathbb{R}}^3$ space. This subspace is equivalent to $\mathscr{C}(\bm{S})$ for this simple hierarchy. Therefore, for any $\mathbold{\breve{y}}_{t+h} \in \mathscr{N}(\bm{S})$ have a zero probability. I.e. $f(\mathbold{\breve{y}}_{t+h})=0$ for any $\mathbold{\breve{y}}_{t+h} \in \mathscr{N}(\bm{S})$.\\

\noindent
By the definition of coherent forecasts, it is clear that there are only $m$ number of linear independent series in the whole hierarchy. We refer to these as $m$ dimensional \textit{basis set of series} since these series generates a \textit{basis} that spans the $\mathscr{C}(\bm{S})$. Other $n-m$ series of the hierarchy can be linearly determined by these basis set of series. This implies that any coherent density is a degenerate density. In particular, the $m$ number of bottom level series of a given hierarchy can be considered as a basis set of series. Then the columns of $\bm{S}$ is a basis that generates through these bottom level series. This basis spans the linear subspace where the degenerate density lives on.\\

\noindent
However bottom level series are not the only set of basis series and we can find many other basis set of series which generates basis that spans the same $\mathscr{C}(\bm{S})$ for a given hierarchy. If we go back to the hierarchy in example 1, instead of two bottom level series $(\bm{A,B})$ we can take $(\bm{T,A})$ as the basis set of series and the basis that generates through $(\bm{T,A})$ will be $\{\begin{pmatrix}
1&0&1 \end{pmatrix}^T, \begin{pmatrix}
0&1&-1 \end{pmatrix}^T\}$. Another possible basis would be the singular value decomposition of $\bm{S}$.\\

\noindent
An important thing to notice here is all of these basis spans the same linear subspace equivalent to the $\mathscr{C}(\bm{S})$. Therefore the definition $(3.2)$ is not unique and one can redefine the coherent probabilistic forecasts with respect to any basis. However we stick to the definition $(3.2)$ and consider the basis defined by the columns of $\bm{S}$ that generates through the bottom levels of the hierarchy in what follows.\\ 

\noindent
It is also worth to mention that the definition $(3.1)$ and $(3.2)$ facilitate extension to the forecast reconciliation which we talk about in the next section. In contrast to our definition, \citet{BenTaieb2017} defines the coherent probabilistic forecasts in terms of convolution. According to their definition, if the forecasts are coherent, then the convolution of forecast distributions of disaggregate series is same as the forecast distribution of the corresponding aggregate series.  

\section{Forecast reconciliation}

Previous studies on point forecast literature has shown that the reconciliation provides better coherent forecasts than the conventional bottom-up and top-down methods (\cite{Hyndman2011}; \cite{VanErven2015a}; \cite{Wickramasuriya2017}). However this was not well established in the context of probabilistic forecasts. Nevertheless, there is no proper definition for the probabilistic forecast reconciliation in the published literature. Thus in this section we focus on defining the probabilistic forecast reconciliation and discuss how that definition can be used in practice.  


\subsection{Point forecast reconciliation}

Initially we redefine point forecast reconciliation using the concepts in linear algebra as a groundwork for the probabilistic forecast reconciliation. \\


\noindent
\textbf{Definition 4.1}\\
\noindent
Let $\bm{g}:\bm{\mathbb{R}}^n \rightarrow \bm{\mathbb{R}}^m $ and $\hat{\bm{y}}_{t+h} \in \bm{\mathbb{R}}^n$ be any set of incoherent forecasts at time $t+h$. Then $\tilde{\bm{b}}_{t+h}$ is said to be reconciled bottom level forecasts if 
\begin{equation}
\tilde{\bm{b}}_{t+h}=\bm{g}(\hat{\bm{y}}_{t+h}),
\end{equation}
\noindent
where $\bm{g}(\hat{\bm{y}}_{t+h})$ is the image of $\hat{\bm{y}}_{t+h}$ under $\bm{g}$ on $\bm{\mathbb{R}}^m$. The reconciled forecasts for the whole hierarchy is then given by $\tilde{\bm{y}}_{t+h}=\bm{S}(\tilde{\bm{b}}_{t+h})$ such that $\tilde{\bm{y}}_{t+h} \in \mathscr{C}(\bm{S})$, where $\bm{S}(\tilde{\bm{b}}_{t+h})$ is the image of $\tilde{\bm{b}}_{t+h}$ under $\bm{S}$ on the $\mathscr{C}(\bm{S}) < \bm{\mathbb{R}}^n$.\\

\noindent
In the following content we explain more on how this definition can be used in practice to reconcile point forecasts in hierarchical time series. Let $\bm{R} \in \bm{\mathbb{R}}^{n \times n-m}$ consists the columns that spans $\mathscr{N}(\bm{S})$ which is orthogonal to $\mathscr{C}(\bm{S})$. $\mathscr{N}(\bm{S})$ is also equivalent to the $\mathscr{C}(\bm{R})$. Note that $\bm{R}$ is also not unique and one example is a matrix whose columns represent the aggregation constraints for the hierarchy. Then for the hierarchy in example 1, $$ \mathbold{S} = \begin{pmatrix} 1& 1 \\ 1 & 0 \\ 0&1 \end{pmatrix} \quad \text{and} \quad \mathbold{R} = \begin{pmatrix}  1 \\ -1 \\ -1 \end{pmatrix}.$$ 
\noindent
Further let $\{\bm{s}_1,...,\bm{s}_m\}$ and $\{\bm{r}_1,...,\bm{r}_{n-m}\}$ denote the columns of $\bm{S}$ and $\bm{R}$ respectively. Then $\bm{B}=\{\bm{s}_1,...,\bm{s}_m, \bm{r}_1,...,\bm{r}_{n-m}\}$ is a basis for $\bm{\mathbb{R}}^n$. Now, using the insights of definition 4.1, we can follow the below steps to reconcile the point forecasts.\\

\noindent
\textbf{Step 1: Obtaining reconciled bottom level point forecasts}\\

\noindent
For a given incoherent set of point forecasts $\hat{\bm{y}}_{t+h} \in \bm{\mathbb{R}}^n$, first we find the coordinates of $\hat{\bm{y}}_{t+h}$ with respect to the basis $\bm{B}$. Let $\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T$ denote these coordinates. Note that $\tilde{\bm{b}}_{t+h}$ is a basis set of series which is really the reconciled bottom level series that generates $\{\bm{s}_1,...,\bm{s}_m\}$ and $\tilde{\bm{t}}_{t+h}$ is another basis set of series that generates $\{\bm{r}_1,...,\bm{r}_{n-m}\}$. Then from basic properties of linear algebra it follows that, 

\begin{flalign}\label{4.1}
\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T = \hat{\bm{y}}_{t+h},
\end{flalign}
\begin{flalign}\label{4.2}
\hat{\bm{y}}_{t+h} = \bm{S}\tilde{\bm{b}}_{t+h} +  \bm{R}\tilde{\bm{t}}_{t+h},
\end{flalign}
\noindent
and
\begin{flalign}\label{4.3}
\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T  = \begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1}\hat{\bm{y}}_{t+h}.
\end{flalign}

\noindent
In order to find $(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1}$, let $\bm{S}_{\bot}$ and $\bm{R}_{\bot}$ be the orthogonal complements of $\bm{S}$ and $\bm{R}$ 
respectively. Then $(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1}$ is given by, 

\begin{flalign}
\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1} = \begin{pmatrix}
(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot\\ \cdots \\ (\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot
\end{pmatrix}.
\end{flalign}

\noindent
Thus we have, 
\begin{flalign} \label{4.4}
\begin{pmatrix}
\tilde{\bm{b}}_{t+h}\\ \cdots \\ \tilde{\bm{t}}_{t+h}
\end{pmatrix} = \begin{pmatrix}
(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot\\ \cdots \\ (\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot
\end{pmatrix}\hat{\bm{y}}_{t+h}.
\end{flalign}

\noindent
From (\ref{4.4}) it follows that, 
\begin{flalign}
\tilde{\bm{b}}_{t+h}=(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot \hat{\bm{y}}_{t+h} 
\end{flalign}
\\

\noindent
\textbf{Step 2: Obtaining reconciled point forecasts for the whole hierarchy}\\

\noindent
This step directly follows by the definition for coherent forecasts. That is, to obtain reconciled point forecasts for the entire hierarchy, we map $\tilde{\bm{b}}_{t+h} \in \bm{\mathbb{R}}^n$ to the $\mathscr{C}(\bm{S})$ through $\bm{S}$. Thus we have, 
\begin{flalign}
\tilde{\bm{y}}_{t+h}=\bm{S}(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot \hat{\bm{y}}_{t+h}, \quad \tilde{\bm{y}}_{t+h} \in \mathscr{C}(\bm{S})<\bm{\mathbb{R}}^n.
\end{flalign}

\noindent
Finding a suitable $\bm{R}_\bot$ with respect to a certain loss function will result optimally reconciled point forecasts of the hierarchy. Notice that, if the mapping $\bm{g}$ in definition 4.1 is considered to be linear we have,
\begin{flalign}
\bm{g}=(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot.
\end{flalign}
\noindent
In previous studies on hierarchical point forecasting, $\bm{g}$ is considered as a $m \times n$ matrix $\bm{P}$ and thus $\tilde{\bm{y}}_{t+h}=\bm{S}\bm{P} \hat{\bm{y}}_{t+h}$. 
In other words, $\bm{g}$ linearly projects incoherent point forecasts onto the $\mathscr{C}(\bm{S})$. Further in our context, we need to find $\bm{R}_\bot$ such that $\bm{R}^T_\bot \bm{S}$ is invertible. i.e, $(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot \bm{S}=\bm{I}$. This condition coincides with the unbiased condition $\bm{SPS}=\bm{S}$ proposed by \citet{Hyndman2011}. \\

\noindent
In their study, \citet{Hyndman2011} proposed to choose,
\begin{flalign*}
\tilde{\bm{b}}^{OLS}_{t+h}=(\bm{S}^T \bm{S})^{-1}\bm{S}^T \hat{\bm{y}}_{t+h},
\end{flalign*}
\noindent
where in this context, $\bm{R}^T_\bot = \bm{S}^T$. Thus the reconciled point forecasts for the entire hierarchy is given by, 
\begin{flalign}
\tilde{\bm{y}}^{OLS}_{t+h}=\bm{S}(\bm{S}^T \bm{S})^{-1}\bm{S}^T \hat{\bm{y}}_{t+h}.
\end{flalign}
\noindent
They referred this to as OLS solution and the loss function they considered is equivalent to the euclidean norm between $\hat{\bm{y}}_{t+h}$ and $\tilde{\bm{y}}_{t+h}$, i.e. $<\hat{\bm{y}}_{T+h}, \tilde{\bm{y}}_{T+h}>$.\\

\noindent
According to a recent study by \citet{Wickramasuriya2017}, choosing $\bm{R}^T_\bot = \bm{S}^T\bm{W}^{-1}_{h}$ will minimize the trace of mean squared reconciled forecasts errors under the property of unbiasedness where, $\bm{W}^{-1}_{h}$ is the variance of the incoherent forecast errors. This will result, 
\begin{flalign*}
\tilde{\bm{b}}^{MinT}_{t+h}=(\bm{S}^T\bm{W}^{-1}_{h} \bm{S})^{-1}\bm{S}^T\bm{W}^{-1}_{h} \hat{\bm{y}}_{t+h},
\end{flalign*}
and thus,
\begin{flalign}
\tilde{\bm{y}}^{MinT}_{t+h}=\bm{S}(\bm{S}^T \bm{W}^{-1}_{h}\bm{S})^{-1}\bm{S}^T\bm{W}^{-1}_{h} \hat{\bm{y}}_{t+h}.
\end{flalign}

\noindent
They referred this to as MinT solution. It is also worth to notice that the loss function they considered is equivalent to the Mahalanobis distance between $\hat{\bm{y}}_{T+h}$ and $\tilde{\bm{y}}_{T+h}$. i.e. $<\hat{\bm{y}}_{T+h}, \tilde{\bm{y}}_{T+h}>_{\bm{W}_h}$.  
 

\subsection{Probabilistic forecast reconciliation}

In terms of probabilistic forecasts, the reconciliation implies finding the probability measure of the coherent forecasts using the information of incoherent probabilistic forecast measure. A more formal definition is given below. \\

\noindent
\textbf{Definition 4.2}\\
\noindent
Suppose $(\bm{\mathbb{R}}^n, \mathscr{F}^n, \hat{\nu})$ be an incoherent probability measure space and $(\bm{\mathbb{R}}^m, \mathscr{F}^m, \nu^m)$ be a probability measure space defined on $\bm{\mathbb{R}}^m$. Let $\bm{g}:\mathbb{R}^n \rightarrow \mathbb{R}^m $. Then the probability measure on reconciled bottom levels is such that, 
\begin{flalign}
\nu^m(\bm{A}) = \hat{\nu}(\bm{g}^{-1}(\bm{A})), \quad \forall \quad \bm{A} \in \mathscr{F}^m.
\end{flalign}
\noindent
Further the reconciled probability measure of the whole hierarchy is given by, 
\begin{flalign}
\tilde{\nu}(\bm{S}(\bm{A})) = \hat{\nu}(\bm{g}^{-1}(\bm{A})), \quad \forall \quad \bm{A} \in \mathscr{F}^m,
\end{flalign}
\noindent
where, $\bm{S}:\mathbb{R}^m \rightarrow \mathbb{R}^n$ and $\tilde{\nu}(.)$ is the probability measure on the measure space $(\mathscr{C}(\bm{S}), \mathscr{F}_S)$.\\

\noindent
Since the above definition seems not to be straight forward in reconciling incoherent forecasts, the following content explains how this can be used in practice to obtain reconciled probabilistic forecasts for hierarchical time series. \\

\noindent
Recall that $\hat{\bm{y}}_{t+h}$ is a set of incoherent point forecasts and the coordinates of that with respect to the basis $\bm{B}$ is given by (\ref{4.3}). Suppose $\hat{\bm{f}}(.)$ is the probability density of $\hat{\bm{y}}_{t+h}$. Our goal is to reconcile $\hat{\bm{f}}(.)$ such that the density lives on the $\mathscr{C}(\bm{S})$. In order to obtain this reconciled density, we need to project $\hat{\bm{f}}(\hat{\bm{y}}_{t+h})$ onto the $\mathscr{C}(\bm{S})$ along the direction of $\mathscr{C}(\bm{R})$. \\

\noindent
Let the density of coordinates of $\hat{\bm{y}}_{t+h}$ with respect to basis $\bm{B}$ is denoted by $\bm{f_B}(.)$. Then it follows from  (\ref{4.3}) and the facts on density of transformed variables, 

\begin{flalign}\label{4.5}
\bm{f_B}(\tilde{\bm{b}}_{t+h},\tilde{\bm{t}}_{t+h})=\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) \quad \Big|\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array}\Big|,
\end{flalign}
\noindent
where $|.|$ denote the determinant of a matrix. Now that we have the density of $\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T $, the marginal density of $(\tilde{\bm{b}}_{t+h})$ can be obtained by integrating (\ref{4.5}) over the range of $\tilde{\bm{t}}_{t+h}$. This will result the reconciled density of the bottom level series $(\tilde{\bm{b}}_{t+h})$. i.e.,
\begin{flalign}\label{4.6}
\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})=\int_{lim(\tilde{\bm{t}}_{t+h})}\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) \quad \Big|\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array}\Big| \quad d\tilde{\bm{t}}_{t+h}.
\end{flalign}

\noindent
Finally to get the reconciled density of the whole hierarchy, we simply follow the definition $(3.2)$ and have, 
\begin{flalign}\label{4.7}
\tilde{\bm{f}}(\tilde{\bm{y}}_{t+h})=\bm{S}\circ \tilde{\bm{f}}(\tilde{\bm{b}}_{t+h}).
\end{flalign}

\noindent
This final step will transform every point in the density $\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})$ to the $\mathscr{C}(\bm{S})<\bm{\mathbb{R}}^n$. Following example illustrates how this method can be used to reconcile an incoherent Gaussian forecast distribution.\\

\noindent
\textbf{Example 2}\\

\noindent  
Suppose $\mathscr{N}(\hat{\bm{\mu}}_{t+h}, \hat{\bm{\Sigma}}_{t+h})\leftrightarrow^d \hat{\bm{f}}(\hat{\bm{y}}_{t+h})$ is an incoherent forecast distribution at time $t+h$. Then from (\ref{4.5}) it follows,

\begin{flalign*}
\bm{f_B}(\tilde{\bm{b}}_{t+h},\tilde{\bm{t}}_{t+h})=\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) \quad \Big|\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array}\Big| = \frac{\hat{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}) }{\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|}. 
\end{flalign*}
\noindent
By substituting the Gaussian distribution function to $\bm{f_B}(.)$ we get, 

\begin{flalign*}
\bm{f_B}(.)&=\frac{\exp\left\{-\frac{1}{2}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}-\bm{\hat{\mu}}_{t+h})^T \bm{\hat{\Sigma}_{t+h}}^{-1}(\bm{S}\tilde{\bm{b}}_{t+h}+\bm{R}\tilde{\bm{t}}_{t+h}-\bm{\hat{\mu}}_{t+h})\right\}}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|},\\
&= \frac{\exp\left\{-\frac{1}{2}\Big(\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\begin{pmatrix}\tilde{\bm{b}}_{t+h}\\ \tilde{\bm{t}}_{t+h}\end{pmatrix}-\bm{\hat{\mu}}_{t+h}\Big)^T \bm{\hat{\Sigma}_{t+h}}^{-1}\Big(\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\begin{pmatrix}\tilde{\bm{b}}_{t+h}\\ \tilde{\bm{t}}_{t+h}\end{pmatrix}-\bm{\hat{\mu}}_{t+h}\Big)\right\}}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|},\\
\end{flalign*}

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|(\begin{array}{ccc}\bm{S} & \vdots& \bm{R}\end{array})^{-1} \Big|}
\exp \Big\{&-\frac{1}{2}\Big(\begin{pmatrix}\tilde{\bm{b}}_{t+h}\\ \tilde{\bm{t}}_{t+h}\end{pmatrix}-\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1}\bm{\hat{\mu}}_{t+h}\Big)^T\\
&\Big[\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^T\Big]^{-1}\\ 
&\Big(\begin{pmatrix}\tilde{\bm{b}}_{t+h}\\ \tilde{\bm{t}}_{t+h}\end{pmatrix}-\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1}\bm{\hat{\mu}}_{t+h}\Big) \Big\}.
\end{flalign*}
\noindent
Recall that, $$\begin{pmatrix}\bm{S} & \vdots& \bm{R}\end{pmatrix}^{-1} = \begin{pmatrix}
(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot\\ \cdots \\ (\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot
\end{pmatrix} = \begin{pmatrix}
\bm{P}\\\bm{Q}
\end{pmatrix},$$ where, $\bm{P}=(\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot$ and $\bm{Q}=(\bm{S}^T_\bot \bm{R})^{-1}\bm{S}^T_\bot$. Then, 

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\bm{\hat{\Sigma}}_{t+h}\Big|^{\frac{1}{2}}\Big|\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix} \Big|}
\exp \Big\{&-\frac{1}{2}\Big[\begin{pmatrix}\tilde{\bm{b}}_{t+h}\\ \tilde{\bm{t}}_{t+h}\end{pmatrix}-\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\mu}}_{t+h}\Big]^T\Big[\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big]^{-1}\\ 
&\Big[\begin{pmatrix}\tilde{\bm{b}}_{t+h}\\ \tilde{\bm{t}}_{t+h}\end{pmatrix}-\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\mu}}_{t+h}\Big] \Big\},
\end{flalign*}

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big|^{\frac{1}{2}}}
\exp \Big\{&-\frac{1}{2} \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix}^T \Big[\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big]^{-1}\\
& \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix} \Big\}.
\end{flalign*}
\noindent
Since, $\Big[\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}\bm{\hat{\Sigma}_{t+h}}\begin{pmatrix}\bm{P}\\\bm{Q}\end{pmatrix}^T\Big] = \begin{pmatrix}
\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T\\
\bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T
\end{pmatrix}$ we have, 

\begin{flalign*}
\bm{f_B}(.)=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\begin{pmatrix}
	\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T\\
	\bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T
	\end{pmatrix}\Big|^{\frac{1}{2}}}
\exp \Big\{&-\frac{1}{2} \begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix}^T\\ 
&\begin{pmatrix}
\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T\\
\bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T & \bm{Q}\bm{\hat{\Sigma}_{t+h}}\bm{Q}^T
\end{pmatrix}^{-1}\\
&\begin{pmatrix}\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}\\ \tilde{\bm{t}}_{t+h}- \bm{Q}\bm{\hat{\mu}}_{t+h}\end{pmatrix} \Big\}.
\end{flalign*}
\noindent
$\bm{f_B}(.)$ gives the joint distribution of $\begin{pmatrix}\tilde{\bm{b}}^T_{t+h} & \vdots& \tilde{\bm{t}}^T_{t+h}\end{pmatrix}^T $, which is a multivariate Gaussian distribution. Then from (\ref{4.6}) and the properties of marginalization of multivariate Gaussian distribution it follows,
\begin{flalign}\label{ex:2.1}
\tilde{\bm{f}}(\tilde{\bm{b}}_{t+h})=\frac{1}{(2\pi)^{\frac{n}{2}}\Big|\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T\Big|^{\frac{1}{2}}}
\exp \Big\{-\frac{1}{2} (\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h})^T (\bm{P}\bm{\hat{\Sigma}_{t+h}}\bm{P}^T)^{-1}(\tilde{\bm{b}}_{t+h} - \bm{P}\bm{\hat{\mu}}_{t+h}) \Big\}.
\end{flalign}

\noindent
(\ref{ex:2.1}) implies $\tilde{\bm{b}}_{t+h} \sim \mathscr{N}(\bm{P}\bm{\hat{\mu}}_{t+h}, \bm{P}\hat{\bm{\Sigma}}_{t+h}\bm{P}^T)$ where $\bm{P} = (\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot$. Then from (\ref{4.7}) it follows that,
\begin{flalign}
\tilde{\bm{f}}(\tilde{\bm{y}}_{t+h})=\tilde{\bm{f}}(\bm{S}\tilde{\bm{b}}_{t+h}).
\end{flalign}
\noindent
Therefore, the reconciled Gaussian forecast distribution of the whole hierarchy is\\
$\mathscr{N}(\bm{SP}\bm{\hat{\mu}}_{t+h}, \bm{SP}\hat{\bm{\Sigma}}_{t+h}\bm{P}^T\bm{S}^T)$.




\section{Evaluation of hierarchical probabilistic forecasts}

The necessary final step in hierarchical forecasting is to make sure that our forecast distributions are accurate enough to predict the uncertain future. In general, forecasters prefer to maximize the sharpness of the predictive distribution subject to the calibration \citep{Gneiting2014}. Therefore the probabilistic forecasts should be evaluated with respect to these two properties.\\

\noindent
Calibration refers to the statistical compatibility between probabilistic forecasts and realizations. In other words, random draws from a perfectly calibrated predictive distribution should be equivalent to the realizations. On the other hand, sharpness refers to the spread or the concentration of prediction distributions and it is a property of forecasts only. The more concentrated the predictive distributions, the sharper the forecasts are \citep{Gneiting2008}. However, independently assessing the calibration and sharpness will not help to properly evaluate the probabilistic forecasts. Therefore to assess these properties simultaneously, we use scoring rules. \\

\noindent
Scoring rules are summary measures obtained based on the relationship between predictive distribution and the realizations. In some studies, researchers take the scoring rules to be positively oriented which they would wish to maximize \citep{Gneiting2007}. However, scoring rules were also defined to be negatively oriented which forecasters wish to minimize \citep{Gneiting2014}. We consider these negatively oriented scoring rules to evaluate probabilistic forecasts in hierarchical time series. \\

\noindent
Let $\bm{{\breve{Y}}}$ and $\bm{Y}$ be a $n$-dimensional random vectors from the predictive distribution $\mathbold{F}$ and the true distribution $\mathbold{G}$. Further let $\mathbold{y}$ be a $n$-dimensional realization. Then the scoring rule is a numerical value $S(\mathbold{{\breve{Y}},y})$ assign to each pair $(\mathbold{{\breve{Y}},y})$ and the proper scoring rule is defined as,
\begin{equation}\label{eq:(3.1.)}
E_{\mathbold{G}}[S(\mathbold{Y,y})] \le E_{\mathbold{G}}[S(\mathbold{{\breve{Y}},y})],
\end{equation}
\noindent
where $E_{\mathbold{G}}[S(\mathbold{Y,y})]$ is the expected score under the true distribution $\mathbold{G}$ \citep{Gneiting2008, Gneiting2014}. \\

\noindent
Following table summarizes few existing proper scoring rules. 


\begin{center}
	\captionof{table}{\textit{Scoring rules to evaluate multivariate forecast densities }} 
	
	\small
	\resizebox{\linewidth}{!}{
		
		\begin{tabular}{ L | L | L}
			\hline
			\hline
			\textbf{Scoring rule} & \textbf{Expression} & \textbf{Reference}\\
			\\
			\hline
			\hline \\
			\text{Log score} & LS(\breve{\bm{F}},\bm{y}_{T+h}) = -log {\breve{\bm{f}}(\bm{y}_{T+h})} & \text{\citet{Gneiting2007}} \\
			\\ 
			\hline \\
			\text{Energy score} & eS(\bm{\breve{Y}_{T+h},y_{T+h}}) = E_{\breve{\bm{F}}}||\breve{\bm{Y}}_{T+h}-\bm{y}_{T+h}||^\alpha - \frac{1}{2}E_{\breve{\bm{F}}}||\breve{\bm{Y}}_{T+h}-\breve{\bm{Y}}'_{T+h}||^\alpha, \alpha \in (0,2] & \text{\citet{Gneiting2008}} \\
			\\
			\hline
			\text{Variogram score} & VS(\breve{\bm{F}}, \bm{y}_{T+h}) = \displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n}w_{ij}\left(|y_{T+h,i} - y_{T+h,j}|^p - E_{\breve{\bm{F}}}|\breve{Y}_{T+h,i}-\breve{Y}_{T+h,j}|^p\right)^2 & \text{\citet{SCHEUERER2015}} \\
			\hline
		\end{tabular}
	
		}
\end{center}
\textit{NOTE: $\bm{\breve{Y}}_{T+h}$ and $\bm{\breve{Y}}'_{T+h}$ be two independent random vectors from the coherent forecast distribution $\breve{\bm{F}}$ with the density function $\breve{\bm{f}}(.)$ at time $T+h$ and $\bm{y}_{T+h}$ is the vector of realizations. Further $\breve{Y}_{T+h,i}$ and $\breve{Y}_{T+h,j}$ are $i^{\text{th}}$ and $j^{\text{th}}$ components of the vector $\breve{\bm{Y}}_{T+h}$. Further the variogram score given is for order $p$ where, $w_{ij}$ are non-negative weights.}\\

\noindent
Even though the log score can be used evaluate simulated forecast densities with large samples \citep{Jordan2017}, it is more convenient to use if it is reasonable to assume a parametric forecast density for the hierarchy. However, the \qq{degenerecy} of coherent forecast densities would be problematic when using log scores. We will discuss more about this in the next sub section.\\

\noindent
In the energy score, for $\alpha=2$, it can be easily shown that
\begin{equation} \label{eq:(5.1)}
eS(\bm{\breve{Y}_{T+h},y_{T+h}}) = ||\bm{y}_{T+h}-\breve{\bm{\mu}}_{T+h}||^2,
\end{equation}

\noindent
where $\breve{\bm{\mu}}_{T+h} =E_{\bm{F}}(\breve{\bm{Y}}_{T+h}) $. Therefore in the limiting case, the energy score only measures the accuracy of the forecast mean, but not the entire distribution. Further \citet{Pinson2013a} argued that the Energy score given in table 1 has a very low discrimination ability for incorrectly specified covariances, even though it discriminates the misspecified means well. \\

\noindent
However, \citet{SCHEUERER2015} have shown that the variogram score has a high discrimination ability of misspecified means, variance and correlation structure than the Energy score. Further they suggested the variogram score with $p=0.5$ is more powerful.\\
\noindent
For a possible finite sample of size $B$ from the multivariate forecast density $\breve{\bm{F}}$, the variogram score is defined as, 

\begin{flalign}
VS(\breve{\bm{F}}, \bm{y}_{T+h}) = \displaystyle\sum_{i=1}^{n}\displaystyle\sum_{j=1}^{n}w_{ij}\left(|y_{T+h,i} - y_{T+h,j}|^p - \frac{1}{m} \displaystyle\sum_{k=1}^{B} |\breve{Y}^k_{T+h,i}-\breve{Y}^k_{T+h,j}|^p\right)^2.
\end{flalign}
  



%For example, assume you have a Gaussian coherent predictive density $\bm{\tilde{f}}_{T+h}$. Then the log score is given by, 

%\begin{equation} 
%LS(\bm{\tilde{f}_{T+h},y_{T+h}}) = -\log \left(\frac{1}{(2\pi)^{\frac{m}{2}}|\bm{\tilde{\Sigma}}_{T+h}|^{\frac{1}{2}}}\exp\left[-\frac{1}{2}(\bm{y}_{T+h}-\bm{\tilde{\mu}}_{T+h})^T %\bm{\tilde{\Sigma}_{T+h}}^{-1}(\bm{y}_{T+h}-\bm{\tilde{\mu}}_{T+h})\right]\right),
%\end{equation}

%\noindent
%where $\bm{\tilde{\mu}}_{T+h}$ and $\bm{\tilde{\Sigma}}_{T+h}$ are the coherent mean and variance-covariance matrix of the predictive Gaussian density and both lies in the linear %subspace spanned by $\mathscr{C}(\bm{S})$. Then $\bm{\tilde{\Sigma}}_{T+h}$ is always a singular matrix and hence the determinant will be zero. Therefore the density function of %degenerate Gaussian distribution is undefined. However, we could use the pseudo determinant, i.e. product of positive eigenvalues and the pseudo inverse to calculate the log score %to compare the predictive ability of coherent forecast densities.  



\subsection{Evaluating coherent forecast densities}

As it was mentioned in the previous section, any coherent hierarchical forecast density is a degenerate density. To the best of our knowledge, there is no proper multivariate scoring rule in literature to evaluate degenerate densities. Further it can be easily seen that some of the existing scoring rules breakdown under the degeneracy. For example take the log score in the univariate case. Suppose the true density is degenerate at $x=0$, i.e. $f(x)=\mathbb{1}\{x=0\}$.  Now consider two predictive densities $p_1(x)$ and $p_2(x)$. Let $p_1(x)$ is equivalent to the true density, i.e. $p_1(x)=\mathbb{1}\{x=0\}$ and $p_2(x) =^d N(0,\sigma^2)$ with $\sigma^2 < (2\pi)^{-1}$. The expected log score of $p_1$ is:
$$E_f[S(f,f)] = E_f[S(p_1,f)] = -ln[p_1(x=0)]=0,$$
\noindent
and that of $p_2$ is:
$$E_f[S(p_2,f)] = -ln[p_2(x=0)]<0.$$ 
\noindent
Therefore $S(f,f) > S(p_2,f)$ and hence there exist at least one forecast density which breaks the condition (\ref{eq:(3.1.)}) for proper scoring rule. This implies log score cannot be used to evaluate the degenerate densities.  \\

\noindent
This suggest that it is necessary to have a rule of thumb to use these scoring rules in order to evaluate coherent forecast densities. First we should notice that, even though the coherent distribution of the entire hierarchy is degenerate, the density of the basis set of series is non-degenerate since these series are linearly independent. Further, if we can correctly specify the forecast distribution of these basis set of series, then we have almost obtained the correct forecast distribution of the whole hierarchy. Therefore, we propose to evaluate the predictive ability of only the basis set of series of the coherent forecast density by using any of the above discussed multivariate scoring rules. This will also avoid the impact of degeneracy for the scoring rules. \\

\noindent
For example, since the bottom level series is a set of basis series for a given hierarchy, we can evaluate the predictive ability of the bottom level series of the coherent forecast distribution instead of evaluating the whole distribution. Further if our purpose is to compare two coherent forecast densities, we can compare the forecast ability of only the bottom level forecast densities. 


\subsection{Comparison of coherent and incoherent forecast densities}

It is also important to assess how the coherent or reconciled forecast densities improve the predictive ability compared to the incoherent forecasts. Clearly we cannot use multivariate scoring rules, even for the basis set of series, since the coherent and incoherent forecast densities lie in two different matrix spaces.\\

\noindent
However we could compare individual margins of the forecast density of the hierarchy using univariate proper scoring rules. Most widely used Continuous Ranked Probability Score (CRPS) for evaluating univariate forecast densities would be helpful for this. \\
\begin{equation} \label{eq:(3.6)}
CRPS(\breve{F}_i,y_{T+h,i}) = E_{\breve{F}_i}|\breve{Y}_{T+h,i}-y_{T+h,i}| - \frac{1}{2}E_{\breve{F}_i}|\breve{Y}_{T+h,i}-\breve{Y}'_{T+h,i}|,
\end{equation}
       
\noindent
where $\breve{Y}_{T+h,i}$ and $\breve{Y}'_{T+h,i}$ are two independent copies from the $i^{\text{th}}$ reconciled marginal forecast distribution $\tilde{F}_i$ of the hierarchy and $y_{T+h,i}$ is the $i^{\text{th}}$ realization from the true marginal distribution $G_i$. We can also use univariate log scores for which we could assume a parametric forecast distribution.
 



\section{Probabilistic forecast reconciliation in the Gaussian framework}


Main purpose of this section is to establish the importance of reconciliation in probabilistic hierarchical forecasting. We narrow down the simulation setting for the Gaussian framework in this work. That is, suppose all the historical data in the hierarchy follows a multivariate Gaussian distribution, i.e. $\bm{y}_T \sim \mathscr{N}(\bm{\mu}_T, \bm{\Sigma}_T)$ where both $\bm{\mu}_T$ and $\bm{\Sigma}_T$ lives in $\mathscr{C}(\bm{S})$ by nature of the hierarchical time series. We are interested in estimating the predictive Gaussian distribution of $\bm{Y}_{T+h}| \bm{\mathcal{I}}_T$ where $\bm{\mathcal{I}}_T= \{\bm{y}_1,\bm{y}_2,....,\bm{y}_T\}$, which should also lives in $\mathscr{C}(\bm{S})$. \\

\noindent
Suppose we independently fit the models for each series in the hierarchy. Let $\hat{Y}_{T+h,i}$ gives the point forecast of $i^{\text{th}}$ series at time $T+h$ conditional on the past observations $\{y_{1,i},...,y_{T,i}\}$ which minimizes the mean squared forecast error of $Y_{T+h,i}|\{y_{1,i},...,y_{T,i}\}$, where $i=1,...,n$. Then we have, $$\hat{Y}_{T+h,i} = E[Y_{T+h,i}|y_{1,i},...,y_{T,i}]$$  

\noindent
Let $\hat{\bm{Y}}_{T+h}=(\hat{Y}_{T+h,1},...,\hat{Y}_{T+h,n})'$. Then, $$\hat{\bm{\mu}}_{T+h}=E[\bm{Y}_{T+h}|\bm{\mathcal{I}}_T] = \hat{\bm{Y}}_{T+h}.$$ 

\noindent
Further we have, 
\begin{flalign*}
Var(\bm{Y}_{T+h}|\bm{\mathcal{I}}_T) = & E[(\bm{Y}_{T+h} - E[\bm{Y}_{T+h,i}|\bm{\mathcal{I}}_T])(\bm{Y}_{T+h} - E[\bm{Y}_{T+h,i}|\bm{\mathcal{I}}_T])T]\\
& = E[(\bm{Y}_{T+h} - \hat{\bm{Y}}_{T+h})(\bm{Y}_{T+h} - \hat{\bm{Y}}_{T+h})T|\bm{\mathcal{I}}_T]\\
& = \bm{W}_{T+h}
\end{flalign*}

\noindent
Then the conditional forecast distribution at time $T+h$ will be multivariate Gaussian distribution with mean $\hat{\bm{Y}}_{T+h}$ and variance $\bm{W}_{T+h}$. 
Since the point forecasts were obtained individually, it is very unlikely that the mean and variance lies in the coherent subspace. Therefore it gives the incoherent conditional forecast distribution which we denote as,
\begin{flalign}\label{eq:(6.1)}
\widehat{\bm{Y}_{T+h,i}|\bm{\mathcal{I}}_T} \sim \mathscr{N}(\hat{\bm{Y}}_{T+h}, \bm{W}_{T+h})
\end{flalign}  

\noindent
Since our primary objective is to find the coherent forecast density of the hierarchy, we need to reconciled \ref{eq:(6.1)}. Recalling from example 2, the reconciled Gaussian predictive distribution is then given by, 
\begin{flalign}\label{eq:(6.1)}
\widetilde{\bm{Y}_{T+h,i}|\bm{\mathcal{I}}_T} \sim \mathscr{N}(\bm{SP}\hat{\bm{Y}}_{T+h}, \bm{SP}\bm{W}_{T+h}\bm{P}^T\bm{S}^T)
\end{flalign}  
\noindent
where, $\bm{P} = (\bm{R}^T_\bot \bm{S})^{-1}\bm{R}^T_\bot$.\\


\noindent
\textbf{Result 1}: Choosing $\bm{R}^T_\bot = \bm{S}^T\bm{W}_{T+h}^{-1}$ will ensure at least the mean of the predictive Gaussian distribution is optimally reconciled with respect to the energy score.\\

\noindent
Result 1 can be easily shown as follows. From (\ref{eq:(5.1)}) the energy score at the upper limit of $\alpha$ is given by, $||\bm{y}_{T+h}-\bm{SP}\hat{\bm{y}}_{T+h}||^2$. Then the expectation of energy score with respect to the true distribution is equivalent to the trace of mean squared forecast error, i.e. $$E_{\bm{G}}[eS(\bm{\tilde{Y}_{T+h},y_{T+h}})]= Tr\{E_{\bm{y}_{T+h}}[(\bm{Y}_{T+h}-\bm{SP}\hat{\bm{Y}}_{T+h})(\bm{Y}_{T+h}-\bm{SP}\hat{\bm{Y}}_{T+h})^T|\mathcal{I}_{T}]\}.$$ 
\noindent
From Theorem 1 of \citet{Wickramasuriya2017} it immediately follows that $\bm{P} = (\bm{S}^T\bm{W}_{T+h}^{-1}\bm{S})^{-1}\bm{S}^T\bm{W}_{T+h}^{-1}$ minimizes the expected energy score constrained on the unbiasedness of reconciled forecasts. Thus we have $\bm{R}^T_\bot = \bm{S}^T\bm{W}_{T+h}^{-1}$. \\

\noindent
It should be noted that $\bm{W}_{T+h}$ can be estimated in different methods which yields different estimation of $\bm{R}^T_\bot$. Following table summarizes these methods. 

\begin{center}
	\captionof{table}{\textit{Summarizing different estimates of $\bm{R}^T_\bot$} }
	
	
	\resizebox{\linewidth}{!}{
		\small
		\small
		\begin{tabular}{c c c }
			\hline\hline\\
			\textbf{Method}& \textbf{Estimate of $\bm{W}_{h}$} & \textbf{Estimate of $\bm{R}^T_\bot$} \\  
			\hline \hline\\
			OLS & $\bm{I}$ & $\bm{S}^T$\\
			\hline\\
			
			MinT(Sample) & 	$\bm{W}_{T+1}$ & $\bm{S}^T\bm{W}_{T+1}^{-1}$\\
			& (1-step ahead insample variance covariance matrix) &\\
			\hline\\
			
			MinT(Shrink) & $\mathbold{W}_{T+1}^{shr} = \tau\text{Diag}(\mathbold{W}_{T+1}) + (1-\tau)\mathbold{W}_{T+1}$ & $\bm{S}^T(\bm{W}_{T+1}^{shr})^{-1}$\\
			\hline\\
			
			MinT(WLS) & $\mathbold{W}_{T+1}^{wls} = \text{Diag}(\mathbold{W}_{T+1}^{shr})$ & 	$\bm{S}^T(\bm{W}_{T+1}^{wls})^{-1}$\\
			\hline
			\hline
			
		\end{tabular}
	}
\end{center}
\textit{Note: $\mathbold{W}_{T+1}^{shr}$ is a shrinkage estimator proposed by \citet{Schafer2005} and also used by \citet{Wickramasuriya2017}, where $\tau = \frac{\sum_{i \ne j}\hat{Var}(\hat{r}_{ij})}{\sum_{i \ne j}\hat{r}_{ij}^2}$, $\hat{r}_{ij}$ is the $ij$-th element of sample correlation matrix. Further Diag($\bm{A}$) denote the diagonal matrix of $\bm{A}$}.\\

\noindent
It is worth mentioning that all these forecasting methods were well established in the context of point forecast reconciliation \citep{Hyndman2011, Wickramasuriya2017, Hyndman2016}. However our attempt is to emphasis the use of these reconciliation methods in the context of probabilistic forecasts at least in the Gaussian framework. \\

\noindent
\textbf{Simulation setup}\\

\noindent
We consider the hierarchy given in figure (1) for this simulation study. This hierarchy consists two aggregation levels with four bottom level series. Each bottom level series will be generated first and add them up to obtain the data for respective upper level series. Hierarchical time series in practice contain much noisier series in the bottom level than in aggregate series. In order to simulate this feature in the hierarchy, we refer to \citet{Wickramasuriya2017} and the data generating process will be given as follows. \\

\noindent
Suppose $\{w_{AA,t},w_{AB,t},w_{BA,t},w_{BB,t}\}$ are generated from $ARIMA(p,d,q)$ processes where, $(p,q)$ and $d$ take integers from $\{1,2\}$ and $\{0,1\}$ respectively with equal probability. Further, the contemporaneous errors $\{\epsilon_{AA,t},\epsilon_{AB,t},\epsilon_{BA,t},\epsilon_{BB,t}\} \sim \mathcal{N}(\bm{0}, \bm{\Sigma})$. The parameters for $AR$ and $MA$ components will be randomly and uniformly generated from $[0.3,0.5]$ and $[0.3,0.7]$ respectively. Then the bottom level series $\{y_{AA,t},y_{AB,t},y_{BA,t},y_{BB,t}\}$ will be obtained as: 
$$y_{AA,t} = w_{AA,t} + u_t - 0.5v_t,$$
$$y_{AB,t} = w_{AB,t} - u_t - 0.5v_t,$$
$$y_{BA,t} = w_{BA,t} + u_t + 0.5v_t,$$
$$y_{BB,t} = w_{BB,t} - u_t + 0.5v_t,$$ 
\noindent
where $u_t \sim N(0,\sigma^2_u)$ and $v_t \sim N(0,\sigma^2_v)$. 

\noindent
To obtain the aggregate series at level 1, we add their respective bottom level series such as: 
$$y_{A,t} = w_{AA,t} + w_{AB,t} - v_t,$$
$$y_{B,t} = w_{BA,t} + w_{BB,t} + v_t,$$
\noindent
and the total series will be obtained as: 
$$y_{Tot,t} = w_{AA,t} + w_{AB,t} + w_{BA,t} + w_{BB,t}.$$
\noindent
To get less noisier aggregate series than disaggregate series, we choose $\bm{\Sigma}, \sigma^2_u$ and $\sigma^2_v$ such that, 

$$Var(\epsilon_{AA,t}+\epsilon_{AB,t}+\epsilon_{BA,t}+\epsilon_{BB,t}) \le Var(\epsilon_{AA,t}+\epsilon_{AB,t}-v_t) \le Var(\epsilon_{AA,t}+u_t-0.5v_t),$$
$$\bm{l}_1\bm{\Sigma} \bm{l}_1^T \le \bm{l}_2\bm{\Sigma} \bm{l}_2^T + \sigma^2_v \le  \bm{l}_3\bm{\Sigma} \bm{l}_3^T + \sigma^2_u + \frac{1}{4}\sigma^2_v,$$
\noindent
where $\bm{l}_1 = \begin{pmatrix} 1&1&1&1 \end{pmatrix}, \bm{l}_2 = \begin{pmatrix} 1&1&0&0 \end{pmatrix}$ and $\bm{l}_3 = \begin{pmatrix} 1&0&0&0 \end{pmatrix}$.
\noindent 
This follows, 
$$\bm{l}_1\bm{\Sigma} \bm{l}_1^T - \bm{l}_2\bm{\Sigma} \bm{l}_2^T \le \sigma^2_v \le \frac{4}{3}(\sigma^2_u + \bm{l}_3\bm{\Sigma} \bm{l}_3^T - \bm{l}_2\bm{\Sigma} \bm{l}_2^T).$$
\noindent
Thus we choose, 
$\mathbold{\Sigma} =  \begin{pmatrix} 5.0 & 3.1 & 0.6 & 0.4\\
3.1 & 4.0 & 0.9 & 1.4\\
0.6 & 0.9 & 2.0 & 1.8\\
0.4 & 1.4 & 1.8 & 3.0\\
\end{pmatrix}, \quad \sigma^2_u = 19$ and $\sigma^2_u = 18$ in our simulation setting. \\

\noindent
As such we generate data for the hierarchy with sample size $T=501$. Then univariate $ARIMA$ models were fitted for each series independently using the first 500 observations and obtain 1-step ahead base (incoherent) forecasts. We use \textit{forecast} package in \textbf{R}-software \citet{hyndman2017forecasting} for model fitting and forecasting. Further, different estimates of $\bm{W}_{T+1}$ and the corresponding $\bm{R}^T_\bot$ were obtained as summarized in Table 2. This process was then replicated using $1000$ different data sets from the same data generating process. \\

\noindent
To assess the predictive performance of different forecasting methods, we use scoring rules as discussed in section 5. In addition to that we use Skill score \citep{Gneiting2007} for any comparison. For a given forecasting method, evaluated by a particular scoring rule $S(.)$ , the skill score will be calculated as follows, 
\begin{flalign}
Ss[S_B(.)] = \frac{S_B(\mathbold{{{Y}},y})^{ref} - S_B(\mathbold{{\breve{Y}},y})}{S_B(\mathbold{{{Y}},y})^{ref}}\times 100\%,
\end{flalign} 
\noindent
where $S_B(.)$ is average score over $B$ samples and $S_B(\mathbold{{{Y}},y})^{ref}$ is the average score of the reference forecasting methods. Thus $Ss[S_B(.)]$ gives the percentage improvement of the preferred forecasting method relative to the reference method. Any negative value of $Ss[S_B(.)]$ indicate that the method we compared is poor than the reference method, whereas any positive value indicates that method is superior to the reference method.\\

\noindent
As it was mentioned before we wish to establish the importance of reconciliation methods from this simulation study. In particular we compare different reconciliation methods over the conventional bottom-up method and also evaluate the predictive ability of coherent forecasts over incoherent forecasts. For the former comparison we use bottom level probabilistic forecasts and calculate the percentage skill score based on energy score, log score and variogram score for each reconciliation method with reference to the bottom up method (presented in Table 3). For the latter comparison we use percentage skill score based on CRPS and univariate log score for coherent probabilistic forecasts of each individual series with reference to incoherent forecasts (presented in Table 5).\\

\noindent
It is clearly evident from the results in table 3 that the multivariate reconciled forecasts for the bottom level series from MinT(Shrink), MinT(Sample) and OLS methods outperform the bottom up forecasts. Further it turns out that probabilistic forecasts from MinT(Sample) has the best predictive ability. MinT(Shrink) also performing fairly well compared to the other methods. \\

\noindent
When it comes to the comparison of incoherent versus coherent probabilistic forecasts using the individual series in the hierarchy, MinT(WLS) seems to produce probabilistic forecasts with the best predictive ability. Nonetheless, MinT(Shrink), MinT(Sample) and OLS (OLS except for total level) produce coherent forecasts with improved predictive ability compared to incoherent forecasts. Further it can be noticed that bottom-up method produce worst forecasts.  

\pagebreak
\begin{center}
	\captionof{table}{\textit{Comparison of reconciliation methods against Bottom up method} }
	
	
	\resizebox{\linewidth}{!}{
		\small
		\begin{tabular}{@{}l S[table-format=2.2] S[table-format=2.2]  S[table-format=2.2] @{}}
			
			\hline
			\hline 
			
			
			{Forecasting method} &  {Energy score} & {Log score} & {Variogram score} \\ 
			\hline
			\hline
			
			MinT(Shrink) & 10.13 & 6.74 & 4.71  \\ 
			
			MinT(Sample) & $\mathB{10.25}$ & $\bm{6.83}$ & $\bm{5.00}$ \\
			
			MinT(WLS) & 5.06 & -4.85 & -2.06  \\
			
			OLS & 6.36 & 4.52 & 0.59  \\
			
			\hline
			
			\textit{Bottom up} & $\mathbi{8.49}$ & $\mathbi{12.16}$ & $\mathbi{3.40}$    \\
			
			\hline
			\hline 
			
			
		\end{tabular}
	}
\end{center}
\textit{Note: "Bottom up" row represent the average score for the bottom up method. Each entry above this row represent the percentage skill score with reference to the bottom up forecasting method. A positive(negative) entry shows the percentage increase(decrease) of score for different reconciliation methods with relative to the bottom up method}.\\\\


\begin{center}
	\captionof{table}{\textit{Comparison of incoherent vs. coherent forecasts for the aggregate series using Skill score}} 
	
	
	\resizebox{\linewidth}{!}{
	\scriptsize	
		\begin{tabular}{@{}l S[table-format=1.2] S[table-format=1.2]  S[table-format=1.2] S[table-format=1.2] S[table-format=1.2]  S[table-format=1.2] S[table-format=1.2] S[table-format=1.2]  S[table-format=1.2] S[table-format=1.2] S[table-format=1.2]  S[table-format=1.2] S[table-format=1.2] S[table-format=1.2] @{}}
		
		\hline
		\hline 
		
		
		\multirow{2}{*}{\text{Forecasting method}} & \multicolumn{2}{ c }{\text{Total}} & \multicolumn{2}{c}{\text{Series - A}} & \multicolumn{2}{c}{\text{Series - B}} & \multicolumn{2}{c}{\text{Series - AA}} & \multicolumn{2}{c}{\text{Series - AB}}& \multicolumn{2}{c}{\text{Series - BA}} & \multicolumn{2}{c}{\text{Series - BB}} \\ \cline{2-15} & \text{CRPS} &  \text{LogS} &   \text{CRPS} &   \text{LogS} &   \text{CRPS} &   \text{LogS} & \text{CRPS} &  \text{LogS}& \text{CRPS} &  \text{LogS}& \text{CRPS} &  \text{LogS}& \text{CRPS} &  \text{LogS} \\ 
		\hline
		\hline
		
		MinT(Shrink)  & 1.99 & 0.50 & 19.82 & 4.33 &	14.18 &	3.21 & 16.09 & 3.40 & 18.88 & 4.08 & 11.59 & 2.88 &	14.68 &	3.04 \\
		
		MinT(Sample) & 2.17 & 0.50 & 19.58 & 4.33 &	13.89 &	3.21 & 16.54 & 3.40 & 19.45 & 4.29 & 11.97 & 2.88 &	14.98 &	3.26 \\
		
		MinT(WLS)  & $\bm{22.83}$ & $\bm{7.04}$ &	$\bm{56.35}$ &	$\bm{18.76}$ &	$\bm{51.44}$ &	$\bm{17.34}$ &	$\bm{37.84}$ &	$\bm{10.43}$ &	$\bm{36.13}$ &	$\bm{10.09}$ &	$\bm{30.17}$ & $\bm{9.73}$ & $\bm{37.35}$ & $\bm{10.65}$ \\
		
		OLS  & -17.39 &	-4.77 &	13.27 &	2.89 & 10.63 & 2.36 & 9.41 & 1.70 &	10.1 & 1.93 & 7.44 & 1.33 &	9.21 & 1.74	\\
		
		Bottom up & -259.78 & -33.67 & -20.22 &	-3.92 &	-19.92 & -3.85 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
		
		\hline
		
		\textit{Incoherent} & $\mathbi{5.52}$ & $\mathbi{3.98}$	& $\mathbi{12.51}$  & $\mathbi{4.85}$ & $\mathbi{10.44}$ & $\mathbi{4.67}$ & $\mathbi{10.94}$ &	$\mathbi{4.70}$ & $\mathbi{10.49}$ & $\mathbi{4.66}$ & $\mathbi{9.14}$ & $\mathbi{9.14}$ & $\mathbi{9.88}$ & $\mathbi{ 4.60}$ \\ 
		
		\hline
		\hline
		
		
	\end{tabular}
	}
\end{center}
\textit{Note: "Incoherent" row represent the average score for incoherent forecasts. Each entry above this row represent the percentage skill score with reference to the incoherent forecasts. A positive(negative) entry shows the percentage increase(decrease) of score for different forecasting methods with relative to incoherent forecasts}.\\\\







\section{Conclusions.}

Although the problem of hierarchical point forecasts is well studied in literature, there is a lack of attention in the context of probabilistic forecasts. Thus we attempted to fill this gap in literature by providing substantial theoretical background to the problem. We initially provided rigorous definitions for the coherent point and probabilistic forecasts using the principles of measure theory. Due to the aggregation nature of hierarchy, the probability density is a degenerate density. Thus the forecast distribution that we opt to find should also lies in a lower dimensional subspace of $\bm{\mathbb{R}}^{n}$.\\ 

\noindent
As it was well established that the reconciliation outperform other conventional point forecasting methods in hierarchical literature, we proposed to use reconciliation in probabilistic forecasting as well to obtain coherent degenerate densities. We provided distinct definition for density forecast reconciliation and how it can be used to reconcile incoherent densities in practice.\\

\noindent
Assuming a multivariate Gaussian distribution for the hierarchy, we showed how to obtain reconciled Gaussian forecast densities, utilizing available information in the hierarchy. The simulation study further showed that the MinT reconciliation method \citep{Wickramasuriya2017} is useful in producing improved coherent probabilistic forecasts at least in the Gaussian framework.    

 







   



\newpage
\printbibliography
\end{document}


