\documentclass{beamer}
\usetheme{Warsaw}
\usepackage{amsmath,amsthm, bm, tikz,qtree}
\usetikzlibrary{arrows,positioning,shapes,fit,calc}
\usepackage[greek,english]{babel}

\title{Probabilistic Forecast Reconciliation}
\date{August 15, 2018}
\author[GPAH]{Puwasala Gamakumara, Anastasios Panagiotelis, George Athanasopoulos and Rob Hyndman}
\begin{document}
  \begin{frame}
    \maketitle
  \end{frame}
  \section{Motivation}
  \begin{frame}{Motivating Examples}
  \begin{itemize}
  	\item Gross Domestic Product
  	\begin{itemize}
  		\item An aggregate of consumption, investment, government spending and trade balance
  		\item Further breakdown, e.g. consumption of food, rent, etc.
  	\end{itemize}
    \item Retail sales
    \begin{itemize}
    	\item Aggregate of different product category sales
    	\item Further breakdown into individual stock keeping units.  
    \end{itemize}
    \item Potentially need forecasts of all time series.
  \end{itemize}
  \end{frame}
  \begin{frame}{Incoherent Forecasts}
  \begin{itemize}
    \item Potential approaches
  	\begin{itemize}
  		\item Univariate models
  		\item Multivariate models
  		\item Judgemental forecasts
  	\end{itemize}
  \item Forecasts do not respect aggregation structure ({\bf Incoherent})
  \item Outcome {\em does} respect aggregation  structure ({\bf Coherent})
  \item Motivation is aggregation but can be generalised to any linear constraints.
  \end{itemize}
  \end{frame}
  \begin{frame}{Reconciliation}
  \begin{itemize}
  	\item Begin with a vector of forecasts that are incoherent.
  	\item Adjust these {\bf ex post} to make them coherent.
  	\item There are good solutions for point forecasting that:
  	\begin{itemize}
  		\item Guarantee coherent forecasts.
  		\item Improve forecast accuracy overall.
  	\end{itemize}
  	\item Generalisation to probabilistic forecasts is our contribution.
  	\item Getting there necessitates a rethink of the existing point forecasting literature. 
  \end{itemize}
  \end{frame}
  \begin{frame}{What we do NOT do}
  \begin{itemize}
  	\item All information is contained in the most disaggregate series.
  	\item In principle using the correct multivariate model for the most disaggregate series and aggregating them should work.
  	\item Disaggregate series are: 
  	\begin{itemize}
  		\item Very noisy
  		\item High-dimensional
  		\item Prone to model misspecification
  	\end{itemize} 
  \end{itemize}
  \end{frame}
  \section{Point Forecast Reconciliation}
 \begin{frame}{A simple hierarchy}
  	Consider a hierarchy given by
  	\begin{figure}
	    \begin{center}
		\leaf{\alt<2>{\color{blue}AA}{AA}} \leaf{\alt<2>{\color{blue}AB}{AB}} 
		\branch{2}{A}
		\leaf{\alt<2>{\color{blue}BA}{BA}} \leaf{\alt<2>{\color{blue}BB}{BB}}
		\branch{2}{B}
		\branch{2}{Tot}
		\qobitree
	    \end{center}
    \end{figure}
    \begin{itemize}
    	\item Let $n$ be the number of series, $\bm{y}$ be an n-vector of all series.
    	\only<2>{\item Let {\color{blue}$m$} be the bottom level series and {\color{blue}$\bm{b}$} be an {\color{blue}$m$}-vector of the bottom level series.}
    \end{itemize}    
\end{frame}  
\begin{frame}{The ${\bm S}$ matrix}
    The matrix ${\bm S}$ defines the aggregation constraints, e.g.
    \begin{equation*}
    {\bm S}=\begin{pmatrix} 1 &1 &1 &1 \\1 &1 &0 &0 \\0 &0 &1 &1 \\ &{\bm I_{4\times 4}}
    \end{pmatrix}  
    \end{equation*}
    Coherence holds when
    \begin{equation*}
    {\bm y}={\bm S}{\bm b}
    \end{equation*} 
\end{frame}
\begin{frame}{As a regression model}
  \begin{itemize}
  	\item Cast the problem as a regression model with base forecasts $\hat{\bm{y}}$ as the ``dependent variable'' and ${\bm S}$ as the ``design matrix''. 
  	\begin{equation*}
  	\hat{\bm{y}}={\bm{S}}{\bm\beta}+{\bm e}
  	\end{equation*}
  	\item Initial approach (Athanasopoulos et al, 2009; Hyndman et al, 2011) was to fit by OLS  yielding reconciled forecasts:
  	\begin{equation*}
  	\tilde{\bm{y}}={\bm S}({\bm S}'{\bm S})^{-1}{\bm S}'{\hat{\bm{y}}}
  	\end{equation*}
  \end{itemize}
\end{frame}
\begin{frame}{Generalisation}
  \begin{itemize}  
  \item Wherever we can use OLS we can use GLS
  \begin{equation*}
  \tilde{\bm{y}}={\bm S}({\bm S}'{\bm W}^{-1}{\bm S})^{-1}{\bm S}'{\bm W}^{-1}{\hat{\bm{y}}}
  \end{equation*}
  \item Diagonal ${\bm W}$ considered by Athanasopoulos et al (2017)
  \item MinT approach (Wickremasuriya et al, 2018) use a ${\bm W}$ that is an estimate of the {\em in-sample} forecast error covariance matrix.
  \end{itemize}
\end{frame}
%  \begin{frame}{Reconciliation in two steps}
%  	\begin{itemize}
%  		\item Many reconciliation methods involve two steps
%  		\begin{enumerate}
%  			\item Pre-multiply $\hat{\bm y}$ by a $m\times n$ matrix $\bm G$ to obtain {\bf bottom} level series ${\bm b}={\bm G}{\hat{\bm y}}$ 
%  			\item Pre-multiply ${\bm b}$ by a $n\times m$ matrix $\bm S$ to obtain ${\tilde{\bm y}}$, i.e. $\tilde{\bm y}={\bm S}{{\bm b}}$
%  		\end{enumerate}
%
%  	    \item Choice of ${\bm G}$ defines reconciliation method, e.g. OLS: ${\bm G}=\left(\bm{S}'\bm{S}\right)^{-1}{\bm S'}$ and Bottom Up: ${\bm G}=\left(\bm{0}_{m\times n-m}~\bm{I}_{m\times m}\right)$ 
%    \end{itemize}
%  \end{frame}
\begin{frame}
\centering
\includegraphics[height=5cm]{Figs/Plato.jpeg}\\
\textgreek{AGEWMTETRHTOS MHDEIS EISITW}\\
Those without knowledge of geometry may not enter.
\end{frame}

  \begin{frame}{Coherent Subspace}
    \begin{definition} 
	  The {\bf coherent subspace} is the linear subspace spanned by the columns of ${\bm S}$, i.e. $\mathfrak{s}=\mbox{sp}({\bm S})$
    \end{definition}
    Instead of using bottom-level series a different combination of $m$ {\bf basis series} could be used (e.g. top and $m-1$ bottom).\\
    Although ${\bm S}$ would be different $\mathfrak{s}$ would be the same.
  \end{frame}
  \begin{frame}{Coherent Point Forecast}
  \begin{definition} 
  	A {\bf coherent point forecast} is any forecast lying in the linear subspace $\mathfrak{s}$
  \end{definition}
  \end{frame}
  \begin{frame}{Reconciled Point Forecast}
  Let $\hat{\bm y}\in\mathbb{R}^n$ be an incoherent forecast and $g(.)$ be a function $g:\mathbb{R}^n\rightarrow\mathbb{R}^m$.
  \begin{definition} 
	A {\bf point forecast} $\tilde{\bm y}$ is reconciled with respect to $g(.)$ iff 
	\begin{equation*}
    \tilde{\bm y}={\bm S}g(\hat{\bm y})
	\end{equation*}
  \end{definition}
  when $g(.)$ is linear it is easier to write $\tilde{\bm y}={\bm S}{\bm G}\hat{\bm y}$
  \end{frame}
  \begin{frame}{Geometry}
  \vspace{-2.3cm}
  \centering 
  \input{Figs/orth_pointforerec_schematic}
  \end{frame}
%  \begin{frame}{Special Case: Projection}
%  \begin{itemize}
%   \item An important special case is where ${\bm S}{\bm G}$ is a projection.
%  \begin{itemize}
%  	\item ${\bm S}{\bm G}$ is symmetric
%  	\item ${\bm S}{\bm G}$ is idempotent
%  \end{itemize}
%  \item Let ${\bm v}\in\mathfrak{s}$ 
%    \begin{itemize}
%    	\item ${\bm S}{\bm G}{\bm v}$ will also lie in $\mathfrak{s}$.
%    	\item ${\bm S}{\bm G}{\bm v}={\bm v}$ only when ${\bm S}{\bm G}$ is a projection.
%    \end{itemize}
%  \end{itemize}
%  \end{frame}
%
%  \begin{frame}{Geometry: Oblique Projection}
%  	\vspace{-0.9cm}
%  	\centering
%  	\input{Figs/pointforerec_schematic}
%  \end{frame}
%  \begin{frame}{Projections preserve unbiasedness}
%	Let $\hat{\bm y}_{t+h|t}$ be an unbiased forecast that is $E_{1:t}(\hat{\bm y}_{t+h|t})={\bm \mu_{t+h|t}}$ where ${\bm \mu_{t+h|t}}=E(\bm{y}_{t+h}\mid\bm{y}_{1},\dots,\bm{y}_{t})$
%	\begin{theorem}
%	  The reconciled forecast $\tilde{\bm y}_{t+h|t}={\bm S}{\bm G}\hat{\bm y}_{t+h|t}$ will also be unbiased iff ${\bm S}{\bm G}$ is a projection.
%    \end{theorem}
%    Previously, this was often stated as an assumption that ${\bm S}{\bm G}{\bm S}={\bm S}$.    
%  \end{frame}
%  \begin{frame}{Proof}
%  Very easy proof
%  \begin{align*}
%  E_{1:t}(\tilde{\bm{y}}_{t+h|t})
%  &= E_{1:t}(\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t})\\
%  &= \bm{S}\bm{G}E_{1:t}(\hat{\bm{y}}_{t+h|t})\\
%  &= \bm{S}\bm{G}\bm{\mu}_{t+h|t}\\
%  &= \bm{\mu}_{t+h|t}
%  \end{align*}
%  The last equality does not hold for ${\bm G}$ in general but does hold when ${\bm S}{\bm G}$ is a projection.
%  \end{frame}
  \begin{frame}{Why reconciliation works}
  \begin{itemize}
  	\item The realised observation always lies on $\mathfrak{s}$.
  	\item Orthogonal projections always get us `closer' to all points in $\mathfrak{s}$ including the actual realisation.
  	\item Ergo reconciliation reduces the error and not just in expectation. 
  	\item What about the MinT approach? 
  \end{itemize}
  \end{frame}
  \begin{frame}{Finding a direction}
  \begin{itemize}
  \item Consider the covariance matrix of ${\bm y}-\hat{\bm{y}}$.
   \item This can be estimated using in-sample forecast errors.
  \item This provides information about the likely direction of an error.
  \item Projecting along this direction is more likely to result in a reconciled forecast closer to the target.
  \end{itemize}  
\end{frame}
  \begin{frame}{Animation}
  content...
  \end{frame}
  \begin{frame}{Geometry: Oblique Projection}
   	\vspace{-0.9cm}
   	\centering
   	\input{Figs/pointforerec_schematic}
  \end{frame}
  \begin{frame}{Is this overkill?}
  \begin{itemize}
  	\item Is this geometric interpretation really necessary?
  	\item When we generalise to probabilistic forecasts the regression interpretation does not really fit.
  	\item Geometric ideas can easily be generalised. 
  \end{itemize}
  \end{frame}
  \section{Probabilistic Reconciliation}
  \begin{frame}{Coherent Probabilistic Forecast}
    Let $(\mathbb{R}^m,\mathcal{F}_{\mathbb{R}^m},\nu)$ and $(\mathfrak{s},\mathcal{F}_{\mathfrak{s}},\mu)$ be probability triples on $m$-dimensional space and the coherent subspace respectively.
    \begin{definition}
      The probability measure $\nu$ is coherent if
      \begin{equation*}
      \nu(\mathcal{B})=\mu(s(\mathcal{B}))\quad\forall\mathcal{B}\in \mathcal{F}_{\mathbb{R}_m}
      \end{equation*} 
    \end{definition}
    where $s(\mathcal{B})$ is the image of $\mathcal{B}$ under premultiplication by ${\bm S}$
  \end{frame}
  \begin{frame}{Reconciled Probabilistic Forecast}
  	Let $g:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a linear function.  Then 
  	\begin{definition}
  	The probability triple $\left(\mathfrak{s},\mathcal{F}_{\mathfrak{s}},\tilde{\nu}\right)$ reconciles the probability triple $\left(\mathbb{R}^n,\mathcal{F}_{\mathbb{R}^n},\hat{\nu}\right)$ with with respect to $g$ iff
  	\begin{equation*}
  	\tilde{\nu}(s(\mathcal{B}))=\nu(\mathcal{B})=\hat{\nu}(g^{-1}(\mathcal{B}))\quad\forall \mathcal{B}\in\mathcal{F}_{\mathbb{R}_m}
  	\end{equation*}
  	\end{definition}
    where $g^{-1}$ is the pre-image of $g$.
  \end{frame}
  \begin{frame}{Geometry}
  	\centering
  	\input{Figs/probforerec_schematic}
  \end{frame}
  \begin{frame}{Analytically}
  	If we have an unreconciled density the reconciled density can be obtained by linear transformations and marginalisation.
  	\begin{align*}
  	\mbox{Pr}(\tilde{\bm{b}}\in \mathcal{B})&=\mbox{Pr}(\hat{\bm{y}}\in g^{-1}(\mathcal{B}))\\
  	&=\int\limits_{g^{-1}(\mathcal{B})}f(\hat{\bm{y}})d\hat{\bm{y}}\\
  	&=\int\limits_{\mathcal{B}}\int f(\bm{S}\tilde{\bm{b}}+\bm{R}\tilde{\bm{a}})|\left(\bm{S}~\bm{R}\right)|d\tilde{\bm{a}}d\tilde{\bm{b}}
  	\end{align*}
  \end{frame}
  \begin{frame}{Elliptical distributions}
  	Let the true predictive densities be elliptical.
  	\begin{theorem}
  		There exists a function $g(.)$ such that the true predictive distribution can be recovered by linear reconciliation as long as the unreconciled probabilistic forecast comes from the correct elliptical class.
  	\end{theorem}
    This follows from the closure property of elliptical distributions under affine transformations and marginalisation.  Conditions are also derived for when this $g(.)$ is also a projection.
  \end{frame}
  \begin{frame}{With a sample}
  	\begin{itemize}
  		\item Often densities are unavailable but we can simulate a sample from the predictive distribution.
  		\item Suppose $\bm{\hat{y}}^{[1]},\ldots,\bm{\hat{y}}^{[J]}$ is a sample from the unreconciled probabilistic forecast.
  		\item Then setting $\tilde{\bm y}^{[j]}=s\circ g(\hat{\bm y}^{[j]})=\bm{S}\bm{G}\hat{\bm y}^{[j]}$ produces a sample from the reconciled distribution with respect to $g$.
  	\end{itemize}
  \end{frame}
  \section{Scoring}
  \begin{frame}{Univariate v Multivariate Scores}
    Scoring rules can be used to evaluate probabilistic forecasts
    \begin{itemize}
    	\item Univariate
    	\begin{itemize}
    		\item Log Score
    		\item Continuous Rank Probability Score
    	\end{itemize}
    	\item Multivariate
        \begin{itemize}
	      \item Log Score
	      \item Energy Score
        \end{itemize}
    \end{itemize}
    These may be computed using densities or a sample.
  \end{frame}
  \begin{frame}{Approaches}
  	\begin{enumerate}
  		\item Use a summary of all univariate scores.
  		\item Make comparisons on the joint distribution of bottom level series only.
        \item Make comparisons using the full joint distribution
  \end{enumerate}
  There are pitfalls to the third approach.
  \end{frame}
  \begin{frame}{Coherent v Incoherent}
	When comparing reconciled and unreconciled probabilistic forecasts on the basis of log score
	\begin{theorem}
		Let $f(\bm{y})$ be the true predictive density (on $\mathfrak{s}$) and $LS$ be the (negatively-oriented) log score.  Then there exists an unreconciled density  $\hat{f}(\bm{y})$ on $\mathbb{R}^n$ such that
		\begin{equation*}
		E_{\bm y}\left[LS(\hat{f},\bm{y})\right]<E_{\bm y}\left[LS(f,\bm{y})\right]
		\end{equation*}
	\end{theorem}
    The log score is not proper {\bf in this context}.
  \end{frame}
  \begin{frame}{Reconciled v Reconciled}
	\begin{itemize}
		\item For two reconciled probabilistic forecasts log score can be used.
		\item Comparisons can be made on the basis of bottom level series (or any basis series).
		\item By the definition of coherence $\log (f({\bm b}))=\log (f({\bm S}{\bm b}){\bm J})$
		\item The Jacobian does not affect the ordering of log score.
	\end{itemize}
  \end{frame}
  \begin{frame}{Energy  score}
  	\begin{itemize}
  		\item Using bottom level series only is a bad idea for energy score.
  		\item Energy score is invariant to orthogonal transformation but not affine transformations.
  		\item Since ${\bm S}$ is not a rotation the ranking of different methods based on the full hierarchy may differ from the ranking based on bottom level series only.
  	\end{itemize}
  \end{frame}
   \begin{frame}{Simulations}
   	\begin{itemize}
   		\item The main takeaway messages are:
   		    \begin{itemize}
   		    	\item Reconciliation is better than no reconciliation.
   		    	\item Bottom up does not do well.
   		    	\item OLS (an orthogonal projection) does poorly.
   		    	\item MinT (an oblique projection) does best.
   		    \end{itemize} 
   	\end{itemize}
   \end{frame}
   \begin{frame}{Looking ahead}
     \begin{itemize}
     	\item The optimal feasible reconciliation method remains an open question even for elliptical distributions.
     	\begin{itemize}
     		\item It is likely to depend on the specific score used.
     	\end{itemize}
        \item Are non-linear reconciliation methods worthwhile?
        \item How should probabilistic reconciliation work for non-elliptical distributions.
        \item Further development of multivariate scoring rules.
     \end{itemize}	
   \end{frame}
\end{document}