\documentclass{beamer}
\usetheme{Warsaw}
\usepackage{amsmath,amsthm, bm, tikz,qtree}
\usetikzlibrary{arrows,positioning,shapes,fit,calc}
\usepackage[greek,english]{babel}

\title{Probabilistic Forecast Reconciliation}
\date{November 14, 2018}
\author[GPAH]{Puwasala Gamakumara, Anastasios Panagiotelis, George Athanasopoulos and Rob Hyndman}
\begin{document}
  \begin{frame}
    \maketitle
  \end{frame}
  \section{Motivation}
  \begin{frame}{Motivating Examples}
  \begin{itemize}
  	\item Multiple time series $\rightarrow$ some series are aggregates of others.
  	\item Gross Domestic Product
  	\pause
  	\begin{itemize}
  		\item An aggregate of consumption, investment, government spending and trade balance
  		\item Further breakdown, e.g. consumption of food, rent, etc.
  	\end{itemize}
    \pause
    \item Wind power
    \begin{itemize}
    	\item Forecasts required at a daily and hourly resolution.
    	\item Daily series is aggregate of 24 hourly series.  
    \end{itemize}
    \pause
    \item Potentially need forecasts of all time series.
  \end{itemize}
  \end{frame}
  \begin{frame}{Incoherent Forecasts}
  \begin{itemize}
    \item Potential approaches
    \pause
  	\begin{itemize}
  		\item Multivariate models
  		\pause
  		\item Univariate models
  		\pause
  		\item Judgemental forecasts
  	\end{itemize}
  \pause
  \item Forecasts do not respect aggregation structure \pause({\bf Incoherent})\pause
  \item Outcome {\em does} respect aggregation  structure \pause ({\bf Coherent})\pause
  \item Motivation is aggregation but can be generalised to any linear constraints.
  \end{itemize}
  \end{frame}
  \begin{frame}{Reconciliation}
  \begin{itemize}
  	\item Begin with a vector of {\em base} forecasts that are incoherent.
  	\pause
  	\item Adjust these {\bf ex post} to make them coherent.
  	\pause
  	\item There are good solutions for point forecasting that:
  	\pause
  	\begin{itemize}
  		\item Guarantee coherent forecasts.
  		\item Improve forecast accuracy overall.
  	\end{itemize}
  	\pause
  	\item Generalisation to probabilistic forecasts is our contribution.
  	\pause
  	\item Getting there necessitates a rethink of the existing point forecasting literature. 
  \end{itemize}
  \end{frame}
%  \begin{frame}{What we do NOT do}
%  \begin{itemize}
%  	\item All information is contained in the most disaggregate series.
%  	\item In principle using the correct multivariate model for the most disaggregate series and aggregating them should work.
%  	\item Disaggregate series are: 
%  	\begin{itemize}
%  		\item Very noisy
%  		\item High-dimensional
%  		\item Prone to model misspecification
%  	\end{itemize} 
%  \end{itemize}
%  \end{frame}
  \section{Point Forecast Reconciliation}
 \begin{frame}{A simple hierarchy}
  	Consider a hierarchy given by
  	\begin{figure}
	    \begin{center}
		\leaf{\alt<2>{\color{blue}AA}{AA}} \leaf{\alt<2>{\color{blue}AB}{AB}} 
		\branch{2}{A}
		\leaf{\alt<2>{\color{blue}BA}{BA}} \leaf{\alt<2>{\color{blue}BB}{BB}}
		\branch{2}{B}
		\branch{2}{Tot}
		\qobitree
	    \end{center}
    \end{figure}
    \begin{itemize}
    	\item Let $n$ be the number of series, $\bm{y}_t$ be an n-vector of all series.
    	\only<2>{\item Let {\color{blue}$m$} be the number of bottom level series and {\color{blue}$\bm{b}_t$} be an {\color{blue}$m$}-vector of the bottom level series.}
    \end{itemize}    
\end{frame}  
\begin{frame}{The ${\bm S}$ matrix}
  Coherence holds when
  \begin{equation*}
  {\bm y}={\bm S}{\bm b}
  \end{equation*} 
    The $n\times m$ matrix ${\bm S}$ defines the aggregation constraints, e.g.
    \begin{equation*}
    {\bm S}=\begin{pmatrix} 1 &1 &1 &1 \\1 &1 &0 &0 \\0 &0 &1 &1 \\ &{\bm I_{4\times 4}}
    \end{pmatrix}  
    \end{equation*}    
\end{frame}
\begin{frame}{As a regression model}
  \begin{itemize}
  	\item Cast the problem as a regression model with base forecasts $\hat{\bm{y}}_{T+h}$ as the ``dependent variable'' and ${\bm S}$ as the ``design matrix''. 
  	\begin{equation*}
  	\hat{\bm{y}}_{T+h}={\bm{S}}{\bm\beta}_{T+h}+{\bm e}_{T+h}
  	\end{equation*}
  	\item Initial approach (Athanasopoulos et al, 2009; Hyndman et al, 2011) was to fit by OLS  yielding reconciled forecasts:
  	\begin{equation*}
  	\tilde{\bm{y}}_{T+h}={\bm S}({\bm S}'{\bm S})^{-1}{\bm S}'{\hat{\bm{y}}}_{T+h}
  	\end{equation*}
  \end{itemize}
\end{frame}
\begin{frame}{Generalisation}
  \begin{itemize}  
  \item Wherever we can use OLS we can use GLS
  \begin{equation*}
  \tilde{\bm{y}}_{T+h}={\bm S}({\bm S}'{\bm W}^{-1}{\bm S})^{-1}{\bm S}'{\bm W}^{-1}{\hat{\bm{y}}}_{T+h}
  \end{equation*}
  \item Diagonal ${\bm W}$ considered by Athanasopoulos et al (2017)
  \item MinT approach (Wickremasuriya et al, 2018) use a ${\bm W}$ that is an estimate of the {\em in-sample} forecast error covariance matrix.
  \end{itemize}
\end{frame}
%  \begin{frame}{Reconciliation in two steps}
%  	\begin{itemize}
%  		\item Many reconciliation methods involve two steps
%  		\begin{enumerate}
%  			\item Pre-multiply $\hat{\bm y}$ by a $m\times n$ matrix $\bm G$ to obtain {\bf bottom} level series ${\bm b}={\bm G}{\hat{\bm y}}$ 
%  			\item Pre-multiply ${\bm b}$ by a $n\times m$ matrix $\bm S$ to obtain ${\tilde{\bm y}}$, i.e. $\tilde{\bm y}={\bm S}{{\bm b}}$
%  		\end{enumerate}
%
%  	    \item Choice of ${\bm G}$ defines reconciliation method, e.g. OLS: ${\bm G}=\left(\bm{S}'\bm{S}\right)^{-1}{\bm S'}$ and Bottom Up: ${\bm G}=\left(\bm{0}_{m\times n-m}~\bm{I}_{m\times m}\right)$ 
%    \end{itemize}
%  \end{frame}
\begin{frame}
\centering
\includegraphics[height=5cm]{Figs/Plato.jpeg}\\
\textgreek{AGEWMTETRHTOS MHDEIS EISITW}\\
Those without knowledge of geometry may not enter.
\end{frame}

  \begin{frame}{Coherent Subspace}
    \begin{definition} 
	  The {\bf coherent subspace} is the $m$-dimensional linear subspace of $\mathbb{R}^n$ spanned by the columns of ${\bm S}$, i.e. $\mathfrak{s}=\mbox{sp}({\bm S})$
    \end{definition}
    Instead of using bottom-level series a different combination of $m$ {\bf basis series} could be used (e.g. top and $m-1$ bottom).\\
    Although ${\bm S}$ would be different $\mathfrak{s}$ would be the same.
  \end{frame}
  \begin{frame}{Coherent Point Forecast}
  \begin{definition} 
  	A {\bf coherent point forecast} is any forecast lying in the linear subspace $\mathfrak{s}$
  \end{definition}
  \end{frame}
  \begin{frame}{Reconciled Point Forecast}
  Let $\hat{\bm y}\in\mathbb{R}^n$ be an incoherent forecast and $g(.)$ be a function $g:\mathbb{R}^n\rightarrow\mathbb{R}^m$.
  \begin{definition} 
	A {\bf point forecast} $\tilde{\bm y}$ is reconciled with respect to $g(.)$ iff 
	\begin{equation*}
    \tilde{\bm y}={\bm S}g(\hat{\bm y})
	\end{equation*}
  \end{definition}
  when $g(.)$ is linear it is easier to write $\tilde{\bm y}={\bm S}{\bm G}\hat{\bm y}$
  \end{frame}
  \begin{frame}{Geometry}
  \vspace{-2.3cm}
  \centering 
  \input{Figs/orth_pointforerec_schematic}
  \end{frame}
%  \begin{frame}{Special Case: Projection}
%  \begin{itemize}
%   \item An important special case is where ${\bm S}{\bm G}$ is a projection.
%  \begin{itemize}
%  	\item ${\bm S}{\bm G}$ is symmetric
%  	\item ${\bm S}{\bm G}$ is idempotent
%  \end{itemize}
%  \item Let ${\bm v}\in\mathfrak{s}$ 
%    \begin{itemize}
%    	\item ${\bm S}{\bm G}{\bm v}$ will also lie in $\mathfrak{s}$.
%    	\item ${\bm S}{\bm G}{\bm v}={\bm v}$ only when ${\bm S}{\bm G}$ is a projection.
%    \end{itemize}
%  \end{itemize}
%  \end{frame}
%
%  \begin{frame}{Geometry: Oblique Projection}
%  	\vspace{-0.9cm}
%  	\centering
%  	\input{Figs/pointforerec_schematic}
%  \end{frame}
%  \begin{frame}{Projections preserve unbiasedness}
%	Let $\hat{\bm y}_{t+h|t}$ be an unbiased forecast that is $E_{1:t}(\hat{\bm y}_{t+h|t})={\bm \mu_{t+h|t}}$ where ${\bm \mu_{t+h|t}}=E(\bm{y}_{t+h}\mid\bm{y}_{1},\dots,\bm{y}_{t})$
%	\begin{theorem}
%	  The reconciled forecast $\tilde{\bm y}_{t+h|t}={\bm S}{\bm G}\hat{\bm y}_{t+h|t}$ will also be unbiased iff ${\bm S}{\bm G}$ is a projection.
%    \end{theorem}
%    Previously, this was often stated as an assumption that ${\bm S}{\bm G}{\bm S}={\bm S}$.    
%  \end{frame}
%  \begin{frame}{Proof}
%  Very easy proof
%  \begin{align*}
%  E_{1:t}(\tilde{\bm{y}}_{t+h|t})
%  &= E_{1:t}(\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t})\\
%  &= \bm{S}\bm{G}E_{1:t}(\hat{\bm{y}}_{t+h|t})\\
%  &= \bm{S}\bm{G}\bm{\mu}_{t+h|t}\\
%  &= \bm{\mu}_{t+h|t}
%  \end{align*}
%  The last equality does not hold for ${\bm G}$ in general but does hold when ${\bm S}{\bm G}$ is a projection.
%  \end{frame}
  \begin{frame}{Why reconciliation works}
  \begin{itemize}
  	\item The realised observation always lies on $\mathfrak{s}$.
  	\item Orthogonal projections always get us `closer' to all points in $\mathfrak{s}$ including the actual realisation.
  	\item Ergo reconciliation reduces the error and not just in expectation. 
  	\pause
  	\item What about the MinT approach? 
  \end{itemize}
  \end{frame}
  \begin{frame}{Finding a direction}
  \begin{itemize}
  \item Consider the covariance matrix of ${\bm y}_{T+h}-\hat{\bm{y}}_{T+h}$.
   \pause
   \item This can be estimated using in-sample forecast errors.
   \begin{equation*}
   \bm{W}=\sum\limits_{t=1}^T ({\bm y}_{t}-\hat{\bm{y}}_{t})({\bm y}_{t}-\hat{\bm{y}}_{t})'
   \end{equation*}
  \pause
  \item This provides information about the likely direction of an error.
  \pause
  \item Projecting along this direction is more likely to result in a reconciled forecast that is closer to the target.
  \end{itemize}  
\end{frame}
\begin{frame}{In-Sample errors}
\vspace{-0.9cm}
\centering
\input{Figs/insample}
\end{frame}
\begin{frame}{Most likely direction}
\vspace{-0.9cm}
\centering
\input{Figs/insampledir}
\end{frame}
  \begin{frame}{Geometry: Oblique Projection}
\vspace{-0.9cm}
\centering
\input{Figs/oblique_justification1}
\end{frame}
  \begin{frame}{Geometry: Oblique Projection}
\vspace{-0.9cm}
\centering
\input{Figs/oblique_justification2}
\end{frame}
  \begin{frame}{Geometry: Oblique Projection}
   	\vspace{-0.9cm}
   	\centering
   	\input{Figs/pointforerec_schematic}
  \end{frame}
  \begin{frame}{Is this overkill?}
  \begin{itemize}
  	\item Is this geometric interpretation really necessary?
  	\pause
  	\item When we generalise to probabilistic forecasts the regression interpretation does not really fit.
  	\pause
  	\item Geometric ideas can easily be generalised. 
  \end{itemize}
  \end{frame}
  \section{Probabilistic Reconciliation}
  \begin{frame}{Coherent Probabilistic Forecast}
    Let $(\mathbb{R}^m,\mathcal{F}_{\mathbb{R}^m},\nu)$ and $(\mathfrak{s},\mathcal{F}_{\mathfrak{s}},\mu)$ be probability triples on $m$-dimensional space and the coherent subspace respectively.
    \begin{definition}
      The probability measure $\mu$ is coherent if
      \begin{equation*}
      \nu(\mathcal{B})=\mu(s(\mathcal{B}))\quad\forall\mathcal{B}\in \mathcal{F}_{\mathbb{R}_m}
      \end{equation*} 
    \end{definition}
    where $s(\mathcal{B})$ is the image of $\mathcal{B}$ under premultiplication by ${\bm S}$
  \end{frame}
  \begin{frame}{Reconciled Probabilistic Forecast}
  	Let $g:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a function.  Then 
  	\begin{definition}
  	The probability triple $\left(\mathfrak{s},\mathcal{F}_{\mathfrak{s}},\tilde{\nu}\right)$ reconciles the probability triple $\left(\mathbb{R}^n,\mathcal{F}_{\mathbb{R}^n},\hat{\nu}\right)$ with with respect to $g$ iff
  	\begin{equation*}
  	\tilde{\nu}(s(\mathcal{B}))=\nu(\mathcal{B})=\hat{\nu}(g^{-1}(\mathcal{B}))\quad\forall \mathcal{B}\in\mathcal{F}_{\mathbb{R}_m}
  	\end{equation*}
  	\end{definition}
    where $g^{-1}$ is the pre-image of $g$.
  \end{frame}
  \begin{frame}{Geometry}
  	\centering
  	\input{Figs/probforerec_schematic}
  \end{frame}
  \begin{frame}{Analytically}
  	If we have an unreconciled density the reconciled density can be obtained by linear transformations and marginalisation.
  	\begin{align*}
  	\mbox{Pr}(\tilde{\bm{b}}\in \mathcal{B})&=\mbox{Pr}(\hat{\bm{y}}\in g^{-1}(\mathcal{B}))\\
  	&=\int\limits_{g^{-1}(\mathcal{B})}f(\hat{\bm{y}})d\hat{\bm{y}}\\
  	&=\int\limits_{\mathcal{B}}\int f(\bm{S}\tilde{\bm{b}}+\bm{R}\tilde{\bm{a}})|\left(\bm{S}~\bm{R}\right)|d\tilde{\bm{a}}d\tilde{\bm{b}}
  	\end{align*}
  \end{frame}
  \begin{frame}{Elliptical distributions}
  	Consider case where the base and true predictive distributions are elliptical.
  	\begin{theorem}
  		There exists a matrix $\bm{G}$ such that the true predictive distribution can be recovered by linear reconciliation.
  	\end{theorem}
    This follows from the closure property of elliptical distributions under affine transformations and marginalisation.  
  \end{frame}
  \begin{frame}{With a sample}
  	\begin{itemize}
  		\item Often densities are unavailable but we can simulate a sample from the predictive distribution.
  		\pause
  		\item Suppose $\bm{\hat{y}}_{T+h}^{[1]},\ldots,\bm{\hat{y}}_{T+h}^{[J]}$ is a sample from the unreconciled probabilistic forecast.
  		\pause
  		\item Then setting $\tilde{\bm y}_{T+h}^{[j]}=\bm{S}\bm{G}\hat{\bm y}_{T+h}^{[j]}$ produces a sample from the reconciled distribution with respect to $g$.
  	\end{itemize}
  \end{frame}
  \section{Scoring}
  \begin{frame}{Multivariate Scores}
    \begin{itemize}
    \item Scoring rules can be used to evaluate probabilistic forecasts
        \pause
        \begin{itemize}
	      \item Log Score
	      \item Energy Score
	      \item Variogram score
        \end{itemize}
    \pause
    \item We may want to compare
    \begin{itemize}
    	\item Coherent v Incoherent
    	\item Coherent v Coherent
    \end{itemize}
    \end{itemize}
  \end{frame}
  \begin{frame}{Coherent v Incoherent}
	When using log score
	\begin{theorem}
		Let $f(\bm{y})$ be the true predictive density (on $\mathfrak{s}$) and $LS$ be the (negatively-oriented) log score.  Then there exists an unreconciled density  $\hat{f}(\bm{y})$ on $\mathbb{R}^n$ such that
		\begin{equation*}
		E_{\bm y}\left[LS(\hat{f},\bm{y})\right]<E_{\bm y}\left[LS(f,\bm{y})\right]
		\end{equation*}
	\end{theorem}
    The log score is not proper {\bf in this context}.
  \end{frame}
  \begin{frame}{Simulations}
   	\begin{itemize}
   		\item We have run lots of simulations.
   		\pause
   		\item The main takeaway messages are:
   		    \pause
   		    \begin{itemize}
   		    	\item Reconciliation is better than no reconciliation.
   		    	\pause
   		    	\item Bottom up does not do well.
   		    	\pause
   		    	\item OLS (an orthogonal projection) does reasonably well.
   		    	\pause
   		    	\item MinT (an oblique projection) does best.
   		    \end{itemize} 
   	\end{itemize}
   \end{frame}
   \begin{frame}{Looking ahead}
     \begin{itemize}
     	\item The optimal feasible reconciliation method remains an open question even for elliptical distributions.
     	\pause
     	\begin{itemize}
     		\item It is likely to depend on the specific score used.
     	\end{itemize}
        \pause
        \item How should probabilistic reconciliation work for non-elliptical distributions.
        \pause
        \item Are non-linear reconciliation methods worthwhile?
        \pause
        \item Further development of multivariate scoring rules.
     \end{itemize}	
   \end{frame}
\end{document}