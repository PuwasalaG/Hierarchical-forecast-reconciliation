\documentclass{beamer}
\usetheme{Warsaw}
\usepackage{amsmath,amsthm, bm, tikz,qtree}
\usetikzlibrary{arrows,positioning,shapes,fit,calc}
\title{Probabilistic Forecast Reconciliation}
\date{August 15, 2018}
\author[GPAH]{Puwasala Gamakumara, Anastasios Panagiotelis, George Athanasopoulos and Rob Hyndman}
\begin{document}
  \begin{frame}
    \maketitle
  \end{frame}
  \section{Concepts}
  \begin{frame}{Motivating Examples}
  \begin{itemize}
  	\item Gross Domestic Product
  	\begin{itemize}
  		\item An aggregate of consumption, investment, government spending and trade balance
  		\item Further breakdown, e.g. consumption of food, rent, etc.
  	\end{itemize}
    \item Retail sales
    \begin{itemize}
    	\item Aggregate of different product category sales
    	\item Further breakdown into individual stock keeping units.  
    \end{itemize}
    \item Potentially need forecasts of all time series.
  \end{itemize}
  \end{frame}
  \begin{frame}{Incoherent Forecasts}
  \begin{itemize}
    \item Potential approaches
  	\begin{itemize}
  		\item Univariate models
  		\item Multivariate models
  		\item Judgemental forecasts
  	\end{itemize}
  \item Forecasts may not respect aggregation structure. ({\bf Incoherent})
  \item Outcome {\em will} respect aggregation  structure. ({\bf Coherent})
  \item Motivation is aggregation but can be generalised to any linear constraints.
  \end{itemize}
  \end{frame}
  \begin{frame}{Reconciliation}
  \begin{itemize}
  	\item Begin with a vector of forecasts that are incoherent.
  	\item Adjust these {\bf ex post} to make them coherent.
  	\item There are good solutions for point forecasting that:
  	\begin{itemize}
  		\item Guarantee coherent forecasts.
  		\item Improve forecast accuracy overall.
  	\end{itemize}
  	\item Generalisation to probabilistic forecasts is our contribution.
  	\item Getting there necessitates a rethink of the existing point forecasting literature. 
  \end{itemize}
  \end{frame}
 
  \begin{frame}{Hierarchical and Grouped Time Series}
  	\begin{itemize}
  	  \item Collections of time series are often characterised by aggregation constraints
  	  \begin{itemize}
  	    \item Cross-Sectionally
  		\item Temporally
  	  \end{itemize}
      \item {\bf Coherent} forecasts respect such constraints. 
      \item Independently produced forecasts are generally incoherent.
  \end{itemize}
\begin{figure}
	\begin{center}
		\leaf{AA} \leaf{AB} 
		\branch{2}{A}
		\leaf{BA} \leaf{BB}
		\branch{2}{B}
		\branch{2}{Tot}
		\qobitree
	\end{center}
\end{figure}
\end{frame}  
  \begin{frame}{Forecast Reconciliation}
  	\begin{itemize}
  	  \item Forecast reconciliation involves
  	  \begin{enumerate}
  	  	\item Producing incoherent {\bf base} forecasts for all series in an $n\times 1$ vector $\hat{\bm y}$
        \item Adjusting base forecasts to obtain coherent {\bf reconciled} forecasts in an $n\times 1$ vector $\tilde{\bm y}$
  	  \end{enumerate} 
      \item Why do we care?
        \begin{enumerate}
        	\item Aligned decision making.
        	\item Improved forecast accuracy
        \end{enumerate}
  	\end{itemize}
  \end{frame}
  \begin{frame}{Reconciliation in two steps}
  	\begin{itemize}
  		\item Many reconciliation methods involve two steps
  		\begin{enumerate}
  			\item Pre-multiply $\hat{\bm y}$ by a $m\times n$ matrix $\bm G$ to obtain {\bf bottom} level series ${\bm b}={\bm G}{\hat{\bm y}}$ 
  			\item Pre-multiply ${\bm b}$ by a $n\times m$ matrix $\bm S$ to obtain ${\tilde{\bm y}}$, i.e. $\tilde{\bm y}={\bm S}{{\bm b}}$
  		\end{enumerate}
  	    \item The matrix ${\bm S}$ defines the aggregation constraints, e.g.
  	    \begin{equation*}
  	    {\bm S}=\begin{pmatrix} 1 &1 &1 &1 \\1 &1 &0 &0 \\0 &0 &1 &1 \\ &{\bm I_{4\times 4}}
  	    \end{pmatrix}
  	    \end{equation*}
  	    \item Choice of ${\bm G}$ defines reconciliation method, e.g. OLS: ${\bm G}=\left(\bm{S}'\bm{S}\right)^{-1}{\bm S'}$ and Bottom Up: ${\bm G}=\left(\bm{0}_{m\times n-m}~\bm{I}_{m\times m}\right)$ 
    \end{itemize}
  \end{frame}
  \begin{frame}{Coherent Subspace}
    \begin{definition} 
	  The {\bf coherent subspace} is the linear subspace spanned by the columns of ${\bm S}$, i.e. $\mathfrak{s}=\mbox{sp}({\bm S})$
    \end{definition}
    Instead of using bottom-level series a different combination of $m$ {\bf basis series} could be used (e.g. top and $m-1$ bottom).  Although ${\bm S}$ would be different $\mathfrak{s}$ would be the same.
  \end{frame}
  \begin{frame}{Coherent Point Forecast}
  \begin{definition} 
  	A {\bf coherent point forecast} is any forecast lying in the linear subspace $\mathfrak{s}$
  \end{definition}
  \end{frame}
  \begin{frame}{Reconciled Point Forecast}
  Let $\hat{\bm y}\in\mathbb{R}^n$ be an incoherent forecast and $g(.)$ be a function $g:\mathbb{R}^n\rightarrow\mathbb{R}^m$.
  \begin{definition} 
	A {\bf point forecast} $\tilde{\bm y}$ is reconciled with respect to $g(.)$ iff 
	\begin{equation*}
    \tilde{\bm y}={\bm S}g(\hat{\bm y})
	\end{equation*}
  \end{definition}
  when $g(.)$ is linear it is easier to write $\tilde{\bm y}={\bm S}{\bm G}\hat{\bm y}$
  \end{frame}
  \begin{frame}{Special Case: Projection}
  \begin{itemize}
   \item An important special case is where ${\bm S}{\bm G}$ is a projection.
  \begin{itemize}
  	\item ${\bm S}{\bm G}$ is symmetric
  	\item ${\bm S}{\bm G}$ is idempotent
  \end{itemize}
  \item Let ${\bm v}\in\mathfrak{s}$ 
    \begin{itemize}
    	\item ${\bm S}{\bm G}{\bm v}$ will also lie in $\mathfrak{s}$.
    	\item ${\bm S}{\bm G}{\bm v}={\bm v}$ only when ${\bm S}{\bm G}$ is a projection.
    \end{itemize}
  \end{itemize}
  \end{frame}
  \begin{frame}{Geometry}
  	\vspace{-2.3cm}
  	\centering
  	\input{Figs/orth_pointforerec_schematic}
  \end{frame}
  \begin{frame}{Geometry: Oblique Projection}
  	\vspace{-0.9cm}
  	\centering
  	\input{Figs/pointforerec_schematic}
  \end{frame}
  \begin{frame}{Projections preserve unbiasedness}
	Let $\hat{\bm y}_{t+h|t}$ be an unbiased forecast that is $E_{1:t}(\hat{\bm y}_{t+h|t})={\bm \mu_{t+h|t}}$ where ${\bm \mu_{t+h|t}}=E(\bm{y}_{t+h}\mid\bm{y}_{1},\dots,\bm{y}_{t})$
	\begin{theorem}
	  The reconciled forecast $\tilde{\bm y}_{t+h|t}={\bm S}{\bm G}\hat{\bm y}_{t+h|t}$ will also be unbiased iff ${\bm S}{\bm G}$ is a projection.
    \end{theorem}
    Previously, this was often stated as an assumption that ${\bm S}{\bm G}{\bm S}={\bm S}$.    
  \end{frame}
  \begin{frame}{Proof}
  Very easy proof
  \begin{align*}
  E_{1:t}(\tilde{\bm{y}}_{t+h|t})
  &= E_{1:t}(\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t})\\
  &= \bm{S}\bm{G}E_{1:t}(\hat{\bm{y}}_{t+h|t})\\
  &= \bm{S}\bm{G}\bm{\mu}_{t+h|t}\\
  &= \bm{\mu}_{t+h|t}
  \end{align*}
  The last equality does not hold for ${\bm G}$ in general but does hold when ${\bm S}{\bm G}$ is a projection.
  \end{frame}
  \begin{frame}{Why reconciliation works}
  \begin{itemize}
  	\item The realised observation always lies on $\mathfrak{s}$.
  	\item Orthogonal projections always get us `closer' to all points in $\mathfrak{s}$ including the actual realisation.
  	\item Ergo reconciliation reduces the error and not just in expectation. 
  	\item Oblique projections have the same property for non-Euclidean distance. 
  \end{itemize}
  \end{frame}
  \begin{frame}{Geometry}
  \vspace{-2.3cm}
  \centering 
  \input{Figs/orth_pointforerec_schematic}
  \end{frame}
  \section{Probabilistic Reconciliation}
  \begin{frame}{Coherent Probabilistic Forecast}
    Let $(\mathbb{R}^m,\mathcal{F}_{\mathbb{R}^m},\nu)$ and $(\mathfrak{s},\mathcal{F}_{\mathfrak{s}},\mu)$ be probability triples on $m$-dimensional space and the coherent subspace respectively.
    \begin{definition}
      The probability measure $\nu$ is coherent if
      \begin{equation*}
      \nu(\mathcal{B})=\mu(s(\mathcal{B}))\quad\forall\mathcal{B}\in \mathcal{F}_{\mathbb{R}_m}
      \end{equation*} 
    \end{definition}
    where $s(\mathcal{B})$ is the image of $\mathcal{B}$ under premultiplication by ${\bm S}$
  \end{frame}
  \begin{frame}{Reconciled Probabilistic Forecast}
  	Let $g:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a linear function.  Then 
  	\begin{definition}
  	The probability triple $\left(\mathfrak{s},\mathcal{F}_{\mathfrak{s}},\tilde{\nu}\right)$ reconciles the probability triple $\left(\mathbb{R}^n,\mathcal{F}_{\mathbb{R}^n},\hat{\nu}\right)$ with with respect to $g$ iff
  	\begin{equation*}
  	\tilde{\nu}(s(\mathcal{B}))=\nu(\mathcal{B})=\hat{\nu}(g^{-1}(\mathcal{B}))\quad\forall \mathcal{B}\in\mathcal{F}_{\mathbb{R}_m}
  	\end{equation*}
  	\end{definition}
    where $g^{-1}$ is the pre-image of $g$.
  \end{frame}
  \begin{frame}{Geometry}
  	\centering
  	\input{Figs/probforerec_schematic}
  \end{frame}
  \begin{frame}{Analytically}
  	If we have an unreconciled density the reconciled density can be obtained by linear transformations and marginalisation.
  	\begin{align*}
  	\mbox{Pr}(\tilde{\bm{b}}\in \mathcal{B})&=\mbox{Pr}(\hat{\bm{y}}\in g^{-1}(\mathcal{B}))\\
  	&=\int\limits_{g^{-1}(\mathcal{B})}f(\hat{\bm{y}})d\hat{\bm{y}}\\
  	&=\int\limits_{\mathcal{B}}\int f(\bm{S}\tilde{\bm{b}}+\bm{R}\tilde{\bm{a}})|\left(\bm{S}~\bm{R}\right)|d\tilde{\bm{a}}d\tilde{\bm{b}}
  	\end{align*}
  \end{frame}
  \begin{frame}{Elliptical distributions}
  	Let the true predictive densities be elliptical.
  	\begin{theorem}
  		There exists a function $g(.)$ such that the true predictive distribution can be recovered by linear reconciliation as long as the unreconciled probabilistic forecast comes from the correct elliptical class.
  	\end{theorem}
    This follows from the closure property of elliptical distributions under affine transformations and marginalisation.  Conditions are also derived for when this $g(.)$ is also a projection.
  \end{frame}
  \begin{frame}{With a sample}
  	\begin{itemize}
  		\item Often densities are unavailable but we can simulate a sample from the predictive distribution.
  		\item Suppose $\bm{\hat{y}}^{[1]},\ldots,\bm{\hat{y}}^{[J]}$ is a sample from the unreconciled probabilistic forecast.
  		\item Then setting $\tilde{\bm y}^{[j]}=s\circ g(\hat{\bm y}^{[j]})=\bm{S}\bm{G}\hat{\bm y}^{[j]}$ produces a sample from the reconciled distribution with respect to $g$.
  	\end{itemize}
  \end{frame}
  \section{Scoring}
  \begin{frame}{Univariate v Multivariate Scores}
    Scoring rules can be used to evaluate probabilistic forecasts
    \begin{itemize}
    	\item Univariate
    	\begin{itemize}
    		\item Log Score
    		\item Continuous Rank Probability Score
    	\end{itemize}
    	\item Multivariate
        \begin{itemize}
	      \item Log Score
	      \item Energy Score
        \end{itemize}
    \end{itemize}
    These may be computed using densities or a sample.
  \end{frame}
  \begin{frame}{Approaches}
  	\begin{enumerate}
  		\item Use a summary of all univariate scores.
  		\item Make comparisons on the joint distribution of bottom level series only.
        \item Make comparisons using the full joint distribution
  \end{enumerate}
  There are pitfalls to the third approach.
  \end{frame}
  \begin{frame}{Coherent v Incoherent}
	When comparing reconciled and unreconciled probabilistic forecasts on the basis of log score
	\begin{theorem}
		Let $f(\bm{y})$ be the true predictive density (on $\mathfrak{s}$) and $LS$ be the (negatively-oriented) log score.  Then there exists an unreconciled density  $\hat{f}(\bm{y})$ on $\mathbb{R}^n$ such that
		\begin{equation*}
		E_{\bm y}\left[LS(\hat{f},\bm{y})\right]<E_{\bm y}\left[LS(f,\bm{y})\right]
		\end{equation*}
	\end{theorem}
    The log score is not proper {\bf in this context}.
  \end{frame}
  \begin{frame}{Reconciled v Reconciled}
	\begin{itemize}
		\item For two reconciled probabilistic forecasts log score can be used.
		\item Comparisons can be made on the basis of bottom level series (or any basis series).
		\item By the definition of coherence $\log (f({\bm b}))=\log (f({\bm S}{\bm b}){\bm J})$
		\item The Jacobian does not affect the ordering of log score.
	\end{itemize}
  \end{frame}
  \begin{frame}{Energy  score}
  	\begin{itemize}
  		\item Using bottom level series only is a bad idea for energy score.
  		\item Energy score is invariant to orthogonal transformation but not affine transformations.
  		\item Since ${\bm S}$ is not a rotation the ranking of different methods based on the full hierarchy may differ from the ranking based on bottom level series only.
  	\end{itemize}
  \end{frame}
   \begin{frame}{Simulations}
   	\begin{itemize}
   		\item The main takeaway messages are:
   		    \begin{itemize}
   		    	\item Reconciliation is better than no reconciliation.
   		    	\item Bottom up does not do well.
   		    	\item OLS (an orthogonal projection) does poorly.
   		    	\item MinT (an oblique projection) does best.
   		    \end{itemize} 
   	\end{itemize}
   \end{frame}
   \begin{frame}{Looking ahead}
     \begin{itemize}
     	\item The optimal feasible reconciliation method remains an open question even for elliptical distributions.
     	\begin{itemize}
     		\item It is likely to depend on the specific score used.
     	\end{itemize}
        \item Are non-linear reconciliation methods worthwhile?
        \item How should probabilistic reconciliation work for non-elliptical distributions.
        \item Further development of multivariate scoring rules.
     \end{itemize}	
   \end{frame}
\end{document}