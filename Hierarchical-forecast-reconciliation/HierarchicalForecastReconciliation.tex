\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL
\usepackage{floatpag}
\usepackage{lipsum}
\usepackage{amssymb, qtree, bm, multirow, textcmds, siunitx,paralist}
\usepackage{mathrsfs, float, booktabs,todonotes,amsthm, xcolor,sidenotes, caption, subcaption}
\usepackage[export]{adjustbox}

\usepackage[bb=boondox]{mathalfa}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,shapes,fit,calc}
\usepackage{amsfonts}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1in}%
\addtolength{\topmargin}{-.8in}%

\DeclareMathOperator*{\argmin}{arg\,min}
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\def\mathbi#1{\textit{ #1}}
\def\mathB#1{\textbf{ #1}}
\def\E{\text{E}}
\def\var{\text{Var}}

\def\PQ{\begin{pmatrix}\bm{G}\\[-0.2cm]\bm{H}\end{pmatrix}}
\def\bt{\begin{pmatrix}\tilde{\bm{b}}\\[-0.2cm]\tilde{\bm{a}}\end{pmatrix}}

%\theoremstyle{theo}
\newtheorem{theo}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{property}
\newtheorem{property}{Property}[section]


\begin{document}
	
	
	%\bibliographystyle{natbib}
	
	\def\spacingset#1{\renewcommand{\baselinestretch}%
		{#1}\small\normalsize} \spacingset{1}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\if1\blind
	{
		\title{\bf Forecast Reconciliation: A geometric view with new insights on bias correction.}
		\author{Anastasios Panagiotelis\thanks{
				The authors gratefully acknowledge the support of Australian Research Council Grant DP140103220.  We also thank Professor Mervyn Silvapulle for valuable comments.}\hspace{.2cm}\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3145, Australia.\\
			Email: Anastasios.Panagiotelis@monash.edu \\
			and \\
			Puwasala Gamakumara\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3800, Australia.\\
			Email: Puwasala.Gamakumara@monash.edu \\
			and \\
			George Athanasopoulos\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3145, Australia.\\
			Email: George.Athanasopoulos@monash.edu \\
			and \\
			Rob J Hyndman\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3800, Australia.\\
			Email: Rob.Hyndman@monash.edu \\}
		\maketitle
	} \fi
	
	\if0\blind
	{
		\bigskip
		\bigskip
		\bigskip
		\begin{center}
			{\LARGE\bf Hierarchical Forecasts Reconciliation}
		\end{center}
		\medskip
	} \fi
	
	\bigskip
	
	
	\begin{abstract} TBC
		%  Forecast reconciliation involves adjusting forecasts to ensure coherence with aggregation constraints. We extend this concept from point forecasts to probabilistic forecasts by redefining forecast reconciliation in terms of linear functions in general, and projections more specifically. New theorems establish that the true predictive distribution can be recovered in the elliptical case by linear reconciliation, and general conditions are derived for when this is a projection. A geometric interpretation is also used to prove two new theoretical results for point forecasting; that reconciliation via projection both preserves unbiasedness and dominates unreconciled forecasts in a mean squared error sense. Strategies for forecast evaluation based on scoring rules are discussed, and it is shown that the popular log score is an improper scoring rule with respect to the class of unreconciled forecasts when the true predictive distribution coheres with aggregation constraints. Finally, evidence from a simulation study shows that reconciliation based on an oblique projection, derived from the MinT method of \citet{Wickramasuriya2017} for point forecasting, outperforms both reconciled and unreconciled alternatives.
	\end{abstract}
	
%	\noindent%
%	{\it Keywords:}  Forecast Reconciliation, Projections, Elliptical Distributions, Scoring Rules, High-dimensional Time Series.
%	\vfill
	
	\newpage
	\spacingset{1.45} % DON'T change the spacing!
	
	\section{Introduction}\label{sec:intro}
	
	The past decade has seen rapid development in methodologies for forecasting time series that follow a hierarchical aggregation structure.  Of particular prominence have been {\em forecast reconciliation} methods involving two steps; first separate forecasts are produced for all series, then these are adjusted ex post to ensure coherence with aggregation constraints.  Forecast reconciliation has mostly been formulated using a regression model, see \cite{Hyndman2011} and \cite{WicEtAl2019} for examples.  This setup can be counter-intuitive since a vector comprised of forecasts from different time series models is also assumed to be the dependent variable in a regression model.  In this paper, we eschew a regression interpretation in favour of a novel, geometric understanding of forecast reconciliation.  This allows us to develop novel proofs and a clearer understanding of the interplay between forecast bias and reconciliation methods.
	
	Multivariate time series following an aggregation structure arise in many disciplines such as manufacturing, engineering, marketing and medicine\todo{include references}. Forecasts of these series should adhere to aggregation constraints to ensure aligned decision making. Earlier studies achieved this by only forecasting a single level of the hierarchy and then either aggregating in a bottom-up fashion \citep{Dunn1976} or disaggregating in a top-down fashion \citep{Gross1990, Athanasopoulos2009}.  For reviews of these approaches including a discussion of their advantages and disadvantages see \citet{Schwarzkopf1988, Kahn1998, Lapide1998, Fliedner2001}.
	
	In contrast to these methods, \cite{Hyndman2011} proposed forecasting all series in the hierarchy, referring to these as {\em base} forecasts.  Since base forecasts were produced independently they were not guaranteed to adhere to aggregation constraints and could thus be improved via further adjustment.  A framework was proposed whereby the base forecasts were assumed to follow a regression model.  The predicted values from this model were guaranteed to adhere to the linear constraints by construction and could thus be used as a new set of forecasts.  This approach and later modifications have subsequently been shown to outperform bottom-up and top-down approaches in a variety of empirical settings\todo{references}.
	
	Some theoretical insight into the performance of forecast reconciliation methods has been provided by \cite{VanErven2015a} and \cite{WicEtAl2019}.  Both papers provide a proof that reconciliation is guaranteed to improve base forecasts.  The latter paper also proposes a particular version of reconciliation known as the Minimum Trace (MinT) method.  This is optimal in the sense of minimising the trace of reconciled forecast error covariance matrix under the assumption that base forecasts are unbiased.
	
	Our main contribution is to propose a geometric interpretation of the entire hierarchical forecasting problem.  In this setting, we show that reconciled forecasts will have a number of attractive properties when they are obtained via projections.  We believe that this is clearer and more intuitive than explanations based on regression modelling, notwithstanding the fact that regression based-methods themselves are indeed projections.  As such, this paper is in part a review of existing results cast in a new light, but one that we believe to be warranted as forecast reconciliation methodologies have become more popular.  In addition, we also propose three major and novel results.
	
	First, our approach makes it clear that the defining characteristic of so-called {\em hierarchical time series} is not aggregation but linear constraints.  As a result forecast reconciliation can be applied in contexts where there are no clear candidates of {\em bottom level} series, an insight that is not apparent when the problem is viewed through the lens of regression modelling.  Second, we provide a new proof that reconciled forecasts dominate unreconciled forecasts which makes explicit the link between a reconciliation method and a loss function.  We believe that this link is lacking in previous work that attempts to establish similar results, in particular \cite{VanErven2015a} and \cite{WicEtAl2019}.  Futhermore, unlike \cite{VanErven2015a} and \cite{WicEtAl2019} our proof does not require an assumption about convexity.  Third, we revisit the issue of bias.  We prove that reconciliation using certain projection matrices guarantees unbiased reconciled forecasts as long as base forecasts are also unbiased.  A natural question that arises is what to do in the case of biased reconciled forecasts.  Rather than addressing this issue by considering matrices that are not projections, we propose to bias-correct before reconciliation.  This is evaluated in an extensive empirical study where we find that even when bias correction fails, the extent of the problem is mitigated by reconciling forecasts.
	
	
	The remainder of this paper is structured as follows. Section~\ref{sec:CoheForecasts} deals with the concept of coherence and defines so called hierarchical time series in a way that does not depend on any notion of bottom-level series.  Section~\ref{sec:Reconciliation} defines forecast reconciliation in terms of projections and includes a proof that reconciled forecasts dominate base forecasts with respect to a specific loss function. In Section~\ref{sec:BiasInRecon} we prove the unbiasedness preserving property of reconciliation via certain projection matrices and propose methods for bias correction. In Section \ref{sec:EmpStudy} we conduct an extensive empirical application to domestic tourism flow in Australia with two objectives; first to demonstrate the theorems discussed in~\ref{sec:Reconciliation}, second to evaluate the methods for bias correction discussed in~\ref{sec:BiasInRecon}.  Section~\ref{sec:conclusions} concludes with some discussion and thoughts on the  future of research in forecast reconciliation.
\section{Coherent forecasts}\label{sec:CoheForecasts}
	
	\subsection{Notation and preliminaries}\label{sec:notation}
	
    We briefly define the concept of a \emph{hierarchical time series} in a fashion similar to \cite{AthEtAl2019_MacroBook}, \cite{FPP2018} and others, before elaborating on some of the limitations of this understanding.  A \emph{hierarchical time series} is a collection of $n$ variables indexed by time, where some variables are aggregates of other variables. We let $\bm{y}_t \in \mathbb{R}^n$ be a vector comprising observations of all variables in the hierarchy at time $t$. The \emph{bottom-level series} are defined as those $m$ variables that cannot be formed as aggregates of other variables; we let $\bm{b}_t \in \mathbb{R}^m$ be a vector comprised of observations of all bottom-level series at time $t$.  The hierarchical structure of the data implies that the following holds for all $t$

    \begin{equation}
    \bm{y}_t = \bm{Sb}_t,
    \end{equation}
    where $\bm{S}$ is an $n \times m$ constant matrix that encodes the aggregation constraints.
	
		\begin{figure}[H]
			\begin{center}
				\leaf{AA} \leaf{AB}
				\branch{2}{A}
				\leaf{BA} \leaf{BB}
				\branch{2}{B}
				\branch{2}{Tot}
				\qobitree
			\end{center}
			\caption{An example of a two level hierarchical structure.}\label{fig:basichier}
		\end{figure}
	
	To clarify these concepts consider the example of the hierarchy in Figure \ref{fig:basichier}.  For this hierarchy, $n=7$, $\bm{y}_t = [y_{Tot,t},y_{A,t}, y_{B,t},y_{C,t},y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}]'$, $m=4$, $\bm{b}_t = [y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}]'$ and
	\[
	\bm{S} = \begin{pmatrix}
	1 & 1 & 1 & 1  \\
	1 & 1 & 0 & 0 \\
	0 & 0 & 1 & 1 \\
	& \multicolumn{2}{c}{\bm{I}_4} &
	\end{pmatrix},
	\]
	where $\bm{I}_4$ is the $4\times 4$ identity matrix.
	
	While such a definition is completely serviceable, it obscures the full generality of the literature on so-called hierarchical time series.  In fact, concepts such as coherence and reconciliation, defined in full below, only require the data to have two important characteristics; the first is that they are multivariate, the second is that they adhere to linear constraints.
	
	\subsection{Coherence}\label{sec:cohpointf}
	
	The property that data adhere to some linear constraints is referred to as {\em coherence}.  We now provide definitions aimed at providing geometric intuition of hierarchical time series.
	
	 \begin{definition}[Coherent subspace]\label{def:cohspace}
	 	The $m$-dimensional linear subspace $\mathfrak{s}\subset \mathbb{R}^n$ for which a set of linear constraints holds for all $\bm{y}\in\mathfrak{s}$ is defined as the \emph{coherent subspace}.
	 \end{definition}
     To further illustrate, Figure \ref{fig:3D_hierarchy} depicts the most simple three variable hierarchy where $y_{Tot,t}=y_{A,t}+y_{B,t}$.  The coherent subspace is depicted as a grey $2$-dimensional plane within $3$-dimensional space, i.e. $m=2$ and $n=3$.  It is worth noting that the coherent subspace is spanned by the columns of $\bm{S}$, i.e.\ $\mathfrak{s}=\text{span}(\bm{S})$.  In Figure~\ref{fig:3D_hierarchy}, these columns are $\vec{s}_1=(1,1,0)'$ and $\vec{s}_2=(1,0,1)'$.  However, it is equally important to recognise that the hierarchy could also have been defined in terms of $y_{Tot,t}$ and $y_{A,t}$ rather than the bottom-level series, $y_{A,t}$ and $y_{B,t}$. In this case the corresponding `$\bm{S}$ matrix' would have columns $(1,0,1)'$ and $(0,1,-1)'$.  However, while there are multiple ways to define an $\bm{S}$ matrix, in all cases the columns will span the same coherent subspace, which is unique.

     \begin{figure}[H]
     	\centering
     	\vspace{-0.9cm}
     	\small
     	\resizebox{\linewidth}{!}{
     		\input{Figs/3D_hierarchy}
     	}
     	\caption{Depiction of a three dimensional hierarchy with $y_{\text{Tot}} = y_{\text{A}} + y_{\text{B}}$. The gray coloured two dimensional plane depicts the coherent subspace $\mathfrak{s}$ where $\vec{s}_1 = (1,1,0)'$ and $\vec{s}_2 = (1, 0, 1)'$ are basis vectors that span $\mathfrak{s}$. The red points in $\mathfrak{s}$ represent realisations or coherent forecasts}\label{fig:3D_hierarchy}
     \end{figure}

	 \begin{definition}[Hierarchical Time Series]\label{def:cohspace}
	 	A hierarchical time series is an $n$-dimensional multivariate time series such that all observed values $\bm{y}_1,\ldots,\bm{y}_T$ and all future values $\bm{y}_{T+1},\bm{y}_{T+2},\ldots$  lie in the coherent subspace, i.e., $\bm{y}_t\in\mathfrak{s}\quad\forall t$.
	 \end{definition}

	
	 Despite the common use of the term {\em hierarchical time series}, it should be clear from the definition that the data need not necessarily follow a hierarchy.  Also notable by its absence in the above definition is any reference to {\em aggregation}. In some ways, terms such as {\em hierarchical} and {\em aggregation} can be misleading since the literature has covered instances that cannot be depicted in a similar fashion to Figure~\ref{fig:basichier} and/or do not involve aggregation. Examples include, temporal hierarchies which involve grouped structures \citep[see][]{AthEtAl2017}, overlapping temporal hierarchies \citep[see][]{JeoEtAl2019}, applications for which the difference rather than the aggregate is of interest \cite[see][]{LiTan2019}, or structures that involve both cross-sectional and temporal dimensions referred to as cross-temporal structures \citep[see][]{KouAth2019}. Finally, although Definition~\ref{def:cohspace} makes reference to time series, this definition can be easily generalised to any vector-valued data for which some linear constraints are known to hold for all realisations.
	
%	It will sometimes be useful to think of pre-multiplication by $\bm{S}$ as a mapping from $\mathbb{R}^m$ to $\mathbb{R}^n$, in which case we use the notation $s(.)$. Although the codomain of $s(.)$ is $\mathbb{R}^n$, its image is the coherent space $\mathfrak{s}$ as depicted in Figure \ref{fig2}.
%	
%	\begin{figure}[H]
%		\begin{center}
%			\begin{tikzpicture}[
%			>=stealth,
%			bullet/.style={
%				fill=black,
%				circle,
%				minimum width=1.5cm,
%				inner sep=0pt
%			},
%			projection/.style={
%				->,
%				thick,
%				label,
%				shorten <=2pt,
%				shorten >=2pt
%			},
%			every fit/.style={
%				ellipse,
%				draw,
%				inner sep=0pt
%			}
%			]
%			\node at (2,3) {$s$};
%			\node at (0,5) {$\mathbb{R}^m$(domain of $s$)};
%			\node at (4,5) {$\mathbb{R}^n$(codomain of $s$)};
%			\node at (4.7,2.0) {$\mathfrak{s}$(image of $s$)};
%			%\node[bullet,label=below:$f(x)$] at (4,2.5){};
%			\draw (0,2.5) ellipse (1.02cm and 2.2cm);
%			\draw (4,2.5) ellipse (1.02cm and 2.2cm);
%			\draw (4,2.5) ellipse (0.51cm and 1.1cm);
%			\draw[projection, label=below:$f$] (0.3,2.5) -- (3.8,2.5) ;
%			\end{tikzpicture}
%		\end{center}
%		\caption{The domain, codomain and image of the mapping $s$.}\label{fig2}
%	\end{figure}
	
	\begin{definition}[Coherent Point Forecasts]\label{def:cohpoint}
        Let $\breve{\bm{y}}_{t+h|t} \in \mathbb{R}^n$ be a vector of point forecasts of all series in the hierarchy where the subscript $t+h|h$ implies that the forecast is made as time $t$ for a period $h$ steps into the future. Then $\breve{\bm{y}}_{t+h|t}$ is \emph{coherent} if $\breve{\bm{y}}_{t+h|t} \in \mathfrak{s}$.
	\end{definition}

    Without any loss of generality, the above definition could also be applied to prediction for multivariate data in general, rather than just forecasting of time series.


    Much of the early literature that dealt with the problem of forecasting hierarchical time series \citep[see][and references therein]{Gross1990} produced forecasts at a single level of the hierarchy in the first stage. Subsequently forecasts for all series were recovered through aggregation, disaggregation according to historical or forecast proportions or some combination of both.  As such incoherent forecasts were not a problem in these earlier papers.

    Forecasting a single level of the hierarchy did not, however echo common practice within many industries.\todo{might need refs}  In many organisations different departments or `silos' each produced their own forecasts, often with their own information sets and judgemental adjustments.  This approach does have several advantages over only forecasting a single level.  First, there is no loss of information since all levels and series are modelled.  Second, modelling higher level series often identifies features such as trend and seasonality that cannot be detected in noisy disaggregate data.  However, when forecasts are produced independently at all levels, forecasts are likely to be incoherent.\footnote{There are some special cases of using simple approaches such as na\"{i}ve, which extrapolate the coherent nature of the data to the forecasts.} This problem of incoherent forecasts cannot in general be solved by multivariate modelling either.  Instead, the solution is to make an ex post adjustment that ensures coherence, a process known as {\em forecast reconciliation}
	
	
	
\section{Forecast reconciliation}\label{sec:Reconciliation}
	
	The concept of forecast reconciliation is predicated on there being an $n$-vector of forecasts that are incoherent.  We call these {\em base forecasts} and denote them as $\hat{\bm{y}}_{t+h|t}$. The subscript $t+h/t$ implies that the forecast is made at time $t$ for a period $h$ steps into the future.  In the sequel, this subscript will be dropped at times for ease of exposition.  In the most general terms, reconciliation can be defined as follows.
	
	\begin{definition}[Reconciled forecasts]\label{def:reconpoint}
		Let $\psi$ be a mapping, $\psi:\mathbb{R}^n\rightarrow\mathfrak{s}$.  The point forecast $\tilde{\bm{y}}_{t+h|t}=\psi\left(\hat{\bm{y}}_{t+h|t}\right)$ ``reconciles'' a base forecast $\hat{\bm{y}}_{t+h|t}$ with respect to the mapping $\psi(.)$
	\end{definition}
	
	All reconciliation methods that we are aware of consider a linear mapping for $\psi$, which involves pre-multiplying base forecasts by an $n\times n$ matrix that has $\mathfrak{s}$ as its image.  One way to achieve this is with a matrix $\bm{SG}$, where $\bm{G}$ is an $m\times n$ matrix  (some authors use $\bm{P}$ in place of $\bm{G}$).  This facilitates an interpretation of reconciliation as a two-step process. In the first step, base forecasts $\hat{\bm{y}}_{t+h|t}$ are combined to form a new set of bottom-level forecasts. In the second step, these are mapped to a full vector of coherent forecasts via pre-multiplication by $\bm{S}$.
	
	Although pre-multiplying base forecasts by $\bm{SG}$ will result in coherent forecasts, a number of desirable properties arise when $\bm{SG}$ has the specific structure of a {\em projection} matrix onto $\mathfrak{s}$.  In general a projection matrix is defined via its idempotence property, i.e. $(\bm{SG})^2=\bm{SG}$.  However a much more important property of projection matrices, used in multiple instances below, is that any vector lying in the image of the projection will be mapped to itself by that projection \citep[see Lemma 2.4 in][for a proof]{rao1974}. In our context this implies that for any $\bm{v}\in\mathfrak{s}$, $\bm{SGv}=\bm{v}$.
	
	We begin by considering the special case of an orthogonal projection whereby $\bm{G}=\left(\bm{S}'\bm{S}\right)^{-1}\bm{S}'$.  This is equivalent to so called OLS reconciliation as introduced by \cite{Hyndman2011}.  We refrain from any discussion of regression models focusing instead on geometric interpretations.  However the connection between OLS and orthogonal projection should be clear, in the context of regression modelling predicted values from OLS are obtained via an orthogonal projection of the response onto the span of the regressors.
	
	\subsection{Orthogonal projection}\label{sec:orthogonal}
	
	In this section we discuss two sensible properties that can be achieved by reconciliation via orthogonal projection.
\begin{itemize}
  \item The  first is that reconciliation should adjust the base forecasts as little as possible, i.e. the base and reconciled forecasts should be `close'.
  \item The second is that reconciliation in some sense should improve forecast accuracy, or more loosely, that the reconciled forecast should be `closer' to the realised value targeted by the forecast.
\end{itemize}

	
	To address the first of these properties we make the concept of closeness more concrete, by considering the Euclidean distance between the base forecast $\hat{\bm{y}}$ and the reconciled forecast  $\tilde{\bm{y}}$.  A property of an orthogonal projection is that the distance between $\hat{\bm{y}}$ and $\tilde{\bm{y}}$ is minimal for over any possible $\tilde{\bm{y}}\in\mathfrak{s}$.  In this sense reconciliation via orthogonal projection leads to the smallest possible adjustments of the base forecasts.
	
	The property that reconciliation should improve forecasts was touched upon in Section 2.3 of \cite{WicEtAl2019}.  The discussion in that paper focuses on the case of MinT. Here we provide a new explicit proof of that result.  We do so first in the case of an orthogonal projection where the geometric intuition of the proof is clear and then generalise the result to reconciliation using any projection matrix in Section~\ref{sec:oblique}.
		
	Consider the Euclidean distance between a forecast and the target. This is equivalent to the root of the sum of squared forecast errors over the entire hierarchy. Let $\bm{y}_{t+h}$ be the realisation of the data generating process at time $t+h$. The following theorem shows that reconciliation never increases, and in most cases reduces, the sum of squared errors of point forecasts.
	
	
	\begin{theo}[Distance reducing property]\label{th:distred}
		If $\tilde{\bm{y}}_{t+h|t}=\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t}$, where $\bm{G}$ is such that $\bm{S}\bm{G}$ is an orthogonal (in the Euclidean sense) projection onto $\mathfrak{s}$ and let $\|\bm{v}\|$ be the $L_2$ norm (in the Euclidean sense) of vector $\bm{v}$ then:
		\begin{equation}
		\|(\tilde{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|\le\|(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|.
		\end{equation}
	\end{theo}
	\begin{proof}
		Since, $\bm{y}_{t+h|t},\tilde{\bm{y}}_{t+h|t}\in\mathfrak{s}$ and since the projection is orthogonal, by Pythagoras' theorem
		\begin{equation}
		\|(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|^2=\|(\tilde{\bm{y}}_{t+h|t}-\hat{\bm{y}}_{t+h|t})\|^2+\|(\tilde{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|^2.
		\end{equation}
		
		Since $\|(\tilde{\bm{y}}_{t+h|t}-\hat{\bm{y}}_{t+h|t})\|^2\ge 0$ this implies,
		\begin{equation}
		\|(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|^2\ge\|(\tilde{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|^2.
		\end{equation}
		with equality only holding when $\tilde{\bm{y}}_{t+h|t}=\hat{\bm{y}}_{t+h|t}$.  Taking the square root of both sides proves the desired result.
	\end{proof}
		
	The simple geometric intuition behind the proof is demonstrated in Figure \ref{fig:Schematic_OLSRecon}.  In this schematic, the coherent subspace is depicted as a black arrow.  The base forecast $\hat{\bm{y}}$ is shown as a blue dot.  Since $\hat{\bm{y}}$ is incoherent, $\hat{\bm{y}}_{t+h|t}\notin\mathfrak{s}$ and in this case the inequality is strict.  Reconciliation is an orthogonal projection from $\hat{\bm{y}}$ to the coherent subspace yielding the reconciled forecast $\tilde{\bm{y}}$ shown in red.  Finally, the target of the forecast $\bm{y}$ is displayed as a black point, and although its exact location is unknown to the forecaster, it is known that it will lie somewhere along the coherent subspace.
	
	\begin{figure}[H]
		\centering
		\vspace{-0.9cm}
		\tiny
		\resizebox{\linewidth}{!}{\input{Figs/orth_pointforerec_schematic}}
		\caption{Orthogonal projection of $\hat{\bm{y}}$ onto $\mathfrak{s}$ yielding the reconciled forecast $\tilde{\bm{y}}$}\label{fig:Schematic_OLSRecon}
		
	\end{figure}
	
	
	Figure \ref{fig:Schematic_OLSRecon} clearly shows that $\hat{\bm{y}}$, $\tilde{\bm{y}}$ and $\bm{y}$ form a right angled triangle with $\tilde{\bm{y}}$ at the right-angled vertex.  In this triangle the line between $\bm{y}$ and $\hat{\bm{y}}$ is the hypotenuse and therefore must be longer than the distance between $\bm{y}$ and $\tilde{\bm{y}}$.  As such reconciliation is guaranteed to reduce the squared error of the forecast.
	
    Theorem~\ref{th:distred} is in some ways more powerful than perhaps previously understood.  Crucially, the result is not a result that requires taking expectations.  This distance reducing property will hold for any realisation and any forecast and not just on average.  Nothing needs to be assumed about the statistical properties of the data generating process or the process by which forecasts are made.

    However, in other ways, Theorem~\ref{th:distred} is weaker than perhaps often understood. First, when improvements in forecast accuracy are discussed in the context of the theorem, this refers to a very specific measure of forecast accuracy.  In particular, this measure is the root of the sum of squared forecast errors of {\em all} variables in the hierarchy.  As such, while forecast improvement is guaranteed for the hierarchy overall, reconciliation can lead to less accurate forecasts for individual series. Second, although orthogonal projections are guaranteed to improve on base forecasts, they are not necessarily the projection that leads to the greatest improvement in forecast accuracy.  As such referring to reconciliation via orthogonal projections as `optimal' is somewhat misleading since it does not have the optimality properties of some oblique projections, in particular MinT. It is to oblique projections that we now turn our attention.
    	
	\subsection{Oblique Projections}\label{sec:oblique}
	
	One justification for using an orthogonal projection is that it leads to improved forecast accuracy in terms of the root of the sum of squared errors of {\em all} variables in the hierarchy.  A clear shortcoming of this measure of forecast accuracy is that forecast errors in all series should not necessarily be treated equally.  For example, in hierarchies, top-level series tend to have a much larger scale than bottom-level series.  Even when two series are on a similar scale, series that are more predictable or less variable will tend to be downweighted by simply aggregating squared errors.  An even more sophisticated understanding may take the correlation between series into account.  All of these considerations lead towards reconciliation of the form $\tilde{\bm{y}}=\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}\hat{\bm{y}}$, where $\bm{W}$ is a symmetric matrix.  Generally, it is assumed that $\bm{W}$ is invertible, otherwise a pseudo inverse can be used.
	
	It should be noted that  $\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}$ is an oblique, rather than an orthogonal projection matrix in the usual Euclidean geometry.  However this matrix can be considered to be an orthogonal projection for a different geometry defined by the norm\break $||\bm{v}||_{{\bm W}^{-1}}=\bm{v}'\bm{W}^{-1}\bm{v}$, referred to as the generalised Euclidean geometry with respect to $\bm{W}^{-1}$.  One way to understand this geometry is that it is the same as Euclidean geometry when all vectors are first transformed by pre-multiplying by $\bm{W}^{-1/2}$.  This leads to a transformed $\bm{S}$ matrix $\bm{S}^*=\bm{W}^{-1/2}\bm{S}$ and transformed $\hat{\bm{y}}$ and $\tilde{\bm{y}}$ vectors $\hat{\bm{y}}^*=\bm{W}^{-1/2}\hat{\bm{y}}$ and $\tilde{\bm{y}}^*=\bm{W}^{-1/2}\tilde{\bm{y}}$.  The transformed reconciled forecast results from an orthogonal projection in the transformed space since
	
	\begin{align}
	\tilde{\bm{y}}^*&=\bm{W}^{-1/2}\tilde{\bm{y}}\\&=\bm{W}^{-1/2}\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}\hat{\bm{y}}
	\\&=\bm{S}^*\left(\bm{S}^{*'}\bm{S}^*\right)^{-1}\bm{S}^{*'}\hat{\bm{y}^*}.
	\end{align}
	
Thinking of the problem in terms of a geometry defined by the norm $\bm{v}'\bm{W}^{-1}\bm{v}$ is also quite instructive when it comes to thinking about the connection between distances and loss functions. In the generalised Euclidean geometry, the distance between the reconciled forecast and the realisation is given by $(\hat{\bm{y}}-\bm{y})'\bm{W}^{-1}(\hat{\bm{y}}-\bm{y})$.  For diagonal $\bm{W}^{-1}$, this is equivalent to a weighted sum of squared error loss function and when $\bm{W}$ is a covariance matrix, this is equivalent to a Mahalanobis distance.  As such Theorem~\ref{th:distred} can easily be generalised as follows:
	
	\begin{theo}[General distance reducing property]\label{th:gdistred}
        If $\tilde{\bm{y}}_{t+h|t}=\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t}$, where $\bm{G}$ is such that $\bm{S}\bm{G}$ is an orthogonal (in the generalised Euclidean sense) projection onto $\mathfrak{s}$ then:
		\begin{equation}
		\|(\tilde{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|_{{\bm W}^{-1}}\le\|(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|_{{\bm W}^{-1}}.
		\end{equation}
	\end{theo}
	\begin{proof}
    The proof is identical to the proof for Theorem~\ref{th:distred} but relies on the generalised Pythagorean Theorem (applicable to Generalised Euclidean space) rather than the Pythagorean Theorem.
    \end{proof}
	
	The implication of Theorem~\ref{th:gdistred} is that if the objective function is some weighted sum of squared errors, or a Mahalanobis distance, then the projection matrix $\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}$ is guaranteed to improve forecast accuracy over base forecasts, for an appropriately selected $\bm{W}$.
	
	\todo{Rob can you please read to make sure you are happy with this.}Note here that we rely here on the generalised Pythagorean Theorem (which involves an equality).  In contrast, \cite{WicEtAl2019} follow \cite{VanErven2015a} in stating their result in terms of the Generalised Pythagorean Inequality.  The proof of \cite{WicEtAl2019} requires an assumption about convexity so that the angle between the base forecast and coherent subspace must be greater than 90 degrees.  The proof we have provided here requires no such assumption, since this may not hold for an arbitrary $\bm{W}$.  As such the statement from \cite{WicEtAl2019} that {\em``MinT reconciled forecasts are at least as good as the incoherent forecasts''} should be qualified; this is only true with respect to a loss function that depends on ${\bm W}$.  If Euclidean distance (or mean squared error) is used, there will be cases where the MinT estimator does not improve upon base forecasts.  This will be demonstrated using a real data set in the empirical study in section~\ref{sec:comparebase}.
	
	
	\subsection{MinT}
	
	While the properties discussed so far hold for any projection matrix, the MinT method of \cite{WicEtAl2019} has an additional optimality property.  \cite{WicEtAl2019} show that for unbiased base forecasts, the trace of the forecast error covariance matrix of reconciled forecasts is minimised by an oblique projection with a particular choice of $\bm{W}$.  This choice is that $\bm{W}$ should be the forecast error covariance matrix where errors come from using the base forecasts.  Although the base forecast error covariance matrix is unknown, it can be estimated using in-sample errors.
	
	
	Figure~\ref{fig:MinT_justification1} provides geometrical intuition into the MinT method.  Suppose the in-sample errors are given by the orange points.  They provide information on the most likely direction of large deviations from the coherent subspace.  This direction is denoted by $\bm{R}$.  Figure~\ref{fig:MinT_justification2} then shows a target value of $\bm{y}$, while the grey points indicate possible values for the base forecasts (the base forecasts are of course stochastic).  One possible value of the forecast is depicted in blue as $\hat{\bm{y}}$.  An oblique projection of the blue point back along the direction of $\bm{R}$ yields a reconciled forecast closer to the target, especially compared to an orthogonal projection shown in Figure \ref{fig:Orthogonal_projection_all_points}.  Figure \ref{fig:Oblique_projection_all_points} depicts an oblique projection along $\bm{R}$ for all the gray points, yielding reconciled forecasts tightly packed near the target $\bm{y}$.  In this sense, the oblique MinT projection minimises the forecast error variance of the reconciled forecasts. In contrast to the result in Theorem~\ref{th:gdistred}, this property is a statistical property in the sense that MinT is optimal in expectation.
	
%	\begin{figure}[H]
%		\centering
%		%\vspace{-0.9cm}
%		\small
%		\resizebox{\linewidth}{!}{
%			\input{Figs/insampledir}
%		}
%		\caption{A schematic to represent MinT reconciliation. Points in orange colour represent the insample errors. $\bm{R}$ shows the most likely direction of deviations from the coherent subspace. $\hat{\bm{y}}$ is projected onto $\mathfrak{s}$ along the the direction of $\bm{R}$.}\label{fig:MinT_justification1}
%	\end{figure}
%		
%	\begin{figure}[H]
%		\centering
%		%\vspace{-0.9cm}
%		\small
%		\resizebox{\linewidth}{!}{
%			\input{Figs/oblique_justification2}
%		}
%		\caption{A schematic to represent MinT reconciliation. Grey points indicate potential realisations of the base forecast while the blue dot indicates one such realisation. The black dot ${\bm y}$ denotes the (unknown) target of the forecast.}\label{fig:MinT_justification2}
%	\end{figure}
%	
%	\begin{figure}[H]
%		\centering
%		%\vspace{-0.9cm}
%		\small
%		\resizebox{\linewidth}{!}{
%			\includegraphics{Figs/OrthProj.pdf}}
%		\caption{All gray points are orthogonally projected onto the coherent subspace $\mathfrak{s}$. Reconciled forecasts marked in red dots are widely spread about the target $\bm{y}$.}\label{fig:Orthogonal_projection_all_points}
%	\end{figure}
%	
%	
%	\begin{figure}[H]
%	\centering
%	%\vspace{-0.9cm}
%	\small
%	\resizebox{\linewidth}{!}{
%		\includegraphics{Figs/ObliqueProjection.pdf}}
%	\caption{All gray points are obliquely projected along the direction of $\bm{R}$. Reconciled points denoted in red dots are concentrated about the target $\bm{y}$.}\label{fig:Oblique_projection_all_points}
%	\end{figure}

%%%%%%%%%%%%%%%%%%%


%	\begin{figure}
%		\centering
%		\begin{subfigure}[t]{0.45\textwidth}
%			\centering
%			\includegraphics[width=\linewidth]{Figs/InsampDir_1.pdf}
%%			\label{fig:InsampDir_1}
%		\end{subfigure}
%		\hfill
%		\begin{subfigure}[t]{0.45\textwidth}
%			\centering
%			\includegraphics[width=\linewidth]{Figs/InsampDir_2.pdf}
%%			\label{fig:InsampDir_2}
%		\end{subfigure}
%		
%		\vspace{1cm}
%		
%		\begin{subfigure}[t]{0.45\textwidth}
%			\centering
%			\includegraphics[width=\linewidth]{Figs/OrthProj.pdf}
%%			\label{fig:OrthProj}
%		\end{subfigure}
%		\hfill
%		\begin{subfigure}[t]{0.45\textwidth}
%			\centering
%			\includegraphics[width=\linewidth]{Figs/ObliqProj.pdf}
%%			\label{fig:ObliqProj}
%		\end{subfigure}
%		
%%		\begin{subfigure}[t]{\textwidth}
%%			\centering
%%			\includegraphics[width=\linewidth]{example-image-c.pdf}
%%			\caption{Price regulation} \label{fig:timing3}
%%		\end{subfigure}
%		\caption{A schematic to represent orthogonal and oblique reconciliation.  Points in orange colour in top left figure represent the insample errors. $\bm{R}$ shows the most likely direction of deviations from the coherent subspace $\mathfrak{s}$. Grey points in top right figure indicate  potential realisations of the base forecast while the blue dot ${\hat{\bm y}}$ indicates one such realisation. The black dot ${\bm y}$ denotes the (unknown) target of the forecast. Bottom left figure shows the orthogonal projection of all potential realisations onto the coherent subspace while bottom right figure shows the oblique projection.} \label{fig:OthogonalVSOblique_projection}
%	\end{figure}

	\begin{figure}
	
	\begin{adjustbox}{minipage=\linewidth,scale=0.7}
	\begin{subfigure}[t]{0.45\textwidth}
%		\centering
		\input{Figs/InsampDir_1}
		%			\label{fig:InsampDir_1}
	\end{subfigure}
	\hspace{1cm}
	\begin{subfigure}[t]{0.45\textwidth}
%		\centering
		\input{Figs/InsampDir_2}
		%			\label{fig:InsampDir_2}
	\end{subfigure}
	
	\vfill
%	\vspace{0.3cm}
	
	\begin{subfigure}[t]{0.45\textwidth}
%		\centering
		\input{Figs/Orthproj}
		%			\label{fig:OrthProj}
	\end{subfigure}
	\hspace{1cm}
	\begin{subfigure}[t]{0.45\textwidth}
%		\centering
		\input{Figs/ObliqProj}
		%			\label{fig:ObliqProj}
	\end{subfigure}
	
	%		\begin{subfigure}[t]{\textwidth}
	%			\centering
	%			\includegraphics[width=\linewidth]{example-image-c.pdf}
	%			\caption{Price regulation} \label{fig:timing3}
	%		\end{subfigure}
	
	\end{adjustbox}
\caption{A schematic to represent orthogonal and oblique reconciliation.  Orange colour points in top left figure represent the insample errors. $\bm{R}$ shows the most likely direction of deviations from the coherent subspace $\mathfrak{s}$. Grey points in top right figure indicate  potential realisations of the base forecast while the blue dot ${\hat{\bm y}}$ indicates one such realisation. The black dot ${\bm y}$ denotes the (unknown) target of the forecast. Bottom left figure shows the orthogonal projection of all potential realisations onto the coherent subspace while bottom right figure shows the oblique projection.} \label{fig:OthogonalVSOblique_projection}
\end{figure}

\begin{figure}[!h]
    \centering
    \begin{subfigure}[!h]{0.48\textwidth}
        \includegraphics[width=\textwidth]{Figs/InsampDir_1_George.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[!h]{0.48\textwidth}
        \includegraphics[width=\textwidth]{Figs/InsampDir_2_George.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[!h]{0.48\textwidth}
        \includegraphics[width=\textwidth]{Figs/ObliqProj_George.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[!h]{0.48\textwidth}
        \includegraphics[width=\textwidth]{Figs/OrthProj_George.pdf}
        \caption{}
    \end{subfigure}
    \caption{A schematic to represent orthogonal and oblique reconciliation.  Orange colour points in top left figure represent the insample errors.$\bm{R}$ shows the most likely direction of deviations from the coherent subspace $\mathfrak{s}$. Grey points in top right figure indicate  potential realisations of the base forecast while the blue dot ${\hat{\bm y}}$ indicates one such realisation. The black dot ${\bm y}$ denotes the (unknown) target of the forecast. Bottom left figure shows the orthogonal projection of all potential realisations onto the coherent subspace while bottom right figure shows the oblique projection. } \label{fig:OthogonalVSOblique_projection_new}
\end{figure}

	
%		\begin{figure}[H]
%		\centering
%		%\vspace{-0.9cm}
%		\small
%		\resizebox{\linewidth}{!}{
%			\input{Figs/ObliqueProjection}
%		}
%		\caption{\textcolor{red}{write caption}}\label{fig:Oblique_projection}
%	\end{figure}
	
%\begin{figure}
%	\centering
%	\begin{subfigure}{0.47\textwidth}
%		\resizebox{\linewidth}{!}{
%			\input{Figs/Orth.proj.tex}}
%		\caption{bla bla}
%	\end{subfigure}
%	\begin{subfigure}{0.47\textwidth}
%		\resizebox{\linewidth}{!}{
%			\input{Figs/ObliqueProjection.tex}}
%			\caption{bla bla}
%	\end{subfigure}
%\end{figure}

	
	\section{Bias in forecast reconciliation}\label{sec:BiasInRecon}
	
	Before turning our attention to the issue of bias itself it is important to state a sensible property that any reconciliation method should have.  That is if base forecasts are already coherent then reconciliation should not change the forecast.  As stated in Section~\ref{sec:Reconciliation}, this property holds only when $\bm{SG}$ is a projection matrix.  As a corollary, reconciling using an arbitrary $\bm{G}$,  may in fact change an already coherent forecast.
	
	The property that projections map all vectors in the coherent subspace onto themselves is also useful in proving the unbiasedness preserving property of reconciliation of \cite{WicEtAl2019}.  Before restating this proof using a  clear geometric interpretation we discuss in a precise fashion what is meant by unbiasedness.
	
	Suppose that the target of a point forecast is $\bm{\mu}_{t+h|t}:=\E(\bm{y}_{t+h}\mid\bm{y}_{1},\dots,\bm{y}_{t})$ where the expectation is taken over the predictive density.  Our point forecast can be thought of as an estimate of this quantity. The forecast is random due to uncertainty in the training sample and it is with respect to this uncertainty that unbiasedness is referred to \todo{or do we want to define unbiasedness of a forecast as the expected value of the forecast equals realisation? George: are these not equivalent? Should be put both? Rob what do think?}.  More concretely, the point forecast will be unbiased if $\E_{1:t}(\hat{\bm{y}}_{t+h|t})=\bm{\mu}_{t+h|t}$, where the subscript $1:t$ denotes an expectation taken over the training sample.
	
	\begin{theo}[Unbiasedness preserving property]
		For unbiased $\hat{\bm{y}}_{t+h|t}$, the reconciled point forecast is also an unbiased prediction as long as $\bm{SG}$ is a projection onto $\mathfrak{s}$.
	\end{theo}
	\begin{proof}
		The expected value of the reconciled forecast is given by
		\[
		\E_{1:t}(\tilde{\bm{y}}_{t+h|t})
		= \E_{1:t}(\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t})
		= \bm{S}\bm{G}\E_{1:t}(\hat{\bm{y}}_{t+h|t})
		= \bm{S}\bm{G}\bm{\mu}_{t+h|t}.
		\]
		Since $\bm{\mu}_{t+h|t}$ is an expectation taken with respect to the degenerate predictive density it must lie in $\mathfrak{s}$. We have already established that when $\bm{S}\bm{G}$ is a projection onto $\mathfrak{s}$ then it maps all vectors in $\mathfrak{s}$ onto themselves. As such $\bm{S}\bm{G}\bm{\mu}_{t+h|t}=\bm{\mu}_{t+h|t}$ when $\bm{S}\bm{G}$ is a projection matrix.
	\end{proof}

	We note that the above result holds when the projection $\bm{SG}$ has the coherent subspace $\mathfrak{s}$ as its image and not for all projection matrices in general. To describe this more explicitly suppose $\bm{SG}$ has as its image  $\mathfrak{L}$ which is itself a lower dimensional linear subspace of $\mathfrak{s}$, i.e. $\mathfrak{L}\subset\mathfrak{s}$. Then for $\left\{\bm{\mu}_{t+h|t}:\bm{\mu}_{t+h|t}\in\mathfrak{s},\bm{\mu}_{t+h|t}\notin\mathfrak{L}\right\}$,  $\bm{S}\bm{G}\bm{\mu}_{t+h|t} \ne \bm{\mu}_{t+h|t}$. This is depicted in Figure~\ref{fig:Schematic_3D} where $\bm{\mu}_{t+h|t}$ is projected to a point $\bar{\bm{\mu}}$ in $\mathfrak{L}$.  In this case, the expectation of reconciled forecast will be $\bar{\bm{\mu}}$ rather than $\bm{\mu}_{t+h|t}$ and hence biased.
	
	This result has implications in practice. The top-down method \citep{Gross1990} has
	\begin{equation}\label{eq:top-downG}
	\bm{G}=\begin{pmatrix}
	\bm{p} & \bm{0}_{(m \times n-1)}
	\end{pmatrix}
	\end{equation}
    where $\bm{p} = (p_1,\dots,p_m)'$ is an $m$-dimensional vector consisting a set of proportions used to disaggregate the top-level forecast.  In this case it can be verified that $\bm{SG}$ is idempotent, i,e. $\bm{SGSG}=\bm{SG}$ and therefore $\bm{SG}$ is a projection matrix.  However the image of this projection is not an $m$-dimensional subspace but a $1$-dimensional subspace.  As such, top-down reconciliation produces biased forecasts even when the base forecasts are unbiased.
		
	\begin{figure}[H]
		\centering
		\vspace{-0.9cm}
		\small
		\resizebox{\linewidth}{!}{
			\input{Figs/Schem_3D}
		}
		\caption{$\mathfrak{L}$ is a linear subspace of the coherent subspace $\mathfrak{s}$. If $s\circ g$ is a projection not onto $\mathfrak{s}$ but onto $\mathfrak{L}$, then $\bm{\mu} \in \mathfrak{s}$ will be moved to $\bar{\bm{\mu}} \in \mathfrak{L}$.}\label{fig:Schematic_3D}
	\end{figure}
	
	Finally, it is often stated that an assumption required to prove the unbiasedness preserving property is that $\bm{SGS}=\bm{S}$ or alternatively that $\bm{GS}=\bm{I}$.  Both of these conditions are equivalent to assuming that $\bm{SG}$ is a projection matrix. However, problems arise when viewing the preservation of unbiasedness through the prism of imposing a constraint $\bm{GS}=\bm{I}$. This thinking suggests that a way to deal with biased forecasts is to select $\bm{G}$ in an unconstrained manner.  However, equipped with a geometric understanding of the problem, we would advise against this approach.  The constraint $\bm{GS}=\bm{I}$  is not just about bias and dropping the constraint compromises all of the attractive properties of projections.  Opening the door to reconciliation methods that change already coherent base forecast would seem to suggest an increase in the variability of the forecast.  This seems particularly perverse when the motivation for using a biased method in the first place is to reduce variance is to reduce variance.

    \subsection{Bias correction}
	
	Our own solution to dealing with biased forecasts is to bias correct {\em before} reconciliation.  In many cases the method for bias correction will be context specific.  For instance, in  our empirical study in Section~\ref{sec:EmpStudy} we consider a scenario where bias is induced via taking a Box-Cox transformation before modelling.  In this well-known case a number of bias correction methods exist.  Our particular choice of bias correction will be the Guererro method \citep{guerrero1993time}. \textcolor{red}{TODO: Read this again after the empirical application has been completed. I am saying this because my understanding is that we use the Guererro method to choose the lambda for the BoxCox transformation. Then we back transform the forecasts using the same lambda once with bias correcting using the factor that comes out of a Taylor series expansion around $\mu$ (programmed in fable/forecast package) and once without it hence looking at medians and not means - hence biased base forecasts. If my understanding here is correct I am not sure we can say we are using the Guererro method to bias correct.}


	
	Alternatively, a more general purpose approach to bias correction is to simply estimate the bias by taking the sample mean of $\bm{y
	}_{t+h}-\hat{\bm{y}}_{t+h|t}$ for all $t+h$ in the training sample.  This can be then subtracted from future forecasts.  As stated in the discussion of MinT, in-sample errors are already used to estimate the optimal direction of projection.  As such we see no problems with using the same errors to bias correct.  Geometrically, the intuition is simple.  In top left panel in Figure \ref{fig:OthogonalVSOblique_projection}, the orange points are centered around the origin as would be expected from an unbiased forecast.  If forecasts are biased, then errors should simply be translated until they are centered at the origin.
	
	\section{Empirical study} \label{sec:EmpStudy}
	
	
	\textcolor{red}{GA: I will rewrite/review this when we decide where we put all our results - I am referring to the projection results - currently I think it is best that everything goes here and we make a comment at the end of Section 3.3 guiding the reader to the results here. This will make the empirical application a bit broader not only about bias correction, i.e., both about showing how projections work and how bias correction works.} Using an empirical application to forecast Australian domestic tourism flow, we illustrate the usefulness of projection based reconciliation in practice. Previous studies have pronounced that the reconciliation improves point forecast accuracy in domestic tourism flow in Australia (\cite{Athanasopoulos2009}, \cite{Hyndman2011}, \cite{WicEtAl2019}). However, our motivation in this study is to demonstrate how the bias correction methods discussed in previous section along with the projection-based reconciliation help to improve the forecast accuracy.
	
	
	\subsection{Data}
	
	We use ``overnight trips'' across Australia as a measure of domestic tourism flows. The data are provided by the National Visitor Survey (NVS) and are collected through telephone interviews from an annual sample of $120,000$ Australian residents aged $15$ years or more. We disaggregate Tourism flows into 7 states, 27 zones and 75 regions forming a natural geographical hierarchy that is of interest to tourism operators and policy makers. Hence, there are $110$ series across the hierarchy with $75$ bottom-level series. More information about the series and the geographical hierarchy is presented in Table \ref{table:A1} in the Appendix. The data span the period January 1998 to December 2017, which gives a total of $240$ observations per series.

Figure \ref{fig:Total_TSplots} shows time, sub-series and seasonal plots of the aggregate overnight trips for Australia. As is usual with tourism data, overnight these show a strong seasonal pattern. Overnight trips peak in January corresponding to the summer vacation season in Australia. There are also some lower peaks observed in April, July and October corresponding to school term breaks. On the other hand the month with the least overnight trips is February indicating that people travel least for the month following their summer vacation. The time plot also shows a pronounced upward trend starting from around 2010 till the end of the sample, with flows being fairly flat from the beginning of the sample and a slight downward trend during 2004-2010.

The top panel of Figure \ref{fig:States_Zones_Regs_TSplots} shows time plots for the seven states, hence the first-level of the hierarchy. The panels below show some selected series from the second-level zones and the bottom-level regions of the geographical hierarchy. The plots display the diversity of time series features, within but also between levels. For example, noticeable at the first-level is the asynchronous seasonal pattern between Northern Territory and the other states. For the Northern Territory the high tourist season occurs during June-August with July being the peak, while the low season is during December-February. This reflects the tropical climate of the Northern Territory, with Australians mostly visiting the North during its dry winter-season rather than the wet summer season. Noticeable as we move to the lower levels is the variation in the signal-to-noise ratio, with the regional bottom-level series being noisier compared to the series from levels above. This of course highlights the importance of modelling series at all levels without any loss of valuable information. We should note here that we observed an anomalous (extremely high) observation for `Adelaide Hills' for December-2002. We replaced this observation with the average overnight trips on December-2001 and December-2003 for the same destination.
		
	\begin{figure}
		\centering
		\small
		\includegraphics[width = \textwidth]{Empirical-results/TS-plots/Total_TSplots.pdf}
		\caption{Total domestic overnight trips (in logs) for Australia from January 1998 to December 2017. The top-panel shows a time plot; the bottom-left panel a sub-series plot for each month; the bottom-right panel shows a seasonal plot coloured by year.} \label{fig:Total_TSplots}
	\end{figure}
	
\begin{figure}
		\centering
		\small
		\includegraphics[width= \textwidth]{Empirical-results/TS-plots/States_Zones_Regs_TSplots1.pdf}
		\caption{Time plot of overnight trips for some selected series from different disaggregate levels of the hierarchy. All values are presented in log scale. To avoid impact from the zero values we added a constant 1 to all observations}\label{fig:States_Zones_Regs_TSplots}
\end{figure}

\clearpage

    \subsection{Comparison to Base Forecasts}\label{sec:comparebase}
    
    To demonstrate the implications of Theorem~\ref{th:distred} we consider the improvement of different reconciliation methods over base forecasts.  For each series the ARIMA model minimising AICc is chosen using the \verb|auto.arima()| function in the \verb|forecast| package.  Using these fitted models, base forecasts are produced for $h=1$ to $6$-steps ahead for each series in the hierarchy.  This is first carried out with a training sample of $100$ observations, i.e., Jan-$1998$ to Apr-$2006$. The training window is then rolled forward one observation at a time until the end of the sample. This generates $140~1$-step-ahead, $139~2$-steps-ahead through to $135~6$-steps-ahead forecasts available for forecast evaluation.
    
    After obtaining the base forecasts these are reconciled using three different methods; OLS reconciliation, WLS reconciliation using variance scaling\todo{George please check} and MinT use a shrinkage estimator for ${\bm W}$.  Squared forecast errors are computed for each series and aggregated, i.e. we compute the loss function described in Section~\ref{sec:othogonal}.  We then compute the difference in this loss function between base forecasts and reconciled forecasts, where positive values indicat that reconciled forecasts are more accurate. The boxplots in Figure~\ref{fig:BaseVSRecon_Fc} summarise the distribution of this measure over each rolling window.  As the theory predicts, OLS reconciliation always leads to an improvement relative to base forecasts, the entire boxplot for OLS reconciliation is above zero.  This is not the case for WLS and MinT which for some windows perform worse than base forecasts.  The boxplots also demonstrate that OLS reconciliation is more stable than WLS and MinT reconciliation, a result not entirely unsurprising since the latter two methods require estimation of a ${\bm W}$ matrix which in practice is not trivial.  Nonetheless when averaging over the entire rolling window the MinT estimator performs best on average which is again precisely what the theorem proved in \cite{WicEtAl2019} would suggest.
    
    \begin{figure}[H]
    	\centering
    	\small
    	\includegraphics[width = \textwidth]{Figs/OrthVsOblq_Proj_Emp_results.pdf}
    	\caption{MSE difference between base forecasts and reconciled forecasts over the replications is presented for forecast horizons $h = 1,...,6$. Positive values of the difference implies reconciliation improves the forecast accuracy than base forecasts.}\label{fig:BaseVSRecon_Fc}
    \end{figure}
         

	\subsection{Transformations and bias adjustment}
	
	We first transform each series in the hierarchy using two types of transformations. Namely, we perform a log-transformation and also the more general Box-Cox transformation. A Box-Cox transformation is defined as,
		
\begin{equation} \label{eq:BoxCox_transformation}
	w_t =
	\begin{cases}
	\log(y_t) & \text{if}~\lambda=0;\\
	\frac{y_t^\lambda - 1}{\lambda}  & \text{otherwise}.
	\end{cases}
\end{equation}

We first set $\lambda=0$ and hence consider only a log transformation. For the second more general Box-Cox transformation we select $\lambda$ using the ``Guerrero'' method \citep{guerrero1993time} implemented in the \verb|BoxCox.lambda()| function in the \verb|forecast| package in R \citep{Rforecast}. As observations with zero value exist in some of the bottom-level series, before transforming we add a constant (more specifically $1$) to each series. This overcomes the challenge of undefined transformed values for zero observations when we specifically implement the log transformation or when $\lambda$ is selected to be zero by the ``Guerrero'' method. The constant is subtracted from the final forecasts.

After transformation we fit univariate ARIMA models to each transformed series. The \verb|auto.arima()| function in the \verb|forecast| package is used to choose the best model that minimises the AICc. Using the fitted models, forecasts are produced for $h=1$ to $6$-steps ahead for each series in the hierarchy.

The forecasts are then back-transformed by simply reversing the Box-Cox transformation using

\begin{equation} \label{eq:BoxCox_back-transformation}
	\hat{y}_{t+h|t} =
	\begin{cases}
	\exp({\hat{w}_{t+h|t}}) & \text{if}~\lambda = 0; \\
	(\lambda \hat{w}_{t+h|t} + 1)^{1/\lambda}  & \text{otherwise.}
	\end{cases}
\end{equation}

These back-transformed forecasts are potentially biased as they are not the mean of the forecast distribution but the median (assuming that the distribution of the transformed space is symmetric). Hence, the reconciled forecasts that follow from these forecasts will also be biased. This is the exact scenario that we want to demonstrate in this study and we next move to our proposed solution of bias correcting the base forecasts before reconciling for which we explore two scenarios.

Using a Taylor series expansion \todo{Rob do we need a reference here or is this obvious/well known?} the back-transformed mean of the forecast distribution for a Box-Cox transformation is given by

\begin{equation} \label{eq:BoxCox_BT_biasadj}
	 \hat{y}_{t+h|t} =
	\begin{cases}
	\exp({\hat{w}_{t+h|t}})[1+\frac{\sigma_h^2}{2}] & \text{if}~\lambda = 0; \\
	(\lambda \hat{w}_{t+h|t} + 1)^{1/\lambda}[1 + \frac{\sigma_h^2(1-\lambda)}{2(\lambda \hat{w}_{t+h|t} + 1)^2}]       & \text{if}~\lambda \ne 0,
	\end{cases}
	\end{equation}
where, $\hat{w}_{t+h|t}$ is the $h$-step-ahead forecast from the Box-Cox transformed series and $\sigma_h^2$ is the variance of $\hat{w}_{t+h|t}$. Using the mean of the forecast distribution returns bias adjusted base forecasts compared to the simple back-transformation of \eqref{eq:BoxCox_back-transformation}. We refer to this as Method-1 in the results that follow.

The second scenario of bias adjustment we explore is using the in-sample forecast error mean of the biased forecasts to adjust the out of sample forecasts. We refer to this a Method-2 in the results that follow.
	
Using the three sets of base forecasts we generate coherent forecasts using the bottom-up approach but also implementing OLS, WLS and MinT reconciliation projections and compare the results for when the base forecasts are biased and bias-adjusted (unbiased). We discuss the results next.

\subsection{Results and discussion}


%	Mean Squared Error (MSE) is used to measure the forecast accuracy and results are presented in Table \ref{tab:Results_MSE}. Recall that projections preserve the unbiasedness in reconciled forecasts only if the base forecasts are unbiased. As a consequence, the reconciled forecasts follow from the biased base forecasts will also be biased. We can see from the results that unbiased-reconciled forecasts from MinT(Shrink) and WLS are better than that of biased-reconciled forecasts although the unbiased-base forecasts are not necessarily better than the biased-base forecasts. Furthermore, unbiased MinT reconciled forecasts are outperforming all biased and unbiased forecasts. These results are true for all forecast horizons.

\begin{table}
	\caption {Average {MSE($\times 10^3$)} of base and reconciled $1$-step-ahead point forecasts are presented for log transformation and Box-Cox transformation. Unbiased(Method-1) follows from the bias adjustment via Taylor's de-biasing factor whereas Unbiased(Method-2) follows from residual mean adjustment.}
	\label{tab:Results_MSE}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{@{}lcccccc@{}}
			\toprule
			\multicolumn{1}{c}{ } & \multicolumn{3}{c}{Log Transformation} &
			\multicolumn{3}{c}{Box-Cox Transformation}\\
			\cmidrule(lr){2-4} \cmidrule(lr){5-7}
			R.method & Biased & Unbiased & Unbiased & Biased & Unbiased & Unbiased \\
			& & (Method-1) & (Method-2) && (Method-1) &(Method-2)\\
			\midrule
			Base & 12.06 & 11.87 & 12.52 & 12.27 & 139.94 & 13.45\\
			\hline
			Bottom-up & 17.37 & 15.47 & 21.58 & 18.06 & 15.76 & 23.33\\
			\hline
			OLS & 11.59 & 11.41 & 11.98 & 11.84 & 116.91 & 12.96\\
			\hline
			WLS & 15.00 & 13.83 & 17.55 & 15.83 & 14.20 & 19.51\\
			\hline
			MinT(Shrink) & 9.36 & $\bm{9.25}$ & 10.35 & 10.27 & $\bm{10.11}$ & 12.43\\
			\bottomrule
		\end{tabular}
	}
	
\end{table}



Table \ref{tab:Results_MSE} presents the Mean Squared Error (MSE) over the $140$ replications for $1$-step-ahead forecasts. Note that the conclusion that follow extend almost identically to forecast horizons $2-6$. We do not present these here to save space but they are available upon request.

General findings:

- Bias adjusting using Method 1, i.e., a proper method improves forecast accuracy.
- Bias adjusting using Method 2, does not. The transformations we consider here are multiplicative type transformations - the in-sample residual adjustment is an additive adjustment hence it does not help.

\textcolor{red}{I have stopped here. Will continue after we have all results.}

Let us first consider the results from log transformation. We see that the unbiased forecasts from Method-1 has less MSE compared to the biased forecasts in base level. This holds for all reconciled forecasts as well. It implies that the bias correction has improved the forecast accuracy. Further, unbiased reconciled forecasts from OLS and MinT perform better than the base forecasts. Moreover, Unbiased MinT reconciled forecasts are outperforming.
We also note that the bias correction via Method-2 has not necessarily work well in this empirical example.

Now turning to the results from Box-Cox transformation, we see that the bias correction via Method-1 has worsen the base forecasts. The same result holds for OLS reconciled forecasts as well. We also observed that this has happened because the bias correction is doing worst for some replications in ``Total" series as depicted in Figure \ref{fig:Total_FcVsTrue_BoxCox}. \textcolor{red}{This because for these replications the selected model has a drift term with a very large standard error. This will blow up the bias-adjusted forecasts.}

However reconciled forecast from, Bottom-up, WLS or MinT has mitigate the effect from this unusual result in unbiased base forecasts. Most importantly, Unbiased MinT is outperforming all forecasts.

We have presented results only for one forecast horizon. However, results for other forecast horizons follows the same conclusions.


\begin{figure}
	\centering
	\small
	\includegraphics[width = \textwidth]{Empirical-results/Final-results/Total_FcVsTrue_BoxCox.pdf}
	\caption{Actual values vs the base forecasts for Total overnight trips follows from Box-Cox transformation}\label{fig:Total_FcVsTrue_BoxCox}
\end{figure}





%%%%%%%%%%%%%%%%%%%
	
%\begin{table}
%	\caption {\textcolor{red}{RESULTS FROM LOG TRANSFORMATION
%			Average {MSE($\times 10^3$)} of base and reconciled point forecasts at forecasts horizons $h=1,...,6$ are presented. Bias and unbiased columns represent the results for biased-base/biased-reconciled and unbiased-base/unbiased-reconciled forecasts respectively. Comparisons can be made across biased vs unbiased forecasts as well as base vs reconciled forecasts.}}
%	\label{tab:Results_MSE_LogTrans}
%	\centering\tabcolsep=0.001cm
%	\small
%	\resizebox{\linewidth}{!}{
%		\begin{tabular}{@{}lSSS@{}}
%		
%		R.method & Biased & Unbiased (Method-1) & Unbiased (Method-2)\\
%		\midrule
%		Base & 12.06 & 11.87 & 12.52\\
%		\hline
%		Bottom-up & 17.37 & 15.47 & 21.58\\
%		\hline
%		MinT(Shrink) & 9.36 & 9.25 & 10.35\\
%		\hline
%		OLS & 11.59 & 11.41 & 11.98\\
%		\hline
%		WLS & 15.00 & 13.83 & 17.55 \\
%		\bottomrule
%	\end{tabular}
%}
%
%\end{table}
%
%\begin{table}
%	\caption {\textcolor{red}{RESULTS FROM BOXCOX TRANSFORMATION
%			Average {MSE($\times 10^3$)} of base and reconciled point forecasts at forecasts horizons $h=1,...,6$ are presented. Bias and unbiased columns represent the results for biased-base/biased-reconciled and unbiased-base/unbiased-reconciled forecasts respectively. Comparisons can be made across biased vs unbiased forecasts as well as base vs reconciled forecasts.}}
%	\label{tab:Results_MSE_BoxCoxTrans}
%	\centering\tabcolsep=0.08cm
%	\resizebox{\linewidth}{!}{
%		\begin{tabular}{@{}lSSS@{}}
%			
%			R.method & Biased & Unbiased (Method-1) & Unbiased (Method-2)\\
%			\midrule
%			Base & 12.27 & 139.94 & 13.45\\
%			\hline
%			Bottom-up & 18.06 & 15.76 & 23.33\\
%			\hline
%			MinT(Shrink) & 10.27 & 10.11 & 12.43\\
%			\hline
%			OLS & 11.84 & 116.91 & 12.96\\
%			\hline
%			WLS & 15.83 & 14.20 & 19.51\\		
%			\bottomrule
%		\end{tabular}
%	}
%	
%\end{table}
%	
%	\begin{figure}
%		\centering
%		\small
%		\includegraphics[width = \textwidth]{Empirical-results/Final-results/SS_MSE_Results.pdf}
%		\caption{\textcolor{red}{include caption}}\label{fig:MSE_SkillScore}
%	\end{figure}
%	
	
	
	
	
\section{Conclusions} \label{sec:conclusions}
	
	%By redefining coherent point forecast and point forecast reconciliation, we rehash all existing reconciliation methods into a single projection based geometric framework. We have also established new theoretical results that support the use of projections for point forecast reconciliation. We show that projection of unbiased base forecasts onto the coherent subspace will always produce unbiased reconciled forecasts. Yet the projection-based reconciliation can be used to reconciled bias base forecasts after bias adjustments. Empirical results from the application of these methods to forecasting Australian domestic tourism flow show that reconciled forecast follows from bias-adjusted base improves the forecast accuracy and unbiased MinT reconciliation is outperforming.
	
	Defining concepts such as coherence and reconciliation in geometric terms provides new insights into forecast reconciliation methods.  We have also provided evidence that reconciliation, particularly using the MinT method, can mitigate the effect of poor bias correction.  Our intention in proposing a geometric interpretation is also to provoke research into new areas.  We now discuss three such possibilites.
	
	First, it should be possible to extend to the concept of coherence to non-linear constraints.  In these cases the coherent space may need to be defined by a manifold.  Although much more challenging, it is still possible define reconciled forecasts in terms of projections onto a manifold.  Second, since we have established that the concept of bottom-level series is not crucial in forecast reconciliation an open question is whether is may be better to construct base forecasts of linear combinations of the time series rather than the time series themselves.  Finally, the geometric interpretations of hierarchical forecast reconciliation facilitates an extension into a probabilistic framework, an issue that we investigate in a separate paper.  	
	
	
	\section{Appendix}
	%\thisfloatpagestyle{empty}
	\subsection{Proof $\bm{S}\bm{G}\bm{S}=\bm{S}$ implies $\bm{S}\bm{G}$ is a projection}
	Here we establish that if $\bm{S}\bm{G}$ is a projection onto the linear subspace spanned by $\bm{S}$ then $\bm{S}\bm{G}\bm{S}=\bm{S}$.  We also prove that the converse holds, namely that if the condition $\bm{S}\bm{G}\bm{S}=\bm{S}$ holds then $\bm{S}\bm{G}$ must be a projection onto the linear subspace spanned by $\bm{S}$.
	
	To establish the first statement let $\bm{s}_j$ be the $j^{th}$ column of $\bm{S}$.  Since by definition, $\bm{s}_j$ lies in $\mathfrak{s}$, it must hold that $\bm{S}\bm{G}\bm{s}_j=\bm{s}_j$.  Stacking these vectors horizontally
	\begin{align}
	\bm{S}\bm{G}\bm{S}&=\begin{pmatrix}
	\bm{S}\bm{G}\bm{s}_1, & \bm{S}\bm{G}\bm{s}_2, & \cdots & \bm{S}\bm{G}\bm{s}_m
	\end{pmatrix}\\
	&=\begin{pmatrix}
	\bm{s}_1, & \bm{s}_2, & \cdots & \bm{s}_m
	\end{pmatrix}\\
	&=\bm{S}
	\end{align}
	
	To establish the converse it suffices to postmultiply the condition $\bm{S}\bm{G}\bm{S}=\bm{S}$ by $\bm{G}$.  This yields $\bm{S}\bm{G}\bm{S}\bm{G}=\bm{S}\bm{G}$ which in turn implies idempotence since $(\bm{S}\bm{G})^2=\bm{S}\bm{G}$.
	\input{Appendix}
	\newpage
	

	
	\bibliographystyle{agsm}
	
	\bibliography{References_paper1}
	
\end{document}

