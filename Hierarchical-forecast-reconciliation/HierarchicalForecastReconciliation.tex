\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

\usepackage{amssymb, qtree, bm, multirow, textcmds, siunitx,paralist}
\usepackage{mathrsfs, float, booktabs,todonotes,amsthm, xcolor,sidenotes}
\usepackage[bb=boondox]{mathalfa}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,shapes,fit,calc}
\usepackage{amsfonts}

\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

\DeclareMathOperator*{\argmin}{arg\,min}
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\def\mathbi#1{\textit{ #1}}
\def\mathB#1{\textbf{ #1}}
\def\E{\text{E}}
\def\var{\text{Var}}

\def\PQ{\begin{pmatrix}\bm{G}\\[-0.2cm]\bm{H}\end{pmatrix}}
\def\bt{\begin{pmatrix}\tilde{\bm{b}}\\[-0.2cm]\tilde{\bm{a}}\end{pmatrix}}

%\theoremstyle{theo}
\newtheorem{theo}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{property}
\newtheorem{property}{Property}[section]


\begin{document}
	
	
	%\bibliographystyle{natbib}
	
	\def\spacingset#1{\renewcommand{\baselinestretch}%
		{#1}\small\normalsize} \spacingset{1}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\if1\blind
	{
		\title{\bf Hierarchical Forecasts Reconciliation}
		\author{Puwasala Gamakumara\thanks{
				The authors gratefully acknowledge the support of Australian Research Council Grant DP140103220.  We also thank Professor Mervyn Silvapulle for valuable comments.}\hspace{.2cm}\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3800, Australia.\\
			Email: Puwasala.Gamakumara@monash.edu \\
			and \\
			Anastasios Panagiotelis\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3800, Australia.\\
			Email: Anastasios.Panagiotelis@monash.edu \\
			and \\
			George Athanasopoulos\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3800, Australia.\\
			Email: george.athanasopoulos@monash.edu \\
			and \\
			Rob J Hyndman\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3800, Australia.\\
			Email: rob.hyndman@monash.edu \\}
		\maketitle
	} \fi
	
	\if0\blind
	{
		\bigskip
		\bigskip
		\bigskip
		\begin{center}
			{\LARGE\bf Hierarchical Forecasts Reconciliation}
		\end{center}
		\medskip
	} \fi
	
	\bigskip
	
	
	\begin{abstract} TBC
		%  Forecast reconciliation involves adjusting forecasts to ensure coherence with aggregation constraints. We extend this concept from point forecasts to probabilistic forecasts by redefining forecast reconciliation in terms of linear functions in general, and projections more specifically. New theorems establish that the true predictive distribution can be recovered in the elliptical case by linear reconciliation, and general conditions are derived for when this is a projection. A geometric interpretation is also used to prove two new theoretical results for point forecasting; that reconciliation via projection both preserves unbiasedness and dominates unreconciled forecasts in a mean squared error sense. Strategies for forecast evaluation based on scoring rules are discussed, and it is shown that the popular log score is an improper scoring rule with respect to the class of unreconciled forecasts when the true predictive distribution coheres with aggregation constraints. Finally, evidence from a simulation study shows that reconciliation based on an oblique projection, derived from the MinT method of \citet{Wickramasuriya2017} for point forecasting, outperforms both reconciled and unreconciled alternatives.
	\end{abstract}
	
%	\noindent%
%	{\it Keywords:}  Forecast Reconciliation, Projections, Elliptical Distributions, Scoring Rules, High-dimensional Time Series.
%	\vfill
	
	\newpage
	\spacingset{1.45} % DON'T change the spacing!
	
	\section{Introduction}\label{sec:intro}
	
	
%	Large collections of time series often follow some aggregation structure. For example, the electricity demand of a country can be disaggregated according to a geographic hierarchy of states, cities, and individual households. To ensure aligned decision making, it is important that forecasts at the most disaggregated level add up to forecasts at more aggregated levels. This property is called ``coherence''.  On the other hand ``reconciliation'' is a process whereby incoherent forecasts are made coherent. Both of these concepts have been developed extensively for point forecasting. Generalising both of these concepts, particularly the latter, to probabilistic forecasting is a gap that we seek to address in this work.  We do so by first providing a novel geometric interpretation to coherence and reconciliation in the point forecasting case. This can easily be generalised to probabilistic forecasting allowing us to derive further results for elliptical distributions as well as provide insight into forecast evaluation via multivariate  scoring rules. 
%	
%	Traditional approaches to ensure coherent point forecasts produce first-stage forecasts at a single level of the hierarchy. To describe these we use the small hierarchy in Figure~\ref{fig1} where the variable labelled $Tot$ is the sum of the series $A$ and series $B$, the series $A$ is the sum of series $AA$ and series $AB$ and the series $B$ is the sum of the series $BA$ and $BB$. In the bottom-up approach \citep{Dunn1976}, forecasts are produced at the most disaggregated level (series $AA$, $AB$, $BA$ and $BB$) and then summed to recover forecasts for all higher-level series. Alternatively, in the top-down approach \citep{Gross1990}, a top-level forecast is first produced (series $Tot$) and bottom-level forecasts are recovered by disaggregating the forecast using either historical or forecasted proportions. A middle-out approach is a hybrid between these two, that for the hierarchy in Figure~\ref{fig1} would produce first stage forecasts for series $A$ and $B$.
%	
%	
%	In recent years, reconciliation methods introduced by \citet{Hyndman2011} have become increasingly popular. For these methods, first stage forecasts are independently produced for all series rather than series at a single level. Since these so-called `base' forecasts are rarely coherent in practice, they are subsequently adjusted or `reconciled' to ensure coherence.  Note that we use coherence and reconciliation as distinct terms, in contrast to their at times ambiguous usage in the past. To date, reconciliation has typically been formulated as a regression problem with alternative reconciliation methods resembling different least squares estimators. These include Ordinary Least Squares {OLS} \citep{Hyndman2011}, Weighted Least Squares {WLS} \citep{AthEtAl2017}, and a Generalised Least Squares (GLS) estimator \citep{Wickramasuriya2017} named MinT since it minimises the trace of the squared error matrix. These methods have been shown to outperform traditional alternatives across a range of simulated and real-world datasets \citep{AthEtAl2009,VanErven2015a,Wickramasuriya2017} since they use information at all levels of the hierarchy and, in some sense, hedge against the risk of model misspecification at a single level.
%	
%	A shortcoming of the existing literature is a focus on point forecasting despite an increased understanding over the past decade of the importance of providing a full predictive distribution for forecast uncertainty \citep[see][and references therein]{Gneiting2014}. Indeed to the best of our knowledge, the (as yet unpublished) work of \citep{BenTaieb2017} is the only paper to deal with coherent probabilistic forecasts, and although they reconcile the means of the predictive distributions, the overall distributions are constructed in a bottom-up fashion rather than use a reconciliation process. In contrast, the main objective of our paper is to generalise both coherence and reconciliation from point to probabilistic forecasting.
%	
%	To facilitate the extension of point forecast reconciliation to probabilistic forecasting, we first provide a geometric interpretation of existing point reconciliation methods, framing them in terms of projections. In addition to being highly intuitive, this allows us to establish a number of theoretical results. We prove two new theorems about point forecast reconciliation, the first showing that reconciliation via projections preserves the unbiasedness of base forecasts, while the second shows that reconciled forecasts dominate unreconciled forecasts via the distance reducing property of projections. We provide definitions of coherence and forecast reconciliation in the probabilistic setting, and describe how these definitions lead to a reconciliation procedure that merely involves a change of basis and marginalisation. We show that probabilistic reconciliation via linear transformations can recover the true predictive distribution as long as the latter is in the elliptical class. We provide conditions for which this linear transformation is a projection, and although this projection cannot be feasibly estimated in practice, we provide a heuristic argument in favour of MinT reconciliation.
%	
%	We also cover the topic of forecast evaluation of probabilistic forecasts via scoring rules. In particular, we prove that for a coherent data generating process, the log score is not proper with respect to incoherent forecasts. Therefore we recommend the use of the energy score or variogram score for comparing reconciled to unreconciled forecasts. Two or more reconciled forecasts can be compared using log score, energy score or variogram score, although we show that comparisons should be made on the full hierarchy for the latter two scores.
%	
%	The remainder of the paper is structured as follows. In Section~\ref{sec:definitions} coherence is defined geometrically for both point and probabilistic forecasts. Section~\ref{sec:reconciliation} contains definitions of point and probabilistic forecast reconciliation as well as our main theoretical results. In Section~\ref{sec:evaluation} we consider the evaluation of probabilistic hierarchical forecasts via scoring rules, while a simulation study comparing unreconciled probabilistic forecasts and different kinds of reconciled probabilistic forecasts is provided in Section~\ref{sec:gaussian}. Section~\ref{sec:conclusions} concludes with some discussion and thoughts on future research.
	
\section{Coherent forecasts}\label{sec:CoheForecasts}
	
	\subsection{Notation and preliminaries}\label{sec:notation}
	
    We briefly define the concept of a \emph{hierarchical time series} in a fashion similar to \cite{Wickramasuriya2017}, \cite{FPP2018} and others, before elaborating on some of the limitations of this understanding.  A \emph{hierarchical time series} is a collection of $n$ variables indexed by time, where some variables are aggregates of other variables. We let $\bm{y}_t \in \mathbb{R}^n$ be a vector comprising observations of all variables in the hierarchy at time $t$. The \emph{bottom-level series} are defined as those $m$ variables that cannot be formed as aggregates of other variables; we let $\bm{b}_t \in \mathbb{R}^m$ be a vector comprised of observations of all bottom-level series at time $t$.  The hierarchical structure of the data implies that the following holds for all $t$
    
    \begin{equation}
    \bm{y}_t = \bm{Sb}_t,
    \end{equation}
    where $\bm{S}$ is an $n \times m$ constant matrix that encodes the aggregation constraints.  
	
		\begin{figure}[H]
			\begin{center}
				\leaf{AA} \leaf{AB}
				\branch{2}{A}
				\leaf{BA} \leaf{BB}
				\branch{2}{B}
				\branch{2}{Tot}
				\qobitree
			\end{center}
			\caption{An example of a two level hierarchical structure.}\label{fig:basichier}
		\end{figure}
	
	To clarify these concepts consider the example of the hierarchy in Figure~\ref{fig:basichier}.  For this hierarchy, $n=7$, $\bm{y}_t = [y_{Tot,t},y_{A,t}, y_{B,t},y_{C,t},y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}]'$, $m=4$, $\bm{b}_t = [y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}]'$ and
	\[
	\bm{S} = \begin{pmatrix}
	1 & 1 & 1 & 1  \\
	1 & 1 & 0 & 0 \\
	0 & 0 & 1 & 1 \\
	& \multicolumn{2}{c}{\bm{I}_4} &
	\end{pmatrix},
	\]
	where $\bm{I}_4$ is the $4\times 4$ identity matrix.
	
	While such a definition is completely serviceable, it obscures the full generality of the literature on so-called hierarchical time series.  In fact, concepts such as coherence and reconciliation, defined in full below, only require the data to have two important characteristics; the first is that they are multivariate, the second is that they adhere to linear constraints.  
	
	\subsection{Coherence}\label{sec:cohpointf}
	
	The property that data adhere to some linear constraints is referred to as {\em coherence}.  We now provide definitions aimed at providing geometric intuition of hierarchical time series.
	
	 \begin{definition}[Coherent subspace]\label{def:cohspace}
	 	The $m$-dimensional linear subspace $\mathfrak{s}\subset \mathbb{R}^n$ for which a set of linear constraints holds for all $\bm{y}\in\mathfrak{s}$ is defined as the \emph{coherent subspace}.
	 \end{definition}
 
     To further illustrate, Figure \ref{fig:3D_hierarchy} depicts the most simple three variable hierarchy where $y_{Tot,t}=y_{A,t}+y_{B,t}$.  The coherent subspace is depicted as a grey $2$-dimensional plane within $3$-dimensional space, i.e. $m=2$ and $n=3$.  It is worth noting that the coherent subspace is spanned by the columns of $\bm{S}$, i.e.\ $\mathfrak{s}=\text{span}(\bm{S})$.  In Figure~\ref{fig:3D_hierarchy}, these columns are $\vec{s}_1=(1,1,0)'$ and $\vec{s}_2=(1,0,1)'$.  However, it is equally important to recognise that the hierarchy could also have been defined in terms of $y_{Tot,t}$ and $y_{A,t}$ rather than the bottom level series, $y_{A,t}$ and $y_{B,t}$. In this case the corresponding `$\bm{S}$ matrix' would have columns $(1,0,1)'$ and $(0,1,-1)'$.  However, while there are multiple ways to define an $\bm{S}$ matrix, in all cases the columns will span the same coherent subspace, which is unique.
     
     \begin{figure}[H]
     	\centering
     	\vspace{-0.9cm}
     	\small
     	\resizebox{\linewidth}{!}{
     		\input{Figs/3D_hierarchy}
     	}
     	\caption{Depiction of a three dimensional hierarchy with $y_{\text{Tot}} = y_{\text{A}} + y_{\text{B}}$. The gray colour two dimensional plane reflects the coherent subspace $\mathfrak{s}$ where $\vec{s}_1 = (1,1,0)'$ and $\vec{s}_2 = (1, 0, 1)'$ are basis vectors that spans $\mathfrak{s}$. The points in $\mathfrak{s}$ represents realisations or coherent forecasts}\label{fig:3D_hierarchy}
     \end{figure}
     
	 \begin{definition}[Hierarchical Time Series]\label{def:cohspace}
	 	A hierarchical time series is an $n$-dimensional multivariate time series such that all observed values $\bm{y}_1,\ldots,\bm{y}_T$ and all future values $\bm{y}_{T+1},\bm{y}_{T+2},\ldots$  lie in the coherent subspace, i.e. $\bm{y}_t\in\mathfrak{s}\quad\forall t$.
	 \end{definition}
	 
	 Despite the common use of the term {\em hierarchical time series}, it should be clear from the definition that the data need not necessarily follow a hierarchy.  Also notable by its absence in the above definition is any reference to {\em aggregation}. In some ways, terms such as {\em hierarchical} and {\em aggregation} can be misleading since the literature has covered instances that cannot easily be depicted in a similar fashion to Figure~\ref{fig:basichier} and or do not involve aggregation. {\bf Include brief summary of all non-traditional hierarchies - e.g. grouped hierarchies, temporal hierarchies with wierd overlapping, problems where we look at differences between variables etc.}  Finally, although Definition~\ref{def:cohspace} makes clear reference to time series, this definition can be easily generalised to any vector-valued data for which some linear constraints are known to hold for all realisations.
	 
	 
	 
	
%	It will sometimes be useful to think of pre-multiplication by $\bm{S}$ as a mapping from $\mathbb{R}^m$ to $\mathbb{R}^n$, in which case we use the notation $s(.)$. Although the codomain of $s(.)$ is $\mathbb{R}^n$, its image is the coherent space $\mathfrak{s}$ as depicted in Figure~\ref{fig2}.
%	
%	\begin{figure}[H]
%		\begin{center}
%			\begin{tikzpicture}[
%			>=stealth,
%			bullet/.style={
%				fill=black,
%				circle,
%				minimum width=1.5cm,
%				inner sep=0pt
%			},
%			projection/.style={
%				->,
%				thick,
%				label,
%				shorten <=2pt,
%				shorten >=2pt
%			},
%			every fit/.style={
%				ellipse,
%				draw,
%				inner sep=0pt
%			}
%			]
%			\node at (2,3) {$s$};
%			\node at (0,5) {$\mathbb{R}^m$(domain of $s$)};
%			\node at (4,5) {$\mathbb{R}^n$(codomain of $s$)};
%			\node at (4.7,2.0) {$\mathfrak{s}$(image of $s$)};
%			%\node[bullet,label=below:$f(x)$] at (4,2.5){};
%			\draw (0,2.5) ellipse (1.02cm and 2.2cm);
%			\draw (4,2.5) ellipse (1.02cm and 2.2cm);
%			\draw (4,2.5) ellipse (0.51cm and 1.1cm);
%			\draw[projection, label=below:$f$] (0.3,2.5) -- (3.8,2.5) ;
%			\end{tikzpicture}
%		\end{center}
%		\caption{The domain, codomain and image of the mapping $s$.}\label{fig2}
%	\end{figure}
	
	\begin{definition}[Coherent Point Forecasts]\label{def:cohpoint}
		Let $\breve{\bm{y}}_{t+h|t} \in \mathbb{R}^n$ be a vector of point forecasts of all series in the hierarchy at time $t+h$, made using information up to and including time $t$. Then $\breve{\bm{y}}_{t+h|t}$ is \emph{coherent} if $\breve{\bm{y}}_{t+h|t} \in \mathfrak{s}$.
	\end{definition}

    Without any loss of generality, that above definition could also be applied to prediction for multivariate data in general, rather than just forecasting of time series.  While the observed data will be coherent by definition, it is important to note that there are a number of reasons why forecasts or predictions may be incoherent.  \todo{some discussion about why reconciliation v single level}    
    
    First, since applications of hierarchical forecasting tend to be very high dimensional a common strategy in practice is to produce forecasts for each time series independently using univariate models.  Second, even where a multivariate model is used for the full vector of observations, it may be difficult to capture the linear constraints inherent in the data particularly for complicated non-linear models.  Third, in some cases judgemental adjustments may be made inducing incoherent forecasts. 
	
	
	
\section{Forecast reconciliation}\label{sec:Reconciliation}
	
	As discussed in the previous section, coherence is not guaranteed for a number of reasons.  To ensure aligned decision making, it is desirable to adjust forecasts ex post to ensure coherence.  This process is referred to as {\em reconciliation}.  In the most general terms, reconciliation can be defined as follows
	
	\begin{definition}[Reconciled forecasts]\label{def:reconpoint}
		Let $\psi$ be a mapping, $\psi:\mathbb{R}^n\rightarrow\mathfrak{s}$.  The point forecast $\tilde{\bm{y}}_{t+h|t}=\psi\left(\hat{\bm{y}}_{t+h|t}\right)$ is said to ``reconcile'' $\hat{\bm{y}}_{t+h|t}$ with respect to the mapping $\psi(.)$
	\end{definition}
	
	All reconciliation methods that we are aware of consider a linear mapping for $\psi$, which involves pre-multiplying base forecasts by an $n\times n$ matrix that has $\mathfrak{s}$ as its image.  One way to achieve this is with a matrix $\bm{SG}$, where $\bm{G}$ is an $(n-m)\times n$ matrix  (some authors use $\bm{P}$ used in place of $\bm{G}$).  This facilitates an interpretation of reconciliation as a two-step process, in the first step, base forecasts $\hat{\bm{y}}_{t+h|t}$ are combined to form a new set of bottom level forecasts, in the second step, these mapped to a full vector of coherent forecasts via pre-multiplication by $\bm{S}$.  
	
	Although pre-multiplying base forecasts by $\bm{SG}$ will result in coherent forecasts, a number of desirable properties arise when $\bm{SG}$ has the specific structure of a {\em projection} matrix onto $\mathfrak{s}$.  In general a projection matrix has the idemoptence property, i.e. $\bm{SG}^2=\bm{SG}$.  However a much more important property of projection matrices, used in multiple instances below, is that any vector lying in the image of the projection will be mapped onto itself by that projection. \todo{perhaps find a reference - it is so fundamental that is is on wikipedia and wolfram mathworld}  In our context this means that for any $\bm{v}\in\mathfrak{s}$, $\bm{SGv}=\bm{v}$.
	
	We begin by considering the special case of an orthogonal projection whereby $\bm{G}=\left(\bm{S}'\bm{S}\right)^{-1}\bm{S}'$.  This is equivalent to so called OLS reconciliation as introduced by \cite{Hyndman2011}.  We refrain from any discussion of regression models focusing instead on geometric interpretations.  However the connection between OLS and orthogonal projection should be clear, in the context of regression modelling predicted values from OLS are obtained via an orthogonal projection of the data onto the span of the regressors.
	
	\subsection{Orthogonal projection}
	
	In this section we discuss two sensible properties that can be achieved by reconciliation via orthogonal projection.  The  first is that reconciliation should adjust the base forecasts as little as possible, i.e. the base and reconciled forecast should be `close'.  The second is that reconciliation in some sense should improve forecast accuracy, or more loosely, that the reconciled forecast should be `closer' to the realisation that is the target of the forecast.  
	
	To address the first of these properties we make the concept of closeness more concrete, by considering the Euclidean distance between the base forecast $\hat{\bm{y}}$ and the reconciled forecast  $\tilde{\bm{y}}$.  A property of an orthogonal projection is that the distance between $\hat{\bm{y}}$ and $\tilde{\bm{y}}$ will be as small as possible while still ensuring that $\tilde{\bm{y}}\in\mathfrak{s}$.  In this sense reconciliation via orthogonal projection leads to the smallest possible adjustments of the base forecasts.
	
	The second property introduced above has been the focus of theoretical results in the forecast reconciliation literature.  He we provide a more streamlined version of proofs by \cite{VanErven2015a} and \cite{Wickramasuriya2017} before providing geometrical intuition aimed at simplifying the reader's understanding of these proofs.
	
	Consider the Euclidean distance between a forecast and the target. This is equivalent to the root of the sum of squared errors over the entire hierarchy. Let $\bm{y}_{t+h}$ be the realisation of the data generating process at time $t+h$, and let $\|\bm{v}\|_2$ be the $L_2$ norm of vector $\bm{v}$. The following theorem shows that reconciliation never increases, and in most cases reduces, the sum of squared errors of point forecasts.
	
	
	\begin{theo}[Distance reducing property]\label{th:distred}
		If $\tilde{\bm{y}}_{t+h|t}=\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t}$, where $\bm{G}$ is such that $\bm{S}\bm{G}$ is an orthogonal projection onto $\mathfrak{s}$, then the following inequality holds:
		\begin{equation}
		\|(\tilde{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|^2_2\le\|(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|^2_2.
		\end{equation}
	\end{theo}
	\begin{proof}
		Since the aggregation constraints must hold for all realisations, $\bm{y}_{t+h}\in\mathfrak{s}$ and $\bm{y}_{t+h}=\bm{S}\bm{G}\bm{y}_{t+h}$ since $\bm{S}\bm{G}$ is a projection onto $\mathfrak{s}$. Therefore,
		\begin{align}
		\|(\tilde{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|_2&=\|(\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t}-\bm{S}\bm{G}\bm{y}_{t+h})\|_2\\
		&=\|\bm{S}\bm{G}(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|_2.
		\end{align}
		The Cauchy-Schwarz inequality can be used to show that orthogonal projections are bounded operators \citep{Hun2001}, therefore
		\begin{equation*}
		\|\bm{S}\bm{G}(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|_2\le
		\|(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|_2.
		\end{equation*}
	\end{proof}
	The inequality is strict whenever $\hat{\bm{y}}_{t+h|t}\notin\mathfrak{s}$.
	
	\todo{compare to van Erven and Wickremasuriya more explictly}
	
	The simple geometric intuition behind the proof is demonstrated in Figure~\ref{fig:Schematic_OLSRecon}.  In this schematic, the coherent subspace is depicted as a black arrow.  The base forecast $\hat{\bm{y}}$ is shown as a blue dot.  Since it is incoherent it does not lie in $\mathfrak{s}$.  Reconciliation is an orthogonal projection from $\hat{\bm{y}}$ to the coherent subspace yielding the reconciled forecast $\tilde{\bm{y}}$ shown in red.  Finally, the target of the forecast $\bm{y}$ is displayed as a black point, and although its exact location is unknown to the forecaster, it is known that it will lie somewhere along the coherent subspace.
	
	\begin{figure}[H]
		\centering
		\vspace{-0.9cm}
		\tiny
		\resizebox{\linewidth}{!}{\input{Figs/orth_pointforerec_schematic}}
		\label{fig:Schematic_OLSRecon}
		\caption{$\hat{\bm{y}}$ is orthogonally projected onto $\mathfrak{s}$}
		
	\end{figure}
	
	
	Figure~\ref{fig:Schematic_OLSRecon} clearly shows that $\hat{\bm{y}}$, $\tilde{\bm{y}}$ and $\bm{y}$ form a right angled triangle.  In this triangle the distance between $\bm{y}$ and $\hat{\bm{y}}$ is the hypotenuse and therefore must be longer than the distance between $\bm{y}$ and $\tilde{\bm{y}}$.  As such reconciliation is guaranteed to reduce the squared error of the forecast.  
	
    Theorem~\ref{th:distred} is in some ways more powerful than perhaps previously understood.  It is often stated in terms of expectations\todo{check other proofs}.  However, the distance reducing property result is even stronger since it will hold for any realisation and any forecast.  Nothing needs to be assumed about the statistical properties of the data generating process or the process by which forecasts are made.  
    f
    However, in other ways, Theorem~\ref{th:distred} is weaker than perhaps often understood. First, when improvements in forecast accuracy are discussed in the context of the theorem, this refers to a very specific measure of forecast accuracy.  In particular, this measure is the root of the sum of squared errors of {\em all} variables in the hierarchy.  As such, while forecast improvement is guaranteed for the hierarchy overall, reconciliation can lead to worse forecasts for individual series. Second, although orthogonal projections are guaranteed to improve on base forecasts both for all realisations and in expectation, they are not necessarily the projection that leads to the greatest improvement in forecast accuracy.  As such referring to reconciliation via orthogonal projections as `optimal' is somewhat misleading since it does not have the optimality properties of some oblique projections, in particular MinT. It is to oblique projections that we now turn our attention.
    	
	\subsection{Oblique Projections}
	
	One justification for using an orthogonal projection is that it leads to improved forecast accuracy in terms of the root of the sum of squared errors of {\em all} variables in the hierarchy.  A clear shortcoming of this measure of forecast accuracy is that forecasts errors in all series should not necessarily be treated equally.  For example, in hierarchies, top-level series tend to have a much larger scale than bottom level series.  Even when two series are on a similar scale, series that are more predictable or less variable will tend to be downweighted by simply aggregating square errors.  An even more sophisticated understanding may take the correlation between series into account.  All of these considerations lead towards reconciliation of the form $\tilde{\bm{y}}=\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}\hat{\bm{y}}$, where $\bm{W}$ is a symmetric matrix.  Generally, it is assumed that $\bm{W}$ is invertible, otherwise a pseudo inverse can be used.
	
	It should be noted that  $\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}$ is an oblique, rather than an orthogonal projection matrix.  However this matrix can be considered to be an orthogonal projection for a different geometry, namely the generalised Euclidean geometry with respect to $\bm{W}^{-1}$.  One way to understand this geometry is that it is the same as Euclidean geometry when all vectors are first transformed by pre-multiplying by $\bm{W}^{-1/2}$.  This leads to a transformed $\bm{S}$ matrix $\bm{S}^*=\bm{W}^{-1/2}\bm{S}$ and transformed $\hat{\bm{y}}$ and $\tilde{\bm{y}}$ vectors $\hat{\bm{y}}^*=\bm{W}^{-1/2}\hat{\bm{y}}$ and $\tilde{\bm{y}}^*=\bm{W}^{-1/2}\tilde{\bm{y}}$.  The transformed reconciled forecast results from an orthogonal projection in the transformed space since 
	
	\begin{align}
	\tilde{\bm{y}}^*&=\bm{W}^{-1/2}\tilde{\bm{y}}\\&=\bm{W}^{-1/2}\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}\hat{\bm{y}}
	\\&=\bm{S}^*\left(\bm{S}^{*'}\bm{S}^*\right)^{-1}\bm{S}^{*'}\hat{\bm{y}^*}
	\end{align}
	
	Another way of understanding the generalised Euclidean geometry is that it is defined by the norm $\bm{v}'\bm{W}^{1}\bm{v}$.  This interpretation is quite instructive when it comes to thinking about the connection between distances and loss functions.  In the generalised Euclidean geometry, the distance between the reconciled forecast and the realisation is given by $(\hat{\bm{y}}-\bm{y})'\bm{W}^{-1}(\hat{\bm{y}}-\bm{y})$.  For diagonal $\bm{W}$ this is equivalent to a weighted sum of squared error loss function and when $\bm{W}$ is a covariance matrix this is equivalent to a Mahalanobis distance.  As such Theorem~\ref{th:distred} can easily be generalised.  If the objective function is some weighted sum of squared errors, or a Mahalanobis distance, then the projection matrix $\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}$ is guaranteed to improve forecast accuracy over base forecasts, for an appropriately selected $\bm{W}$.
	
	\subsection{MinT}
	
	While the properties discussed so far hold for any projection matrix, the MinT method of \cite{Wickramasuriya2017} has an additional optimality property.  \cite{Wickramasuriya2017} show that for unbiased base forecasts, the trace of the forecast error covariance matrix of reconciled forecasts is minimised by an oblique projection with a particular choice of $\bm{W}$.  This choice is that $\bm{W}$ should be the forecast error covariance matrix where errors come from using the base forecasts.  Although the base forecast error covariance matrix is unknown, it can be estimated using in-sample errors.
	
	Figure~\ref{fig:MinT_justification1} provides geometrical intuition into the MinT method.  Suppose the in-sample errors are given by the orange points.  They provide information on the most likely direction of large deviations from the coherent subspace.  This direction is denoted by $\bm{R}$.  Figure~\ref{fig:MinT_justification2} then shows a target value of $\bm{y}$, while the grey points indicate possible values for the base forecasts.  One such forecast is depicted in blue as $\hat{\bm{y}}$.  An oblique projection of the blue point back along the direction of $\bm{R}$ yields a reconciled forecast closer to the target, especially compared to an orthogonal projection.  Imagining a similar oblique projection along $\bm{R}$ for all the grey points yield reconciled forecasts tightly packed near the target $\bm{y}$.  In this sense, the oblique MinT projection minimises the forecast error variance of reconciled forecasts.\todo{needs picture with all points projected obliquely then orthogonally}.
	
	\begin{figure}[H]
		\centering
		%\vspace{-0.9cm}
		\small
		\resizebox{\linewidth}{!}{
			\input{Figs/insampledir}
		}
		\caption{A schematic to represent MinT reconciliation. Points in orange colour represent the insample errors. $\bm{R}$ shows the most likely direction of deviations from the coherent subspace. $\hat{\bm{y}}$ is projected onto $\mathfrak{s}$ along the the direction of $\bm{R}$.}\label{fig:MinT_justification1}
	\end{figure}
		
	\begin{figure}[H]
		\centering
		%\vspace{-0.9cm}
		\small
		\resizebox{\linewidth}{!}{
			\input{Figs/oblique_justification2}
		}
		\caption{A schematic to represent MinT reconciliation. Grey points indicate potential realisations of the base forecast while the blue dot indicates one such realisation. The black dot ${\bm y}$ denotes the (unknown) target of the forecast.}\label{fig:MinT_justification2}
	\end{figure}
	
	\section{Bias in forecast reconciliation}
	
	Before turning our attention to the issue of bias itself it is important to state a sensible property that any reconciliation method should have.  That is if base forecasts are already coherent then reconciliation should not change the forecast.  As stated in Section~\ref{sec:Reconciliation}, this property holds when $\bm{SG}$ is a projection matrix.  This implies for arbitrary $\bm{G}$, reconciliation may in fact change an already coherent forecast.  
	
	The property that projections map all vectors in the coherent subspace onto themselves is useful in proving the unbiasedness preserving property of reconciliation \todo{reference Shanika and maybe van Erven Culigari}.  Before restating this proof using a  clear geometric interpretation we discuss in a precise fashion what is meant by unbiasedness.  
	
	Suppose that the target of a point forecast is $\bm{\mu}_{t+h|t}:=\E(\bm{y}_{t+h}\mid\bm{y}_{1},\dots,\bm{y}_{t})$ where the expectation is taken over the predictive density.  Our point forecast can be thought of as an estimate of this quantity.  The forecast is random due to uncertainty in the training sample and it is with respect to this uncertainty that unbiasedness refers.  More concretely, the point forecast will be unbiased if $\E_{1:t}(\hat{\bm{y}}_{t+h|t})=\bm{\mu}_{t+h|t}$, where the subscript $1:t$ denotes an expectation taken over the training sample.
	
	\begin{theo}[Unbiasedness preserving property]
		For unbiased $\hat{\bm{y}}_{t+h|t}$, the reconciled point forecast is also an unbiased prediction as long as $\bm{SG}$ is a projection onto $\mathfrak{s}$.
	\end{theo}
	\begin{proof}
		The expected value of the reconciled forecast is given by
		\[
		\E_{1:t}(\tilde{\bm{y}}_{t+h|t})
		= \E_{1:t}(\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t})
		= \bm{S}\bm{G}\E_{1:t}(\hat{\bm{y}}_{t+h|t})
		= \bm{S}\bm{G}\bm{\mu}_{t+h|t}.
		\]
		Since $\bm{\mu}_{t+h|t}$ is an expectation taken with respect to the degenerate predictive density it must lie in $\mathfrak{s}$. We have already established that when $\bm{S}\bm{G}$ is a projection onto $\mathfrak{s}$ then it maps all vectors in $\mathfrak{s}$ onto themselves. As such $\bm{S}\bm{G}\bm{\mu}_{t+h|t}=\bm{\mu}_{t+h|t}$ when $\bm{S}\bm{G}$ is a projection matrix.
	\end{proof}

	We note that the above result holds when the projection $\bm{SG}$ is only onto the coherent subspace $\mathfrak{s}$ and not for all projection matrices in general. To describe this more explicitly suppose $\bm{SG}$ has as its image  $\mathfrak{L}$ which is itself a a lower dimensional linear subspace of $\mathfrak{s}$, i.e. $\mathfrak{L}\subset\mathfrak{s}$. Then for $\left\{\bm{\mu}_{t+h|t}:\bm{\mu}_{t+h|t}\in\mathfrak{s},\bm{\mu}_{t+h|t}\notin\mathfrak{L}\right\}$,  $\bm{S}\bm{G}\bm{\mu}_{t+h|t} \ne \bm{\mu}_{t+h|t}$. This is depicted in Figure~\ref{fig:Schematic_3D} where $\bm{\mu}_{t+h|t}$ is projected to a point $\bar{\bm{\mu}}$ in $\mathfrak{L}$.  Therefore in this case, the reconciled forecast will have as its expectation $\bar{\bm{\mu}}$ rather than $\bm{\mu}_{t+h|t}$ and be biased.  This result has implications in practice, in particular, the top-down method \citep{Gross1990} has 
	\begin{equation}\label{eq:top-downG}
	\bm{G}=\begin{pmatrix}
	\bm{p} & \bm{0}_{(m \times n-1)}
	\end{pmatrix}
	\end{equation}
    where $\bm{p} = (p_1,\dots,p_m)'$ is an $m$-dimensional vector consisting a set of proportions which is use to disaggregate the top-level forecasts along the hierarchy.  In this case it can be verified that $\bm{SG}$ is idempotent, i,e. $\bm{SGSG}=\bm{SG}$ and therefore $\bm{SG}$ is a projection matrix.  However the image of this projection is not an $m$-dimensional subspace but a $1$-dimensional subspace.  As such, top-down reconciliation will bias base forecasts when those base forecasts are unbiased.
		
	\begin{figure}[H]
		\centering
		\vspace{-0.9cm}
		\small
		\resizebox{\linewidth}{!}{
			\input{Figs/Schem_3D}
		}
		\caption{$\mathfrak{L}$ is a linear subspace of the coherent subspace $\mathfrak{s}$. If $s\circ g$ is a projection not onto $\mathfrak{s}$ but onto $\mathfrak{L}$, then $\bm{\mu} \in \mathfrak{s}$ will be moved to $\bar{\bm{\mu}} \in \mathfrak{L}$.}\label{fig:Schematic_3D}
	\end{figure}
	
	Finally, it is often stated that an assumption required to prove the unbiasedness preserving property is that $\bm{SGS}=\bm{S}$ or alternatively that $\bm{GS}=\bm{I}$.  Both of these conditions are equivalent to assuming that $\bm{SG}$ is a projection matrix.  \todo{perhaps elaborate in a proof in appendix}  When the problem is viewed through the prism of imposing a constraint $\bm{GS}=\bm{I}$ to ensure unbiasedness is preserved, one may be tempted to deal with biased forecasts by selecting $\bm{G}$ is an unconstrained manner.  However, equipped with a geometric understanding of the problem, we would advise against this approach.  Our own solution to dealing with biased forecasts is discussed in detail in the next section.  
	
	
%	Representation of a coherent subspace in a three dimensional hierarchy where $y_{\text{Tot}} = y_{\text{A}} + y_{\text{B}}$. The coherent subspace is depicted as a gray two dimensional plane labelled $\mathfrak{s}$. Note that the columns of $\vec{s}_1 = (1,1,0)'$ and $\vec{s}_2 = (1, 0, 1)'$ form a basis for $\mathfrak{s}$. The red points lying on $\mathfrak{s}$ can be either realisations or coherent forecasts.
	

	
	
	\section{Bias correction}
	
	\section{Application}
	
	\section{Conclusions}
	
	\newpage
	
	\bibliographystyle{agsm}
	
	\bibliography{References_paper1}
	
\end{document}

