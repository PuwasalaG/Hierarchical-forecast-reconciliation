\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{floatpag}
\usepackage{lipsum}
\usepackage{amssymb, qtree, bm, multirow, textcmds, siunitx,paralist}
\usepackage{mathrsfs, float, booktabs,todonotes,amsthm, xcolor,sidenotes, caption, subcaption}

\usepackage[bb=boondox]{mathalfa}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,shapes,fit,calc}
\usepackage{amsfonts}

\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

\DeclareMathOperator*{\argmin}{arg\,min}
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\def\mathbi#1{\textit{ #1}}
\def\mathB#1{\textbf{ #1}}
\def\E{\text{E}}
\def\var{\text{Var}}

\def\PQ{\begin{pmatrix}\bm{G}\\[-0.2cm]\bm{H}\end{pmatrix}}
\def\bt{\begin{pmatrix}\tilde{\bm{b}}\\[-0.2cm]\tilde{\bm{a}}\end{pmatrix}}

%\theoremstyle{theo}
\newtheorem{theo}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{property}
\newtheorem{property}{Property}[section]


\begin{document}
	
	
	%\bibliographystyle{natbib}
	
	\def\spacingset#1{\renewcommand{\baselinestretch}%
		{#1}\small\normalsize} \spacingset{1}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\if1\blind
	{
		\title{\bf Forecasts Reconciliation: A geometric view with new insights on bias correction.}
		\author{Puwasala Gamakumara\thanks{
				The authors gratefully acknowledge the support of Australian Research Council Grant DP140103220.  We also thank Professor Mervyn Silvapulle for valuable comments.}\hspace{.2cm}\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3800, Australia.\\
			Email: Puwasala.Gamakumara@monash.edu \\
			and \\
			Anastasios Panagiotelis\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3800, Australia.\\
			Email: Anastasios.Panagiotelis@monash.edu \\
			and \\
			George Athanasopoulos\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3800, Australia.\\
			Email: george.athanasopoulos@monash.edu \\
			and \\
			Rob J Hyndman\\
			Department of Econometrics and Business Statistics,\\
			Monash University,\\ VIC 3800, Australia.\\
			Email: rob.hyndman@monash.edu \\}
		\maketitle
	} \fi
	
	\if0\blind
	{
		\bigskip
		\bigskip
		\bigskip
		\begin{center}
			{\LARGE\bf Hierarchical Forecasts Reconciliation}
		\end{center}
		\medskip
	} \fi
	
	\bigskip
	
	
	\begin{abstract} TBC
		%  Forecast reconciliation involves adjusting forecasts to ensure coherence with aggregation constraints. We extend this concept from point forecasts to probabilistic forecasts by redefining forecast reconciliation in terms of linear functions in general, and projections more specifically. New theorems establish that the true predictive distribution can be recovered in the elliptical case by linear reconciliation, and general conditions are derived for when this is a projection. A geometric interpretation is also used to prove two new theoretical results for point forecasting; that reconciliation via projection both preserves unbiasedness and dominates unreconciled forecasts in a mean squared error sense. Strategies for forecast evaluation based on scoring rules are discussed, and it is shown that the popular log score is an improper scoring rule with respect to the class of unreconciled forecasts when the true predictive distribution coheres with aggregation constraints. Finally, evidence from a simulation study shows that reconciliation based on an oblique projection, derived from the MinT method of \citet{Wickramasuriya2017} for point forecasting, outperforms both reconciled and unreconciled alternatives.
	\end{abstract}
	
%	\noindent%
%	{\it Keywords:}  Forecast Reconciliation, Projections, Elliptical Distributions, Scoring Rules, High-dimensional Time Series.
%	\vfill
	
	\newpage
	\spacingset{1.45} % DON'T change the spacing!
	
	\section{Introduction}\label{sec:intro}
	
	The past decade has seen rapid development in methodologies for forecasting time series that follow a hierarchical aggregation structure.  Of particular prominence have been {\em forecast reconciliation} methods involving two steps; first separate forecasts are produced for all series, then these are adjusted ex post to ensure coherence with aggregation constraints.  Forecast reconciliation has mostly been formulated using a regression model, see \cite{Hyndman2011} and \cite{WicEtAl2019} for examples.  This setup can be counter-intuitive since a vector comprised of forecasts from different time series models is also assumed to be the dependent variable in a regression model.  In this paper, we eschew a regression interpretation in favour of a novel, geometric understanding of forecast reconciliation.  This allows us to develop novel proofs and a clearer understanding of the interplay between forecast bias and reconciliation methods. 
	
	Multivariate time series following an aggregation structure arise in many disciplines such as manufacturing, engineering, marketing and medicine\todo{include references}. Forecasts of these series should adhere to aggregation constraints to ensure aligned decision making. Earlier studies achieved this by only forecasting a single level of the hierarchy and then either aggregating in a bottom up fashion \citep{Dunn1976} or disaggregating in a top-down fashion \citep{Gross1990, Athanasopoulos2009}.  For reviews of these approaches including a discussion of their advantages and disadvantages see \citet{Schwarzkopf1988, Kahn1998, Lapide1998, Fliedner2001}. 
	
	In contrast to these methods, \cite{Hyndman2011} proposed forecasting all series in the hierarchy, referring to these as {\em base} forecasts.  Since base forecasts were produced independently they were not guaranteed to adhere to aggregation constraints and could thus be improved via further adjustment.  A framework was proposed whereby the base forecasts were assumed to follow a regression model.  The predicted values from this model were guaranteed to adhere to the linear constraints by construction and could thus be used as a new set of forecasts.  This approach and later modifications have subsequently been shown to outperform bottom up and top down approaches in a variety of empirical settings\todo{references}. 
	
	Some theoretical insight into the performance of forecast reconciliation methods has been provided by \cite{VanErven2015a} and \cite{WicEtAl2019}.  Both papers provide a proof that reconciliation is guaranteed to improve base forecasts.  The latter paper also proposes a particular version of reconciliation known as the Minimum Trace (MinT) method.  This is optimal in the sense of minimising the trace of reconciled forecast error covariance matrix under the assumption that base forecasts are unbiased.
	
	Our main contribution is to propose a geometric interpretation of the entire hierarchical forecasting problem.  In this setting, we show that reconciled forecasts will have a number of attractive properties when they are obtained via projections.  We believe that this is clearer and more intuitive than explanations based on regression modelling, notwithstanding the fact that regression based-methods themselves are indeed projections.  As such, this paper is in part a review of existing results cast in a new light, but one that we believe to be warranted as forecast reconciliation methodologies have become more popular.  In addition, we also propose three major and novel results.
	
	First, our approach makes it clear that the defining characteristic of so-called {\em hierarchical time series} is not aggregation but linear constraints.  As a result forecast reconciliation can be applied in contexts where there are no clear candidates of {\em bottom level} series, an insight that is not apparent when the problem is viewed through the lens of regression modelling.  Second, we provide a new proof that reconciled forecasts dominate unreconciled forecasts which makes explicit the link between a reconciliation method and a loss function.  We believe that this link is lacking in previous work that attempts to establish similar results, in particular \cite{VanErven2015a} and \cite{WicEtAl2019}.  Futhermore, unlike \cite{VanErven2015a} and \cite{WicEtAl2019} our proof does not require an assumption about convexity.  Third, we revisit the issue of bias.  We prove that reconciliation using certain projection matrices guarantees unbiased reconciled forecasts as long as base forecasts are also unbiased.  A natural question that arises is what to do in the case of biased reconciled forecasts.  Rather than addressing this issue by considering matrices that are not projections, we propose to bias-correct before reconciliation.  This is evaluated in an extensive empirical study where we find that even when bias correction fails, the extent of the problem is mitigated by reconciling forecasts.
	     
	
	The remainder of this paper is structured as follows. Section~\ref{sec:CoheForecasts} deals with the concept of coherence and defines so called hierarchical time series in a way that does not depend on any notion of bottom level series.  Section~\ref{sec:Reconciliation} defines forecast reconciliation in terms of projections and includes a proof that reconciled forecasts dominate base forecasts with respect to a specific loss function. In Section~\ref{sec:BiasInRecon} we prove the unbiasedness preserving property of reconciliation via certain projection matrices and propose methods for bias correction. In Section \ref{sec:EmpStudy} we evaluate these methods for in an extensive emprirical application to domestic tourism flow in Australia.  Section~\ref{sec:conclusions} concludes with some discussion and thoughts on the  future of research in forecast reconciliation.  
\section{Coherent forecasts}\label{sec:CoheForecasts}
	
	\subsection{Notation and preliminaries}\label{sec:notation}
	
    We briefly define the concept of a \emph{hierarchical time series} in a fashion similar to \cite{WicEtAl2019}, \cite{FPP2018} and others, before elaborating on some of the limitations of this understanding.  A \emph{hierarchical time series} is a collection of $n$ variables indexed by time, where some variables are aggregates of other variables. We let $\bm{y}_t \in \mathbb{R}^n$ be a vector comprising observations of all variables in the hierarchy at time $t$. The \emph{bottom-level series} are defined as those $m$ variables that cannot be formed as aggregates of other variables; we let $\bm{b}_t \in \mathbb{R}^m$ be a vector comprised of observations of all bottom-level series at time $t$.  The hierarchical structure of the data implies that the following holds for all $t$
    
    \begin{equation}
    \bm{y}_t = \bm{Sb}_t,
    \end{equation}
    where $\bm{S}$ is an $n \times m$ constant matrix that encodes the aggregation constraints.  
	
		\begin{figure}[H]
			\begin{center}
				\leaf{AA} \leaf{AB}
				\branch{2}{A}
				\leaf{BA} \leaf{BB}
				\branch{2}{B}
				\branch{2}{Tot}
				\qobitree
			\end{center}
			\caption{An example of a two level hierarchical structure.}\label{fig:basichier}
		\end{figure}
	
	To clarify these concepts consider the example of the hierarchy in Figure~\ref{fig:basichier}.  For this hierarchy, $n=7$, $\bm{y}_t = [y_{Tot,t},y_{A,t}, y_{B,t},y_{C,t},y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}]'$, $m=4$, $\bm{b}_t = [y_{AA,t}, y_{AB,t}, y_{BA,t}, y_{BB,t}]'$ and
	\[
	\bm{S} = \begin{pmatrix}
	1 & 1 & 1 & 1  \\
	1 & 1 & 0 & 0 \\
	0 & 0 & 1 & 1 \\
	& \multicolumn{2}{c}{\bm{I}_4} &
	\end{pmatrix},
	\]
	where $\bm{I}_4$ is the $4\times 4$ identity matrix.
	
	While such a definition is completely serviceable, it obscures the full generality of the literature on so-called hierarchical time series.  In fact, concepts such as coherence and reconciliation, defined in full below, only require the data to have two important characteristics; the first is that they are multivariate, the second is that they adhere to linear constraints.  
	
	\subsection{Coherence}\label{sec:cohpointf}
	
	The property that data adhere to some linear constraints is referred to as {\em coherence}.  We now provide definitions aimed at providing geometric intuition of hierarchical time series.
	
	 \begin{definition}[Coherent subspace]\label{def:cohspace}
	 	The $m$-dimensional linear subspace $\mathfrak{s}\subset \mathbb{R}^n$ for which a set of linear constraints holds for all $\bm{y}\in\mathfrak{s}$ is defined as the \emph{coherent subspace}.
	 \end{definition}
 
     To further illustrate, Figure \ref{fig:3D_hierarchy} depicts the most simple three variable hierarchy where $y_{Tot,t}=y_{A,t}+y_{B,t}$.  The coherent subspace is depicted as a grey $2$-dimensional plane within $3$-dimensional space, i.e. $m=2$ and $n=3$.  It is worth noting that the coherent subspace is spanned by the columns of $\bm{S}$, i.e.\ $\mathfrak{s}=\text{span}(\bm{S})$.  In Figure~\ref{fig:3D_hierarchy}, these columns are $\vec{s}_1=(1,1,0)'$ and $\vec{s}_2=(1,0,1)'$.  However, it is equally important to recognise that the hierarchy could also have been defined in terms of $y_{Tot,t}$ and $y_{A,t}$ rather than the bottom level series, $y_{A,t}$ and $y_{B,t}$. In this case the corresponding `$\bm{S}$ matrix' would have columns $(1,0,1)'$ and $(0,1,-1)'$.  However, while there are multiple ways to define an $\bm{S}$ matrix, in all cases the columns will span the same coherent subspace, which is unique.
     
     \begin{figure}[H]
     	\centering
     	\vspace{-0.9cm}
     	\small
     	\resizebox{\linewidth}{!}{
     		\input{Figs/3D_hierarchy}
     	}
     	\caption{Depiction of a three dimensional hierarchy with $y_{\text{Tot}} = y_{\text{A}} + y_{\text{B}}$. The gray coloured two dimensional plane depicts the coherent subspace $\mathfrak{s}$ where $\vec{s}_1 = (1,1,0)'$ and $\vec{s}_2 = (1, 0, 1)'$ are basis vectors that spans $\mathfrak{s}$. The red points in $\mathfrak{s}$ represent realisations or coherent forecasts}\label{fig:3D_hierarchy}
     \end{figure}
     
	 \begin{definition}[Hierarchical Time Series]\label{def:cohspace}
	 	A hierarchical time series is an $n$-dimensional multivariate time series such that all observed values $\bm{y}_1,\ldots,\bm{y}_T$ and all future values $\bm{y}_{T+1},\bm{y}_{T+2},\ldots$  lie in the coherent subspace, i.e. $\bm{y}_t\in\mathfrak{s}\quad\forall t$.
	 \end{definition}
	 
	 Despite the common use of the term {\em hierarchical time series}, it should be clear from the definition that the data need not necessarily follow a hierarchy.  Also notable by its absence in the above definition is any reference to {\em aggregation}. In some ways, terms such as {\em hierarchical} and {\em aggregation} can be misleading since the literature has covered instances that cannot be depicted in a similar fashion to Figure~\ref{fig:basichier} and/or do not involve aggregation. {\bf Include brief summary of all non-traditional hierarchies - e.g. grouped hierarchies, temporal hierarchies with weird overlapping, problems where we look at differences between variables etc.}  Finally, although Definition~\ref{def:cohspace} makes reference to time series, this definition can be easily generalised to any vector-valued data for which some linear constraints are known to hold for all realisations.
	 
	 
	 
	
%	It will sometimes be useful to think of pre-multiplication by $\bm{S}$ as a mapping from $\mathbb{R}^m$ to $\mathbb{R}^n$, in which case we use the notation $s(.)$. Although the codomain of $s(.)$ is $\mathbb{R}^n$, its image is the coherent space $\mathfrak{s}$ as depicted in Figure~\ref{fig2}.
%	
%	\begin{figure}[H]
%		\begin{center}
%			\begin{tikzpicture}[
%			>=stealth,
%			bullet/.style={
%				fill=black,
%				circle,
%				minimum width=1.5cm,
%				inner sep=0pt
%			},
%			projection/.style={
%				->,
%				thick,
%				label,
%				shorten <=2pt,
%				shorten >=2pt
%			},
%			every fit/.style={
%				ellipse,
%				draw,
%				inner sep=0pt
%			}
%			]
%			\node at (2,3) {$s$};
%			\node at (0,5) {$\mathbb{R}^m$(domain of $s$)};
%			\node at (4,5) {$\mathbb{R}^n$(codomain of $s$)};
%			\node at (4.7,2.0) {$\mathfrak{s}$(image of $s$)};
%			%\node[bullet,label=below:$f(x)$] at (4,2.5){};
%			\draw (0,2.5) ellipse (1.02cm and 2.2cm);
%			\draw (4,2.5) ellipse (1.02cm and 2.2cm);
%			\draw (4,2.5) ellipse (0.51cm and 1.1cm);
%			\draw[projection, label=below:$f$] (0.3,2.5) -- (3.8,2.5) ;
%			\end{tikzpicture}
%		\end{center}
%		\caption{The domain, codomain and image of the mapping $s$.}\label{fig2}
%	\end{figure}
	
	\begin{definition}[Coherent Point Forecasts]\label{def:cohpoint}
		Let $\breve{\bm{y}}_{t+h|t} \in \mathbb{R}^n$ be a vector of point forecasts of all series in the hierarchy where the subscript $t+h|h$ implies that the forecast is made as time $t$ for a period $h$ steps into the future. Then $\breve{\bm{y}}_{t+h|t}$ is \emph{coherent} if $\breve{\bm{y}}_{t+h|t} \in \mathfrak{s}$.
	\end{definition}

    Without any loss of generality, the above definition could also be applied to prediction for multivariate data in general, rather than just forecasting of time series.  Much of the early literature that dealt with the problem of forecasting hierarchical time series \citep[see][and references therein]{Gross1990} produced forecasts at a single level of the hierarchy in the first stage. Subsequently forecasts for all series were recovered through aggregation, disaggregation according to historical or forecast proportions or some combination of both.  As such incoherent forecasts were not a problem in these earlier papers.  
    
    Forecasting a single level of the hierarchy did not, however echo common practice within many industries.\todo{might need refs}  In many organisations different departments or `silos' each produced their own forecasts, often with their own information sets and judgemental adjustments.  This approach does have several advantages over only forecasting a single level.  First, there is no loss of information since all bottom levels are modelled.  Second, modelling top level series often identifies features such as trend and seasonality that cannot be detected in noisy disaggregate data.  Unfortunately, however, when forecasts are produced independently at all levels, forecasts are likely to be incoherent.  This problem of incoherent forecasts cannot in general be solved by multivariate modelling either.  Instead, the solution to incoherent forecasts is to make an ex post adjustment that ensures coherence, a process known as {\em forecast reconciliation}
	
	
	
\section{Forecast reconciliation}\label{sec:Reconciliation}
	
	The problem of forecast reconciliation is predicated on there being an $n$-vector of forecasts that are incoherent.  We will call these {\em base forecasts} and denote them as $\hat{\bm{y}}_{t+h|h}$.  In the sequel, this subscript will be dropped at times for ease of exposition.  In the most general terms, reconciliation can be defined as follows
	
	\begin{definition}[Reconciled forecasts]\label{def:reconpoint}
		Let $\psi$ be a mapping, $\psi:\mathbb{R}^n\rightarrow\mathfrak{s}$.  The point forecast $\tilde{\bm{y}}_{t+h|t}=\psi\left(\hat{\bm{y}}_{t+h|t}\right)$ is said to ``reconcile'' a base forecast $\hat{\bm{y}}_{t+h|t}$ with respect to the mapping $\psi(.)$
	\end{definition}
	
	All reconciliation methods that we are aware of consider a linear mapping for $\psi$, which involves pre-multiplying base forecasts by an $n\times n$ matrix that has $\mathfrak{s}$ as its image.  One way to achieve this is with a matrix $\bm{SG}$, where $\bm{G}$ is an $(n-m)\times n$ matrix  (some authors use $\bm{P}$ used in place of $\bm{G}$).  This facilitates an interpretation of reconciliation as a two-step process, in the first step, base forecasts $\hat{\bm{y}}_{t+h|t}$ are combined to form a new set of bottom level forecasts, in the second step, these mapped to a full vector of coherent forecasts via pre-multiplication by $\bm{S}$.  
	
	Although pre-multiplying base forecasts by $\bm{SG}$ will result in coherent forecasts, a number of desirable properties arise when $\bm{SG}$ has the specific structure of a {\em projection} matrix onto $\mathfrak{s}$.  In general a projection matrix is defined via the idemoptence property, i.e. $\bm{SG}^2=\bm{SG}$.  However a much more important property of projection matrices, used in multiple instances below, is that any vector lying in the image of the projection will be mapped to itself by that projection \citep[see Lemma 2.4 in][for a proof]{rao1974}. In our context this implies that for any $\bm{v}\in\mathfrak{s}$, $\bm{SGv}=\bm{v}$.
	
	We begin by considering the special case of an orthogonal projection whereby $\bm{G}=\left(\bm{S}'\bm{S}\right)^{-1}\bm{S}'$.  This is equivalent to so called OLS reconciliation as introduced by \cite{Hyndman2011}.  We refrain from any discussion of regression models focusing instead on geometric interpretations.  However the connection between OLS and orthogonal projection should be clear, in the context of regression modelling predicted values from OLS are obtained via an orthogonal projection of the response onto the span of the regressors.
	
	\subsection{Orthogonal projection}
	
	In this section we discuss two sensible properties that can be achieved by reconciliation via orthogonal projection.  The  first is that reconciliation should adjust the base forecasts as little as possible, i.e. the base and reconciled forecast should be `close'.  The second is that reconciliation in some sense should improve forecast accuracy, or more loosely, that the reconciled forecast should be `closer' to the realised value targeted by the forecast.  
	
	To address the first of these properties we make the concept of closeness more concrete, by considering the Euclidean distance between the base forecast $\hat{\bm{y}}$ and the reconciled forecast  $\tilde{\bm{y}}$.  A property of an orthogonal projection is that the distance between $\hat{\bm{y}}$ and $\tilde{\bm{y}}$ is minimal for over any possible $\tilde{\bm{y}}\in\mathfrak{s}$.  In this sense reconciliation via orthogonal projection leads to the smallest possible adjustments of the base forecasts.
	
	The property that reconciliation should improve forecasts was touched upon in Section 2.3 of \cite{WicEtAl2019}.  The discussion in that paper focuses on the case of MinT. Here we provide a new explicit proof of that result.  We do so first in the case of an orthogonal projection where the geometric intuition of the proof is clear and then generalise the result to reconciliation using any projection matrix in Section~\ref{sec:oblique}.
		
	Consider the Euclidean distance between a forecast and the target. This is equivalent to the root of the sum of squared errors over the entire hierarchy. Let $\bm{y}_{t+h}$ be the realisation of the data generating process at time $t+h$. The following theorem shows that reconciliation never increases, and in most cases reduces, the sum of squared errors of point forecasts.
	
	
	\begin{theo}[Distance reducing property]\label{th:distred}
		If $\tilde{\bm{y}}_{t+h|t}=\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t}$, where $\bm{G}$ is such that $\bm{S}\bm{G}$ is an orthogonal (in the Euclidean sense) projection onto $\mathfrak{s}$ and let $\|\bm{v}\|$ be the $L_2$ norm (in the Euclidean sense) of vector $\bm{v}$ then:
		\begin{equation}
		\|(\tilde{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|\le\|(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|.
		\end{equation}
	\end{theo}
	\begin{proof}
		Since, $\bm{y}_{t+h},\tilde{\bm{y}}_{t+h}\in\mathfrak{s}$ and since the projection is orthogonal, by Pythagoras' theorem
		\begin{equation}
		\|(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|^2=\|(\tilde{\bm{y}}_{t+h|t}-\hat{\bm{y}}_{t+h})\|^2+\|(\tilde{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|^2.
		\end{equation}
		
		Since $\|(\tilde{\bm{y}}_{t+h|t}-\hat{\bm{y}}_{t+h})\|^2\ge 0$ this implies,
		\begin{equation}
		\|(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|^2\ge\|(\tilde{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|^2.
		\end{equation}
		with equality only holding when $\tilde{\bm{y}}_{t+h|t}=\hat{\bm{y}}_{t+h}$.  Taking the square root of both sides proves the desired result.
	\end{proof}
		
	The simple geometric intuition behind the proof is demonstrated in Figure~\ref{fig:Schematic_OLSRecon}.  In this schematic, the coherent subspace is depicted as a black arrow.  The base forecast $\hat{\bm{y}}$ is shown as a blue dot.  Since $\hat{\bm{y}}$ is incoherent, $\hat{\bm{y}}_{t+h|t}\notin\mathfrak{s}$ and in this case the inequality is strict.  Reconciliation is an orthogonal projection from $\hat{\bm{y}}$ to the coherent subspace yielding the reconciled forecast $\tilde{\bm{y}}$ shown in red.  Finally, the target of the forecast $\bm{y}$ is displayed as a black point, and although its exact location is unknown to the forecaster, it is known that it will lie somewhere along the coherent subspace.
	
	\begin{figure}[H]\label{fig:Schematic_OLSRecon}
		\centering
		\vspace{-0.9cm}
		\tiny
		\resizebox{\linewidth}{!}{\input{Figs/orth_pointforerec_schematic}}
		\caption{Orthogonal projection of $\hat{\bm{y}}$ onto $\mathfrak{s}$ yielding the reconciled forecast $\tilde{\bm{y}}$}
		
	\end{figure}
	
	
	Figure~\ref{fig:Schematic_OLSRecon} clearly shows that $\hat{\bm{y}}$, $\tilde{\bm{y}}$ and $\bm{y}$ form a right angled triangle with $\tilde{y}$ at the right-angled vertex.  In this triangle the line between $\bm{y}$ and $\hat{\bm{y}}$ is the hypotenuse and therefore must be longer than the distance between $\bm{y}$ and $\tilde{\bm{y}}$.  As such reconciliation is guaranteed to reduce the squared error of the forecast.
	
    Theorem~\ref{th:distred} is in some ways more powerful than perhaps previously understood.  Crucially, the result is not a result that requires taking expectations.  This distance reducing property will hold for any realisation and any forecast and not just on average.  Nothing needs to be assumed about the statistical properties of the data generating process or the process by which forecasts are made. 
    
    However, in other ways, Theorem~\ref{th:distred} is weaker than perhaps often understood. First, when improvements in forecast accuracy are discussed in the context of the theorem, this refers to a very specific measure of forecast accuracy.  In particular, this measure is the root of the sum of squared errors of {\em all} variables in the hierarchy.  As such, while forecast improvement is guaranteed for the hierarchy overall, reconciliation can lead to worse forecasts for individual series. Second, although orthogonal projections are guaranteed to improve on base forecasts, they are not necessarily the projection that leads to the greatest improvement in forecast accuracy.  As such referring to reconciliation via orthogonal projections as `optimal' is somewhat misleading since it does not have the optimality properties of some oblique projections, in particular MinT. It is to oblique projections that we now turn our attention.
    	
	\subsection{Oblique Projections}\label{sec:oblique}
	
	One justification for using an orthogonal projection is that it leads to improved forecast accuracy in terms of the root of the sum of squared errors of {\em all} variables in the hierarchy.  A clear shortcoming of this measure of forecast accuracy is that forecasts errors in all series should not necessarily be treated equally.  For example, in hierarchies, top-level series tend to have a much larger scale than bottom level series.  Even when two series are on a similar scale, series that are more predictable or less variable will tend to be downweighted by simply aggregating square errors.  An even more sophisticated understanding may take the correlation between series into account.  All of these considerations lead towards reconciliation of the form $\tilde{\bm{y}}=\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}\hat{\bm{y}}$, where $\bm{W}$ is a symmetric matrix.  Generally, it is assumed that $\bm{W}$ is invertible, otherwise a pseudo inverse can be used.
	
	It should be noted that  $\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}$ is an oblique, rather than an orthogonal projection matrix in the usual Euclidean geometry.  However this matrix can be considered to be an orthogonal projection for a different geometry defined by the norm $||\bm{v}||_{{\bm W}^{-1}}=\bm{v}'\bm{W}^{-1}\bm{v}$, which we will call the generalised Euclidean geometry with respect to $\bm{W}^{-1}$.  One way to understand this geometry is that it is the same as Euclidean geometry when all vectors are first transformed by pre-multiplying by $\bm{W}^{-1/2}$.  This leads to a transformed $\bm{S}$ matrix $\bm{S}^*=\bm{W}^{-1/2}\bm{S}$ and transformed $\hat{\bm{y}}$ and $\tilde{\bm{y}}$ vectors $\hat{\bm{y}}^*=\bm{W}^{-1/2}\hat{\bm{y}}$ and $\tilde{\bm{y}}^*=\bm{W}^{-1/2}\tilde{\bm{y}}$.  The transformed reconciled forecast results from an orthogonal projection in the transformed space since 
	
	\begin{align}
	\tilde{\bm{y}}^*&=\bm{W}^{-1/2}\tilde{\bm{y}}\\&=\bm{W}^{-1/2}\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}\hat{\bm{y}}
	\\&=\bm{S}^*\left(\bm{S}^{*'}\bm{S}^*\right)^{-1}\bm{S}^{*'}\hat{\bm{y}^*}
	\end{align}
	
	Thinking of the problem in terms of a geometry defined by the norm $\bm{v}'\bm{W}^{-1}\bm{v}$ is also quite instructive when it comes to thinking about the connection between distances and loss functions.  In the generalised Euclidean geometry, the distance between the reconciled forecast and the realisation is given by $(\hat{\bm{y}}-\bm{y})'\bm{W}^{-1}(\hat{\bm{y}}-\bm{y})$.  For diagonal $\bm{W}^{-1}$, this is equivalent to a weighted sum of squared error loss function and when $\bm{W}$ is a covariance matrix, this is equivalent to a Mahalanobis distance.  As such Theorem~\ref{th:distred} can easily be generalised as follows:
	
	\begin{theo}[General distance reducing property]\label{th:gdistred}
        If $\tilde{\bm{y}}_{t+h|t}=\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t}$, where $\bm{G}$ is such that $\bm{S}\bm{G}$ is an orthogonal (in the generalised Euclidean sense) projection onto $\mathfrak{s}$ then:
		\begin{equation}
		\|(\tilde{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|_{{\bm W}^{-1}}\le\|(\hat{\bm{y}}_{t+h|t}-\bm{y}_{t+h})\|_{{\bm W}^{-1}}.
		\end{equation}
	\end{theo}
	\begin{proof}
    The proof is identical to the proof for Theorem~\ref{th:distred} but relies on the generalised Pythagorean Theorem (applicable to Generalised Euclidean space) rather than the Pythagorean Theorem.
    \end{proof}  
	
	The implication of Theorem~\ref{th:gdistred} is that if the objective function is some weighted sum of squared errors, or a Mahalanobis distance, then the projection matrix $\bm{S}\left(\bm{S}'\bm{W}^{-1}\bm{S}\right)^{-1}\bm{S}'\bm{W}^{-1}$ is guaranteed to improve forecast accuracy over base forecasts, for an appropriately selected $\bm{W}$.
	
	Note here that we rely here on the generalised Pythagorean Theorem (which involves an equality).  In contrast, \cite{WicEtAl2019} follow \cite{VanErven2015a} in stating their result in terms of the Generalised Pythagorean Inequality.  The proof of \cite{WicEtAl2019} requires an assumptions about convexity so that the angle between the base forecast and coherent subspace must be greater than 90 degrees.  The proof we have provided here requires no such assumption, since this may not hold for an arbitrary $\bm{W}$.  As such the statement from \cite{WicEtAl2019} that {\em``MinT reconciled forecasts are at least as good as the incoherent forecasts''} should be qualified; this is only true in with respect to a loss function that depends on ${\bm W}$.  If Euclidean distance (or mean squared error) is used, there will be cases where the MinT estimator does not improve upon base forecasts.
	
	{\bf Discuss results in Figure 11 here} 
	
	
	\subsection{MinT}
	
	While the properties discussed so far hold for any projection matrix, the MinT method of \cite{WicEtAl2019} has an additional optimality property.  \cite{WicEtAl2019} show that for unbiased base forecasts, the trace of the forecast error covariance matrix of reconciled forecasts is minimised by an oblique projection with a particular choice of $\bm{W}$.  This choice is that $\bm{W}$ should be the forecast error covariance matrix where errors come from using the base forecasts.  Although the base forecast error covariance matrix is unknown, it can be estimated using in-sample errors.
	
	Figure~\ref{fig:MinT_justification1} provides geometrical intuition into the MinT method.  Suppose the in-sample errors are given by the orange points.  They provide information on the most likely direction of large deviations from the coherent subspace.  This direction is denoted by $\bm{R}$.  Figure~\ref{fig:MinT_justification2} then shows a target value of $\bm{y}$, while the grey points indicate possible values for the base forecasts (the base forecasts are of course stochastic).  One possible value of the forecast is depicted in blue as $\hat{\bm{y}}$.  An oblique projection of the blue point back along the direction of $\bm{R}$ yields a reconciled forecast closer to the target, especially compared to an orthogonal projection showed as in Figure \ref{fig:Orthogonal_projection_all_points}.  Figure \ref{fig:Oblique_projection_all_points} depicts a similar oblique projection along $\bm{R}$ for all the gray points yield reconciled forecasts tightly packed near the target $\bm{y}$.  In this sense, the oblique MinT projection minimises the forecast error variance of reconciled forecasts. In contrast to the result in Theorem~\ref{th:gdistred}, this property is a statistical property in the sense that MinT is optimal in expectation.
	
	\begin{figure}[H]
		\centering
		%\vspace{-0.9cm}
		\small
		\resizebox{\linewidth}{!}{
			\input{Figs/insampledir}
		}
		\caption{A schematic to represent MinT reconciliation. Points in orange colour represent the insample errors. $\bm{R}$ shows the most likely direction of deviations from the coherent subspace. $\hat{\bm{y}}$ is projected onto $\mathfrak{s}$ along the the direction of $\bm{R}$.}\label{fig:MinT_justification1}
	\end{figure}
		
	\begin{figure}[H]
		\centering
		%\vspace{-0.9cm}
		\small
		\resizebox{\linewidth}{!}{
			\input{Figs/oblique_justification2}
		}
		\caption{A schematic to represent MinT reconciliation. Grey points indicate potential realisations of the base forecast while the blue dot indicates one such realisation. The black dot ${\bm y}$ denotes the (unknown) target of the forecast.}\label{fig:MinT_justification2}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		%\vspace{-0.9cm}
		\small
		\resizebox{\linewidth}{!}{
			\includegraphics{Figs/OrthProj.pdf}}
		\caption{All gray points are orthogonally projected onto the coherent subspace $\mathfrak{s}$. Reconciled forecasts marked in red dots are widely spread about the target $\bm{y}$.}\label{fig:Orthogonal_projection_all_points}
	\end{figure}
	
	
	\begin{figure}[H] 
	\centering
	%\vspace{-0.9cm}
	\small
	\resizebox{\linewidth}{!}{
		\includegraphics{Figs/ObliqueProjection.pdf}}
	\caption{All gray points are obliquely projected along the direction of $\bm{R}$. Reconciled points denoted in red dots are concentrated about the target $\bm{y}$.}\label{fig:Oblique_projection_all_points}
	\end{figure}
	
%		\begin{figure}[H]
%		\centering
%		%\vspace{-0.9cm}
%		\small
%		\resizebox{\linewidth}{!}{
%			\input{Figs/ObliqueProjection}
%		}
%		\caption{\textcolor{red}{write caption}}\label{fig:Oblique_projection}
%	\end{figure}
	
%\begin{figure}
%	\centering
%	\begin{subfigure}{0.47\textwidth}
%		\resizebox{\linewidth}{!}{
%			\input{Figs/Orth.proj.tex}}
%		\caption{bla bla}
%	\end{subfigure}
%	\begin{subfigure}{0.47\textwidth}
%		\resizebox{\linewidth}{!}{
%			\input{Figs/ObliqueProjection.tex}}
%			\caption{bla bla}
%	\end{subfigure}
%\end{figure}

	
	\section{Bias in forecast reconciliation}\label{sec:BiasInRecon}
	
	Before turning our attention to the issue of bias itself it is important to state a sensible property that any reconciliation method should have.  That is if base forecasts are already coherent then reconciliation should not change the forecast.  As stated in Section~\ref{sec:Reconciliation}, this property holds only when $\bm{SG}$ is a projection matrix.  As a corollary, reconciling using an arbitrary $\bm{G}$,  may in fact change an already coherent forecast.  
	
	The property that projections map all vectors in the coherent subspace onto themselves is also useful in proving the unbiasedness preserving property of reconciliation \cite{WicEtAl2019}.  Before restating this proof using a  clear geometric interpretation we discuss in a precise fashion what is meant by unbiasedness.  
	
	Suppose that the target of a point forecast is $\bm{\mu}_{t+h|t}:=\E(\bm{y}_{t+h}\mid\bm{y}_{1},\dots,\bm{y}_{t})$ where the expectation is taken over the predictive density.  Our point forecast can be thought of as an estimate of this quantity.\todo{or do we want to define unbiasedness of a forecast as the expected value of the forecast equals realisation?}  The forecast is random due to uncertainty in the training sample and it is with respect to this uncertainty that unbiasedness refers.  More concretely, the point forecast will be unbiased if $\E_{1:t}(\hat{\bm{y}}_{t+h|t})=\bm{\mu}_{t+h|t}$, where the subscript $1:t$ denotes an expectation taken over the training sample.
	
	\begin{theo}[Unbiasedness preserving property]
		For unbiased $\hat{\bm{y}}_{t+h|t}$, the reconciled point forecast is also an unbiased prediction as long as $\bm{SG}$ is a projection onto $\mathfrak{s}$.
	\end{theo}
	\begin{proof}
		The expected value of the reconciled forecast is given by
		\[
		\E_{1:t}(\tilde{\bm{y}}_{t+h|t})
		= \E_{1:t}(\bm{S}\bm{G}\hat{\bm{y}}_{t+h|t})
		= \bm{S}\bm{G}\E_{1:t}(\hat{\bm{y}}_{t+h|t})
		= \bm{S}\bm{G}\bm{\mu}_{t+h|t}.
		\]
		Since $\bm{\mu}_{t+h|t}$ is an expectation taken with respect to the degenerate predictive density it must lie in $\mathfrak{s}$. We have already established that when $\bm{S}\bm{G}$ is a projection onto $\mathfrak{s}$ then it maps all vectors in $\mathfrak{s}$ onto themselves. As such $\bm{S}\bm{G}\bm{\mu}_{t+h|t}=\bm{\mu}_{t+h|t}$ when $\bm{S}\bm{G}$ is a projection matrix.
	\end{proof}

	We note that the above result holds when the projection $\bm{SG}$ has the coherent subspace $\mathfrak{s}$ as its image and not for all projection matrices in general. To describe this more explicitly suppose $\bm{SG}$ has as its image  $\mathfrak{L}$ which is itself a lower dimensional linear subspace of $\mathfrak{s}$, i.e. $\mathfrak{L}\subset\mathfrak{s}$. Then for $\left\{\bm{\mu}_{t+h|t}:\bm{\mu}_{t+h|t}\in\mathfrak{s},\bm{\mu}_{t+h|t}\notin\mathfrak{L}\right\}$,  $\bm{S}\bm{G}\bm{\mu}_{t+h|t} \ne \bm{\mu}_{t+h|t}$. This is depicted in Figure~\ref{fig:Schematic_3D} where $\bm{\mu}_{t+h|t}$ is projected to a point $\bar{\bm{\mu}}$ in $\mathfrak{L}$.  In this case, the expectation of reconciled forecast will be $\bar{\bm{\mu}}$ rather than $\bm{\mu}_{t+h|t}$ and hence biased.  
	
	This result has implications in practice. The top-down method \citep{Gross1990} has 
	\begin{equation}\label{eq:top-downG}
	\bm{G}=\begin{pmatrix}
	\bm{p} & \bm{0}_{(m \times n-1)}
	\end{pmatrix}
	\end{equation}
    where $\bm{p} = (p_1,\dots,p_m)'$ is an $m$-dimensional vector consisting a set of proportions used to disaggregate the top-level forecast.  In this case it can be verified that $\bm{SG}$ is idempotent, i,e. $\bm{SGSG}=\bm{SG}$ and therefore $\bm{SG}$ is a projection matrix.  However the image of this projection is not an $m$-dimensional subspace but a $1$-dimensional subspace.  As such, top-down reconciliation produces biased forecasts even when the base forecasts are unbiased.
		
	\begin{figure}[H]
		\centering
		\vspace{-0.9cm}
		\small
		\resizebox{\linewidth}{!}{
			\input{Figs/Schem_3D}
		}
		\caption{$\mathfrak{L}$ is a linear subspace of the coherent subspace $\mathfrak{s}$. If $s\circ g$ is a projection not onto $\mathfrak{s}$ but onto $\mathfrak{L}$, then $\bm{\mu} \in \mathfrak{s}$ will be moved to $\bar{\bm{\mu}} \in \mathfrak{L}$.}\label{fig:Schematic_3D}
	\end{figure}
	
	Finally, it is often stated that an assumption required to prove the unbiasedness preserving property is that $\bm{SGS}=\bm{S}$ or alternatively that $\bm{GS}=\bm{I}$.  Both of these conditions are equivalent to assuming that $\bm{SG}$ is a projection matrix.  \todo{perhaps elaborate in a proof in appendix}  However, problems arise when viewing the preservation of unbiasedness through the prism of imposing a constraint $\bm{GS}=\bm{I}$. This thinking suggests that a way to deal with biased forecasts is to select $\bm{G}$ in an unconstrained manner.  However, equipped with a geometric understanding of the problem, we would advise against this approach.  The constraint $\bm{GS}=\bm{I}$  is not just about bias and dropping the constraint compromises all of the attractive properties of projections.  Opening the door to reconciliation methods that change already coherent base forecast would seem to suggest an increase in the variability of the forecast.  This seems particularly perverse when the motivation for using a biased method in the first place is to reduce variance is to reduce variance. 
	
	Our own solution to dealing with biased forecasts is to bias correct {\em before} reconciliation.  In many cases the method for bias correction will be context specific.  For instance, in  our empirical study in Section~\ref{sec:EmpStudy} we consider a scenario where bias is induced via taking a Box-Cox transformation before modelling.  In this well-known case a number of bias correction methods exist.  Our particular choice of bias correction will be the Guererro method \citep{guerrero1993time}.
	
	Alternatively, a more general purpose approach to bias correction is to simply estimate the bias by taking the sample mean of $\bm{y
	}_{t+h}-\hat{\bm{y}}_{t+h|t}$ for all $t+h$ in the training sample.  This can be then subtracted from future forecasts.  As stated in the discussion of MinT, in-sample errors are already used to estimate the optimal direction of projection.  As such we see no problems with using the same errors to bias correct.  Geometrically, the intuition is simple.  In Figure~\ref{fig:MinT_justification1}, the orange points are centered around the origin as would be expected from an unbiased forecast.  If forecasts are biased, then errors should simply be translated until they are centered at the origin.
	
	\section{Empirical study} \label{sec:EmpStudy}
	
	Total tourism flow of any country can naturally disaggregate along a geographical hierarchy. Producing accurate forecasts for these hierarchical time series while preserving the coherency is important for making align business decisions in the tourism industry. Many applications related to Australian tourism flow have pronounced that reconciliation improves the point forecast accuracy (\cite{Athanasopoulos2009}, \cite{Hyndman2011}, \cite{WicEtAl2019}). 
	
	Using a similar empirical application to tourism forecasting in Australia, we show how to use projection-based reconciliation when we have a set of biased incoherent forecasts. As we mentioned in the previous section our focus is to bias adjust the incoherent forecasts first and then do the reconciliation. 
	
	\subsubsection*{Data:}
	We use ``overnight trips" as a measure of domestic tourism flow in Australia. Total ``overnight trips" in Australia can be disaggregated into 7 states, 27 zones and 75 regions forming a geographical hierarchy with $110$ total number of series and $75$ bottom level series. More information about the series of this hierarchy is given in Table \ref{table:A1} in the appendix. Data were obtained from the National Visitor Survey (NVS) which were collected through telephone interviews from an annual sample of $120,000$ Australian residents aged $15$ years or more. Data form a monthly series starting from January 1998 to December 2017 which gives a total of $240$ observations per series. 
	
	We produce $h=1$ to $h=6$ months ahead forecasts using a rolling window approach. First training window of $100$ observations is considered from Jan-$1998$ to Apr-$2006$ and produce forecasts for May-$2006$ to Oct-$2006$. Then we roll the training window one month at a time where the final forecast is produced for Dec-$2017$. This directs to $140 \quad 1$-step-ahead, $139 \quad 2$-step-ahead through $135 \quad 6$-step-ahead forecasts left for evaluation. 
	
	
	\subsubsection*{Preliminary analysis:}
		
	While observing the patterns of individual series, we noticed that the signal-to-noise ratio decreases with the disaggregate level of the hierarchy. From Figure \ref{fig:Total_TSplots} and \ref{fig:States_Zones_Regs_TSplots} this is depicted well as we see much noisier series in bottom levels whereas the noise level is comparatively stabilised in the upper levels. We have also observed that the overnight trips for `Adelaide Hills` has an anomalous observation on December-2002. We replaced this observation with the average overnight trips on December-2001 and December-2003 for the same destination.
	
	\begin{figure}
		\centering
		\small
		\includegraphics[width = \textwidth]{Empirical-results/TS-plots/Total_TSplot.pdf}
		\caption{Time plot for total overnight trips.}\label{fig:Total_TSplots}
	\end{figure}
	
	
	\begin{figure}
		\centering
		\small
		\includegraphics[width = \textwidth]{Empirical-results/TS-plots/States_Zones_Regs_TSplots.pdf}
		\caption{Time plot for some selected series from different disaggregate levels of the hierarchy.}\label{fig:States_Zones_Regs_TSplots}
	\end{figure}

	\subsubsection*{Base forecasts and reconciliation:}
	
	We first transform each series in the hierarchy using the Box-Cox transformation as given in equation (\ref{eq:BoxCox_transformation}) to stabilise any variations in the data. 
		
	\begin{equation} \label{eq:BoxCox_transformation}
	x_t = 
	\begin{cases} 
	\log(y_t) & \text{if } \lambda = 0 \\
	\frac{y_t^\lambda - 1}{\lambda}  & \text{if } \lambda \ne 0
	\end{cases}
	\end{equation}
	
	We use "guerrero" method \citep{guerrero1993time} implemented in \verb|BoxCox.lambda()| function in \verb|forecast| package R software \citep{Rforecast} to select optimal $\lambda$. Then  we fit univariate ARIMA models for the transformed series. We use \verb|auto.arima()| function in \verb|forecast| package to choose the best model that minimises the AIC. Using the fitted models we produce $h=1,...,6$ months ahead forecasts for each series in the hierarchy. These forecasts are then back-transformed using equation (\ref{eq:BoxCox_back-transformation}) to scale them back into the original space. However, the back transformed forecasts will be biased \citep{FPP2018}. 
	
	\begin{equation} \label{eq:BoxCox_back-transformation}
	y_{t+h} = 
	\begin{cases} 
	\exp({x_{t+h}}) & \text{if } \lambda = 0 \\
	(\lambda x_{t+h} + 1)^{1/\lambda}  & \text{if } \lambda \ne 0
	\end{cases}
	\end{equation}
	
	
	Hence the reconciled forecasts follow from these will also biased. This is the exact scenario that we want to demonstrate in this study. As we proposed before, we first bias correct the base forecasts prior to the reconciliation. We use the mathematical formula in equation (\ref{eq:BoxCox_BT_biasadj}) to calculate the back-transformed and bias-adjusted forecasts.
	
	\begin{equation} \label{eq:BoxCox_BT_biasadj}
	 y_{t+h} = 
	\begin{cases} 
	\exp({x_{t+h}})[1+\frac{\sigma_h^2}{2}] & \text{if } \lambda = 0 \\
	(\lambda x_{t+h} + 1)^{1/\lambda}[1 + \frac{\sigma_h^2(1-\lambda)}{2(\lambda x_{t+h} + 1)^2}]       & \text{if } \lambda \ne 0
	\end{cases}
	\end{equation}
	where, $x_{t+h}$ is the $h$-step-ahead forecasts from the Box-Cox transformed series and $\sigma_h^2$ is the variance of $x_{t+h}$. 
	
	Once we get the base forecasts for all series in the hierarchy, we can do the projection-based reconciliation to get coherent forecasts. We reconcile both biased and bias-corrected (unbiased) base forecasts to demonstrate how the reconciliation follows from unbiased base forecasts improves the forecast accuracy. MinT(Shrink), WLS, OLS and bottom-up methods are used for reconciliation.
	
	\subsubsection*{Results and discussion:}
	
	Mean Squared Error (MSE) is used to measure the forecast accuracy and results are presented in Table \ref{tab:Results_MSE}. Recall that projections preserve the unbiasedness in reconciled forecasts only if the base forecasts are unbiased. As a consequence, the reconciled forecasts follow from the biased base forecasts will also be biased. We can see from the results that unbiased-reconciled forecasts from MinT(Shrink) and WLS are better than that of biased-reconciled forecasts although the unbiased-base forecasts are not necessarily better than the biased-base forecasts. Furthermore, unbiased MinT reconciled forecasts are outperforming all biased and unbiased forecasts. These results are true for all forecast horizons. 
	
	\begin{table}
		\caption {Average {MSE($\times 10^3$)} of base and reconciled point forecasts at forecasts horizons $h=1,...,6$ are presented. Bias and unbiased columns represent the results for biased-base/biased-reconciled and unbiased-base/unbiased-reconciled forecasts respectively. Comparisons can be made across biased vs unbiased forecasts as well as base vs reconciled forecasts.}
		\label{tab:Results_MSE}
		\centering\tabcolsep=0.08cm
		\resizebox{\linewidth}{!}{
		\begin{tabular}{@{}lSSSSSSSSSSSS@{}}
			\toprule
			\multicolumn{1}{c}{ } & \multicolumn{2}{c}{h=1} & \multicolumn{2}{c}{h=2} & \multicolumn{2}{c}{h=3} & \multicolumn{2}{c}{h=4} & \multicolumn{2}{c}{h=5} & \multicolumn{2}{c}{h=6} \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
			Method & Biased & Unbiased & Biased & Unbiased & Biased & Unbiased & Biased & Unbiased & Biased & Unbiased & Biased & Unbiased\\
			\midrule
			Base & 12.18 & 161.63 & 13.14 & 243.13 & 14.08 & 269.20 & 15.88 & 325.41 & 15.49 & 332.11 & 15.79 & 344.40\\
			\hline
			MinT(Shrink) & 10.14 & $\bm{9.96}$ & 11.47 & $\bm{10.78}$ & 12.55 & $\bm{11.94}$ & 15.53 & $\bm{14.03}$ & 14.63 & $\bm{13.46}$ & 15.28 & $\bm{13.78}$\\
			\hline
			WLS & 15.62 & 13.97 & 15.93 & 13.71 & 16.65 & 14.50 & 18.91 & 16.26 & 18.76 & 16.20 & 18.85 & 15.83\\	
			\hline
			OLS & 11.73 & 135.17 & 12.80 & 204.42 & 13.73 & 225.59 & 15.56 & 272.41 & 15.17 & 278.08 & 15.49 & 288.21\\
			\hline
			Bottom-up & 17.86 & 15.55 & 17.86 & 14.90 & 18.47 & 15.61 & 20.50 & 17.21 & 20.34 & 17.04 & 20.39 & 16.69\\
			
			\bottomrule
		\end{tabular}
	}
	\end{table}
	
	
	
%	\begin{figure}
%		\centering
%		\small
%		\includegraphics[width = \textwidth]{Empirical-results/Final-results/SS_MSE_Results.pdf}
%		\caption{\textcolor{red}{include caption}}\label{fig:MSE_SkillScore}
%	\end{figure}
%	
	
	
	Following the distance reducing property in Theorem \ref{th:distred} and the discussion therein, we have noted that the orthogonal projections will always improve the base forecasts with respect to the mean squared errors whereas, the oblique projections will not always, but improves only on average. Figure \ref{fig:BaseVSRecon_Fc} shows the MSE difference between base forecasts and reconciled forecasts - MinT, OLS and WLS for all replications in different forecast horizons. Recall that OLS reconciliation is doing orthogonal projection whereas MinT and WLS are doing oblique projections onto the coherent subspace. It is apparent from the plot that the MSE difference for OLS is always positive, implying that OLS improves the base in terms of MSE in all replications. On the other hand,  there are some instances that this difference for MinT and WLS is negative which is reflected from the short negative tails of box-plots. This implies that oblique projections will not improve the base forecasts in every case. However, these projections will outperform the base with respect to MSE on average which is again reflected from the highly positive skewed MSE differences.  
	
	
	 \begin{figure}[H]
		\centering
		\small
		\includegraphics[width = \textwidth]{Empirical-results/Final-results/Base-Recon.pdf}
		\caption{Difference of MSE between Base forecasts and reconciled forecasts over the replications in distinct forecast horizons. Positive values of the difference implies reconciliation improves the forecast accuracy than base forecasts.}\label{fig:BaseVSRecon_Fc}
	\end{figure}
	
	
	
	
	
	\section{Conclusions} \label{sec:conclusions}
	
	By redefining coherent point forecast and point forecast reconciliation, we rehash all existing reconciliation methods into a single projection based geometric framework. We have also established new theoretical results that support the use of projections for point forecast reconciliation. We show that projection of unbiased base forecasts onto the coherent subspace will always produce unbiased reconciled forecasts. Yet the projection-based reconciliation can be used to reconciled bias base forecasts after bias adjustments. Empirical results from the application of these methods to forecasting Australian domestic tourism flow show that reconciled forecast follows from bias-adjusted base improves the forecast accuracy and unbiased MinT reconciliation is outperforming. 
	
	
	These new geometric interpretations of hierarchical forecast reconciliation facilitate extensions of the problem to a probabilistic framework. We leave this discussion into a different paper. 	
	
	\newpage
	\section{Appendix}
	\thisfloatpagestyle{empty}
	\input{Appendix}

	
	\newpage
	
	\bibliographystyle{agsm}
	
	\bibliography{References_paper1}
	
\end{document}

